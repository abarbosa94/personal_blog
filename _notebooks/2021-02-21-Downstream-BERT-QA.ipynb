{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"BERT applied to Multiple Choice\"\n",
    "> \"Step by step about a specific fine tuning task:)\"\n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- author: Andre Barbosa\n",
    "- badges: true\n",
    "- hide_binder_badge: false\n",
    "- hide_colab_badge: false\n",
    "- comments: true\n",
    "- categories: [masters, nlp, knowledge-distill]\n",
    "- hide: false\n",
    "- search_exclude: false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drilling down Multiple Choice downstream task\n",
    "\n",
    "> Note: I have learned how to use bibtex citations with fastpages! Therefore, all my next post are going to follow these kind of formatting whenever possible. If you are interested, check [this](https://drscotthawley.github.io/devblog4/2020/07/01/Citations-Via-Bibtex.html) out.\n",
    "\n",
    "When I started studying Language Models, I remember when I've found the following image from GPT paper {% cite Radford2018ImprovingLU %} :\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/downstream-gpt.png \"Example of fine-tuning tasks from GPT paper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the only difference is that the **input data** should be _slightly_ different:\n",
    "\n",
    "> For these tasks, we are given a context\n",
    "document $z$, a question $q$, and a set of possible answers ${a_k}$. We concatenate the document context\n",
    "and question with each possible answer, adding a delimiter token in between to get [$z$; $q$; $ \\$ $; $a_k$]. Each of these sequences are **processed independently with our model and then normalized via a softmax layer to produce an output distribution over possible answers**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, these inputs could be optimized via [Categorical Cross Entropy Loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html), where $C$ is the number of options available. For a specific question.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From GPT to BERT\n",
    "\n",
    "As we will see with [Hugging Face's transformer library](https://huggingface.co/transformers/), when we considerer application from a fine tuning task, the approach of BERT can be derived directly from the tecnique presented by {% cite Radford2018ImprovingLU %}.\n",
    "It is possible to check it from [documentation](https://huggingface.co/transformers/model_doc/bert.html#transformers.BertForMultipleChoice)\n",
    "\n",
    "> Bert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a softmax) e.g. for RocStories/SWAG tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Downloading: 100%|██████████| 232k/232k [00:01<00:00, 171kB/s]\n",
      "Downloading: 100%|██████████| 433/433 [00:00<00:00, 122kB/s]\n",
      "Downloading: 100%|██████████| 440M/440M [02:06<00:00, 3.48MB/s]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultipleChoice: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#collapse\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForMultipleChoice\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForMultipleChoice.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"George wants to warm his hands quickly by rubbing them. Which skin surface will produce the most heat?\"\n",
    "option_a = \"dry palms\"\n",
    "option_b = \"wet palms\"\n",
    "option_c = \"palms covered with oil\"\n",
    "option_d = \"palms covered with lotion\""
   ]
  },
  {
   "source": [
    "In this case, option A is the correct one. Furthermore, the batch size here would be 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor(0).unsqueeze(0) "
   ]
  },
  {
   "source": [
    "Notice that the question is the same for each option"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer(\n",
    "            [question, question, question, question],\n",
    "            [option_a, option_b, option_c, option_d],\n",
    "            return_tensors='pt',\n",
    "            padding=True\n",
    "           )\n",
    "\n",
    "outputs = model(**{k: v.unsqueeze(0) for k,v in encoding.items()}, labels=labels)"
   ]
  },
  {
   "source": [
    "> Important: Notice that if we have a dataset such as SquaD where each question comes with a context, we could append this context to either the question text or the option text and we would then have the tuple cited by [GPT paper](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)\n",
    "\n",
    "The output is a linear layer which would still be trained through a Cross Entropy loss. Then, as stated by the documentation, we still need to apply softmax to the logits"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = outputs.loss\n",
    "logits = outputs.logits"
   ]
  },
  {
   "source": [
    "\n",
    "Linear Logits output:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-0.3457, -0.3295, -0.3271, -0.3342]], grad_fn=<ViewBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "# hide-input\n",
    "logits"
   ]
  },
  {
   "source": [
    "Logits after the softmax function. Since this model did not learn anything, the result below is expected:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0.2471, 0.2511, 0.2518, 0.2500]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "# hide-input\n",
    "\n",
    "torch.nn.functional.softmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Congratulations! Adding up with the [first part](https://abarbosa94.github.io/personal_blog/masters/nlp/2020/09/19/Distilling-BERT.html), you have learned the end-to-end BERT Flow :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "source": [
    "{% bibliography --cited %}"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit ('emma-watson')",
   "metadata": {
    "interpreter": {
     "hash": "64aefe5b869f1290ff4de3ee336b91b598f3cf379f1c85f644156f79c1a37abc"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}