{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Destilando Pré Treinamento do BERT\"\n",
    "> \"Um passo a passo sobre como ele funciona :)\"\n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- author: Andre Barbosa\n",
    "- badges: true\n",
    "- hide_binder_badge: false\n",
    "- hide_colab_badge: false\n",
    "- comments: true\n",
    "- categories: [masters, nlp, knowledge-distill]\n",
    "- hide: true\n",
    "- search_exclude: true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: Para uma versão em **inglês** confira [aqui](https://abarbosa94.github.io/personal_blog/masters/nlp/knowledge-distill/2020/09/19/Distilling-BERT.html)\n",
    "\n",
    "# Uma rápida revisão\n",
    "\n",
    "Eu lembro algum dia de 2016, quando eu estava no início da kinha carreira, eu encontrei por acaso o [blog do Chirs McCormick sobre Word2Vec](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/). Honestamente, acredito que o [artigo escrito pelo Tomas Mikolov](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) foi uma das indéias mais interessantes que eu já encontrei nessa minha jornada como cientista de dados {% fn 1 %} :) \n",
    "\n",
    "{{ 'Fun Fact: O [perfil do LinkedIn do Miklov](https://www.linkedin.com/in/tomas-mikolov-59831188/?originalSubdomain=cz) mostra que ele trabalhou na Microsoft, Google e Facebook; outro autor do W2V, [Ilya Sutskever](http://www.cs.toronto.edu/~ilya/) teve oportunidades de trabalhar com os maiores pesquisadores da área moderna de IA, tais como [Geoffrey Hinton](https://www.cs.toronto.edu/~hinton/) e [Andrew Ng](https://www.andrewng.org/). Além disso, ele é um dos fundadores da [Open AI](https://openai.com/)! ' | fndetail: 1 }}\n",
    "\n",
    "## O que são Word Embeddings\n",
    "\n",
    "\n",
    "Segundo a documentação do [Pytorch](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html),  um **Embedding** pode ser definido da seguinte forma: \n",
    "\n",
    "   >Uma tabela de lookup formada por um _dicionário_ de tamanho fixo.\n",
    "\n",
    "Podemos interpretar os embeddings como uma forma de converter _índices_ em _vetores_ de um tamanho específico. Logo, **word embeddings**, podem ser entendidos como palavras que são convertidas para inteiros e **esses** números servem de índices para diferentes linhas de uma matriz que representa o espaço vetorial.'\n",
    "\n",
    "Eu escrevi um código usando [manim](https://github.com/3b1b/manim) que ilustra isso:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/media/videos/scene/720p30/EmbeddingExample.gif \"Nesse exemplo, a dimensão do embedding é NxM, em que N seria o tamanho do vocabulário (8) e M é 4.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos interpretar cada dimensão como um único neurônio de uma camada oculta, e, então, **o tamanho desses embeddings podem ter seus números alterados** a partir de uma rede neural. Essa é, basicamente, a ideia por trás de algoritmos como [Word2Vec](https://patents.google.com/patent/US9037464B1/en) e [fastText](https://fasttext.cc/) {% fn 2 %} \n",
    "\n",
    "Já existem algumas bibliotecas que já fornecem alguns vetores pré-treinados. Por exemplo, considere o [código Spacy](https://spacy.io/models) abaixo:\n",
    "\n",
    "{{ 'Eu não irei cobrir Word2Vec nesse blog post. Se você não tem familiaridade com isso, [consulte aqui](http://jalammar.github.io/illustrated-word2vec/); [aqui](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/) e [aqui](https://www.youtube.com/watch?v=ASn7ExxLZws). Infelizmente, todos os links estão em inglês. Se você achar quiser que eu escreva um post sobre Word2Vec em português, me envie uma mensagem no meu [linkedin](https://www.linkedin.com/in/barbosaandre/) :)' | fndetail: 2 }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Considere a sentença 'O rato roeu a roupa do rei de Roma!'\n'O' representação vetorial com tamanho 96. Seus primeiros 5 elementos são: [ 1.2   0.18 -0.97 -5.64 -4.65]\n'rato' representação vetorial com tamanho 96. Seus primeiros 5 elementos são: [ 3.17  5.36  0.14 -1.27  3.09]\n'roeu' representação vetorial com tamanho 96. Seus primeiros 5 elementos são: [ 1.17 -2.8   2.39 -0.33  0.4 ]\n'a' representação vetorial com tamanho 96. Seus primeiros 5 elementos são: [-0.99  4.67  1.21 -3.48 -2.62]\n'roupa' representação vetorial com tamanho 96. Seus primeiros 5 elementos são: [ 3.6   3.54 -0.66 -1.9   1.99]\n'do' representação vetorial com tamanho 96. Seus primeiros 5 elementos são: [-3.28 -2.15 -1.62  4.33  0.55]\n'rei' representação vetorial com tamanho 96. Seus primeiros 5 elementos são: [ 2.43  2.99 -2.72  2.31  5.31]\n'de' representação vetorial com tamanho 96. Seus primeiros 5 elementos são: [-4.49 -1.73  2.27  7.9   3.35]\n'Roma' representação vetorial com tamanho 96. Seus primeiros 5 elementos são: [ 4.87  0.42  1.91 -1.68  6.37]\n'!' representação vetorial com tamanho 96. Seus primeiros 5 elementos são: [ 0.61 -3.03 -1.37 -0.38 -2.72]\n"
     ]
    }
   ],
   "source": [
    "#collapse-hide\n",
    "import spacy\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "print(\"Considere a sentença 'O rato roeu a roupa do rei de Roma!'\")\n",
    "text = nlp(\"O rato roeu a roupa do rei de Roma!\")\n",
    "for word in text:\n",
    "    print(\n",
    "        f\"'{word.text}' representação vetorial com tamanho {word.vector.shape[0]}. Seus primeiros 5 elementos são: {word.vector[:5].round(2)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essas palavras são representações que foram treinadas com base nos dados do [Common Crawl usando o algoritmo GloVe](https://github.com/explosion/spacy-models/releases/tag/en_core_web_md-3.0.0). Diferente do exemplo usado no começo deste blog, a palavra '!' também teve uma representação vetorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para formar frases, podemos combinar embeddings de palavras de formas diferentes. Segundo a [documentação do spacy](https://spacy.io/usage/vectors-similarity#_title):\n",
    "\n",
    "> Modelos que possuem vetores de palavras estão disponíveis pelo atributo Token.vector. Doc.vector e Span.vector, por padrão são representados pela **média** da representação de seus vetores. \n",
    "\n",
    "\n",
    "Logo, a frase que estamos usando como exemplo tem a seguinte representação vetorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Os primeiros 5 valores de 'The quick brown fox jumps over the lazy dog!!': [-0.23  0.08 -0.03 -0.07 -0.02]\n"
     ]
    }
   ],
   "source": [
    "#hide_input\n",
    "print(f\"Os primeiros 5 valores de 'The quick brown fox jumps over the lazy dog!!': {text.vector[:5].round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitações dos Word Embeddings\n",
    "\n",
    "Apesar de Word Embeddings trouxeram muitos benefícios na área de linguística computacional, eles possuem algumas limitações. Existe um fenômeno na linguística chamado _polissemia_. De acordo com o [wikipedia](https://pt.wikipedia.org/wiki/Sem%C3%A2ntica):\n",
    "\n",
    "> É a propriedade que uma mesma palavra tem de apresentar vários significados. Exemplos: Ele ocupa um alto posto na empresa. / Abasteci meu carro no posto da esquina. / Os convites eram de graça. / Os fiéis agradecem a graça recebida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considerando o exemplo acima, mesmo que as palavras tenham **significados diferentes** por conta do contexto, **sua representação vetorial é a mesma**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Primeiros cinco valores da palavra 'posto': [ 2.23  0.89  1.63  1.8  -0.12]\n"
     ]
    }
   ],
   "source": [
    "#hide_input\n",
    "print(f\"Primeiros cinco valores da palavra 'posto': {nlp('posto').vector[:5].round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se pegarmos duas frases: `Ele ocupa um alto posto na empresa` e `Abasteci meu carro no posto da esquina`, então nós teremos os seguintes vetores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Os primeiros 5 valores do vetor da sentença 'Ele ocupa um alto posto na empresa ': [ 0.55  1.04  0.21 -0.36  0.51]\nOs primeiros 5 valores do vetor da sentença Abasteci meu carro no posto do alto do morro ': [ 0.73  0.82 -0.78  2.11  0.61]\n"
     ]
    }
   ],
   "source": [
    "text1 = nlp(\"Ele ocupa um alto posto na empresa\")\n",
    "text2 = nlp(\"Abasteci meu carro no posto do alto do morro\")\n",
    "\n",
    "print(f\"Os primeiros 5 valores do vetor da sentença '{text1} ': {text1.vector[:5].round(2)}\")\n",
    "print(f\"Os primeiros 5 valores do vetor da sentença {text2} ': {text2.vector[:5].round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ao calcular a **similaridade de cossenos** entre a média destes vetores:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Similaridade:\n 'Ele ocupa um alto posto na empresa' and 'Abasteci meu carro no posto do alto do morro': 0.5666443705558777\n"
     ]
    }
   ],
   "source": [
    "# collapse-hide\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(\n",
    "    f\"Similaridade:\\n '{text1}' and '{text2}': \"\n",
    "    f\"{cosine_similarity(text1.vector.reshape(1, -1),text2.vector.reshape(1, -1))[0][0]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso indica que ambos os vetores tem alguma similares. Contudo, a razão disso foi o uso de palavras parecidas, uma vez que o significado das sentenças é **completamente** diferente. \n",
    "\n",
    "Isso é algo que o BERT tenta resolver.{% fn 3 %} \n",
    "\n",
    "\n",
    "\n",
    "{{ 'Existem alguns percursores do BERT como o [ELMo](https://allennlp.org/elmo); [ULMFit](https://arxiv.org/abs/1801.06146) e [Open AI Transformer](https://openai.com/blog/language-unsupervised/) que eu não irei cobrir aqui. Por favor, caso você queira, confira esse post [aqui](http://jalammar.github.io/illustrated-bert/) para saber mais' | fndetail: 3 }}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention is all you need\n",
    "\n",
    "The [Attention is all you need](https://arxiv.org/abs/1706.03762) paper have introduced the Transformer architeture for us :) In sense, it can be summarized as the picture below:\n",
    "\n",
    "![](images/transformer.png \"The transformer- model architeture, taken from: https://arxiv.org/abs/1706.03762\")\n",
    "\n",
    "Strictly speaking, the motivation behind the paper is that _RNN_-like architetures are memory-expensive. The purpose behind Transformer models is that it you can achieve similar results using more computer efficient resources by applying **just attention mechanisms** (and exluding the CNN or RNN-like architetures) !{% fn 4 %} Despite the fact that the Transformer model was proposed to deal with translation problems, it turns out that we can also use variations of it to achieve awesome results in different tasks. This is the **motivation behind BERT**!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "{{ '[The NLP group from Harvard](http://nlp.seas.harvard.edu/2018/04/03/attention.html) has written a great blog post distilling the paper as well as implementing them in pytorch. If you have some interest in knowing details about the transformer architecture, I recommend looking at it! ' | fndetail: 4 }}\n",
    "\n",
    "### Attention?\n",
    "\n",
    "According to the [Transformer and Attention lecture from NYU foundations of Deep Learning Course](https://atcold.github.io/pytorch-Deep-Learning/en/week12/12-3/):\n",
    "\n",
    "   > Transformers are made up of attention modules, which are mappings between sets, rather than sequences, which means we do not impose an ordering to our inputs/outputs.\n",
    "   \n",
    "\n",
    "\n",
    "When we analyze the transformer architeture, we can see that both _Multi-Head Attention_ and _Multi-Head Masked Attention_ box have 3 Arrow Heads. Each one represents one of the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - _Q_ that stands for **query** vector with dimension $d_k$ \n",
    "   - _K_ that stands for **key** vector that also has dimension $d_k$\n",
    "   - _V_ that stands for **value** vector that also has dimension $d_v$\n",
    "   \n",
    "Where these three can be understood as projections over the input embeddings.\n",
    "\n",
    "### Key-Value Store\n",
    "\n",
    "Again, from the [Deep Learning Foundations Course from NYU](https://atcold.github.io/pytorch-Deep-Learning/en/week12/12-3/):\n",
    "\n",
    "> A key-value store is a paradigm designed for storing (saving), retrieving (querying), and managing associative arrays (dictionaries/hash tables)\n",
    "\n",
    ">For example, say we wanted to find a recipe to make lasagne. We have a recipe book and search for “lasagne” - this is the query. This query is checked against all possible keys in your dataset - in this case, this could be the titles of all the recipes in the book. We check how aligned the query is with each title to find the maximum matching score between the query and all the respective keys. If our output is the argmax function - we retrieve the single recipe with the highest score. Otherwise, if we use a soft argmax function, we would get a probability distribution and can retrieve in order from the most similar content to less and less relevant recipes matching the query.\n",
    "\n",
    "> Basically, the query is the question. Given one query, we check this query against every key and retrieve all matching content.\n",
    "\n",
    "Then, we can say that _K_; _Q_ and _V_ are specific rotations around a given input vector _x_ (the embedding one, for instance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Warning: I have decided not to cover attention concepts in this post, giving just a higher-level introduction. As you might have noticed, NYU Deep Learning Foundations Course provides a really nice introduction about the topic that I recommend going through if you want to learn more :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "\n",
    "This was taken from [The annotated transformer blog](http://nlp.seas.harvard.edu/2018/04/03/attention.html#prelims) where you can find a cool pytorch implementation. It turns out that actually this is a quote from Attention is all you need [paper](https://arxiv.org/pdf/1706.03762.pdf):\n",
    "\n",
    ">Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add “positional encodings” to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension \n",
    "$d_{model}$ as the embeddings, so that the two can be summed\n",
    "\n",
    "\n",
    "![](images/positional_encoding.png \"One example of a positional encoding that generates sine wave based on length. Notice that each dimension generates a sine wave with different frequency. Source: http://nlp.seas.harvard.edu/2018/04/03/attention.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The BERT model\n",
    "\n",
    "BERT model itself is an _encoder model_ only from the transformer model. Considering the models trained from the [paper](https://arxiv.org/pdf/1810.04805.pdf), the **base** model consists of 12 _encoder-stacked_ layers and the **large** model consists of 24 _encoder-stacked_ layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the [Attention is all you need paper](https://arxiv.org/pdf/1706.03762.pdf):\n",
    "\n",
    "> The encoder is composed of a stack of $N = 6$ identical layers. Each layer has **two\n",
    "sub-layers**. The first is a **multi-head self-attention mechanism**, and the second is a simple, **position wise fully connected feed-forward network**. We employ a [residual connection](https://arxiv.org/abs/1512.03385) around **each** of the two sub-layers, followed by [layer normalization](https://arxiv.org/abs/1607.06450)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/sublayers.jpg \"The encoder layer. Source: https://atcold.github.io/pytorch-Deep-Learning/en/week12/12-1/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Multi-Head Attention\n",
    "\n",
    "Basically, the multi head attention is a _type_ of an attention mechanism. It is basically a _concatenation_ of another type of attention, the _scaled dot_. Both mechanisms works together as represented in the following image:\n",
    "\n",
    "![](images/attention_specific.png \"(left) Scaled Dot-Product Attention followed by the Multi-Head Attention which consists of several attention layers running in parallel. Source: https://atcold.github.io/pytorch-Deep-Learning/en/week12/12-1/\")\n",
    "\n",
    "Here, _h_, or the number o attention heads (or layers) is equal to $12$ in the case of $\\text{BERT}_\\text{base}$ and $16$ in the case of  $\\text{BERT}_\\text{large}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Conections\n",
    "\n",
    "\n",
    "Each sublayer of the encoder stack contains a residual connection (the left curved arrow) added to the sublayer output before layer normalization. The [idea of Residual Conections](https://arxiv.org/pdf/1512.03385.pdf) came from Computer Vision domain, and actually, it is a relatively simple technique that can be summarized by the following image:\n",
    "\n",
    "![](images/residual_connection.png \"Residual Connection example. Source (https://arxiv.org/pdf/1512.03385.pdf)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the image above and the case of Encoder stack, each $\\mathcal{F}(x)$ means either the _Multi-Head Attention_ or _Feed Forward_. Therefore, quoting the paper:\n",
    "\n",
    "> That is, the **output of each sub-layer is LayerNorm(x + Sublayer(x))**, where Sublayer(x) is the function implemented by the sub-layer itself. To _facilitate these residual connections_, all sub-layers in the model, as well as the embedding\n",
    "layers, produce outputs of dimension $d_{model} = 512$ {% fn 5 %}.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "{{ 'In the case of [BERT model](https://arxiv.org/pdf/1810.04805.pdf), please have in mind that $N$ is either $12$ \\(BERT<sub>base</sub>\\) or $24$ (\\(BERT<sub>large</sub>\\) and _d<sub>model</sub>_ is 768 for BERT base and 1024 for BERT large' | fndetail: 5 }}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, what, in fact, is being encoded?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Representation\n",
    "\n",
    "The authors would like to make BERT to perform well in different downstream tasks such as _binary and multi lablel classification_; _language modeling_; _question and answering_; _named entity recognition_; _etc_. Therefore, they said the following:\n",
    "\n",
    "> our input representation is able to unambiguously represent both a single sentence and a pair of sentences\n",
    "(e.g., h Question, Answeri) in one token sequence. Throughout this work, a “sentence” can be an arbitrary span of contiguous text, rather than an actual linguistic sentence. A “sequence” refers to the input token sequence to BERT, which may be a single sentence or two sentences packed together\n",
    "\n",
    "In order to perform and create the sentence embeddings, [WordPiece tokenize is applied](https://arxiv.org/abs/1609.08144). Then, besides adding [CLS] token, pairs of sentence (e.g. sentence _A_ and _B_) are concatenated into a single sentence, being separated with a special token [SEP] (e.g. _A_ [SEP] _B_). Then, a learned embedding [explained in NSP section](#Next-Sentence-Prediction) that indicates if the token becomes to either A or B is also add to the token representation.\n",
    "\n",
    "Then:\n",
    "\n",
    "> For a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/token_embeddings.png \"BERT input representation. Source: https://arxiv.org/pdf/1810.04805.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT Pre Training\n",
    "\n",
    "The first part of BERT is a pre Training procedure that involved two objective functions\n",
    "\n",
    "## Masked Language Model (MLM)\n",
    "\n",
    "As we are feeding the whole sentence into the model, it is possible to say that the model is bidirectional and hence as we are trying to predict the next word in a sentence, it would has access to it! Then, the idea behind this task is pretty simple. We can directly quote from the [paper](https://arxiv.org/pdf/1810.04805.pdf):\n",
    "\n",
    "> Unfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirectional conditioning would allow each word to indirectly “see itself”, and the model could trivially\n",
    "predict the target word in a multi-layered context. \n",
    "\n",
    "> In order to train a deep bidirectional representation, we simply mask some percentage of the input\n",
    "tokens at random, and then predict those masked tokens. We refer to this procedure as a “masked LM” (MLM), although it is often referred to as a _Cloze task_ in the [literature](https://journals.sagepub.com/doi/abs/10.1177/107769905303000401). In this\n",
    "case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM.\n",
    "\n",
    "In the case of BERT model, 15% of each sentence were masked during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/mlm.png \"MLM task. Taken from here: http://jalammar.github.io/illustrated-bert/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Sentence Prediction (NSP)\n",
    "\n",
    "In order to learn relationships between pair of sentence (e.g. Question and Ansering tasks) the authors needed a different approach than plain Language Modeling. Then:\n",
    "\n",
    ">  In order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus. Specifically, when choosing the sentences A and B for each pretraining example, 50% of the time B is the actual next sentence that follows A (labeled as `IsNext`), and 50% of the time it is a random sentence from the corpus (labeled as `NotNext`). \n",
    "\n",
    "\n",
    "Once defined, both objected functions are used in BERT Pre training learning :)\n",
    "\n",
    "![](images/nsp.png \"Next Sentence Preiction. Taken from here: http://jalammar.github.io/illustrated-bert/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: The training loss is the sum of the mean masked LM (MLM) likelihood and the mean next sentence prediction (NSP) likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Important: You may have noticed but this training procedure **does not require labeling**. As we are using the raw text inputs to generate the _labels_ during training, e considerer this BERT Pre Training as a _self-surpervised_ model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting all together\n",
    "\n",
    "As we are dealing with **sentence** embeddings than **word** embeddings we need a clever way to, well, encode these sentences. Let's see how BERT do it:\n",
    "\n",
    "   - We first take a text as input\n",
    "   - We apply WordPiece Tokenizer\n",
    "   - We fed the input into the Encoder stack\n",
    "   - We train the network (Pre-Training step)\n",
    "   - For those familiar with _CNN_ we can say that [CLS] embedding works as a \"pooled\" representation ([ref](https://arxiv.org/pdf/2002.08909.pdf)) of the sentence and then can be used as a **contextual embedding feature**. Hence, it can be fed into a Neural Net to solve classification tasks!\n",
    "   - Depending on the downstreaming task (_Fine tuning task_) other token embeddings can be used as well\n",
    "   \n",
    " > Important: without the fine-tuning task, CLS vector is not a meaninful representation since it was trained with NSP ([ref](https://arxiv.org/pdf/1810.04805.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have tried to summarize a foward pass of BERT thorugh the following gif:\n",
    "\n",
    "![](images/media/videos/scene/720p30/TransformerEncoderExample.gif \"Entire Forward passing in BERT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working in Practice\n",
    "\n",
    "To show sentence embedding from BERT working, I usually rely on [Hugging Face's transformer library](https://huggingface.co/transformers/). Here, since the **Bert Model for Language Model** was trained already, I will be using the bare BERT Model without any specific head (e.g., `LanguageModeling head` or `Sentence Classification head`) on top of it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertModel,BertTokenizer, BertForPreTraining\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_0 = \"He will get scared\"\n",
    "sequence_1 = \"She will get the drinks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence 0 word2Id mapping: [101, 2002, 2097, 2131, 6015, 102]\n",
      "Sequence 1 word2Id mapping: [101, 2016, 2097, 2131, 1996, 8974, 102]\n"
     ]
    }
   ],
   "source": [
    "#collapse-hide\n",
    "sequence_0_w2id = tokenizer.encode(sequence_0) # we need to map words to id's :)\n",
    "sequence_1_w2id = tokenizer.encode(sequence_1)\n",
    "\n",
    "print(f\"Sequence 0 word2Id mapping: {sequence_0_w2id}\")\n",
    "print(f\"Sequence 1 word2Id mapping: {sequence_1_w2id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "sequence_0_embeddings = torch.tensor(sequence_0_w2id).unsqueeze(0)  # Batch size 1\n",
    "sequence_0_embeddings = model(sequence_0_embeddings, return_dict=True)[\n",
    "    \"last_hidden_state\"\n",
    "].detach().numpy()\n",
    "sequence_1_embeddings = torch.tensor(sequence_1_w2id).unsqueeze(0)  # Batch size 1\n",
    "sequence_1_embeddings = model(sequence_1_embeddings, return_dict=True)[\n",
    "    \"last_hidden_state\"\n",
    "].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 6, 768), (1, 7, 768))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_0_embeddings.shape, sequence_1_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the first dimension means the batch size, we can get rid of it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6, 768), (7, 768))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-hide\n",
    "sequence_0_embeddings=sequence_0_embeddings[0]\n",
    "sequence_1_embeddings=sequence_1_embeddings[0]\n",
    "sequence_0_embeddings.shape, sequence_1_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that this model generates one embedding for each word plus `CLS` and `SEP` tokens. This explains why sentence_0 and sentence_1 both start and end with the same token number! Let's perform some cool math to analyze some patterns :)\n",
    "\n",
    "First, let's analyze the similarity between CLS and token words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similatiry between CLS token and the average of\n",
      "'He will get scared' tokens: 0.29071152210235596\n"
     ]
    }
   ],
   "source": [
    "# collapse-hide\n",
    "CLS_TOKEN_0 = sequence_0_embeddings[0]\n",
    "CLS_TOKEN_WORDS_0 = np.mean(sequence_0_embeddings[[1, 2, 3, 4]], axis=0)\n",
    "print(\n",
    "    f\"Cosine Similatiry between CLS token and the average of\\n'{sequence_0}'\"\n",
    "    f\" tokens: {cosine_similarity(CLS_TOKEN_0.reshape(1, -1), CLS_TOKEN_WORDS_0.reshape(1, -1))[0][0]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similatiry between CLS token and the average of \n",
      "'She will get the drinks' tokens: 0.32392317056655884\n"
     ]
    }
   ],
   "source": [
    "# collapse-hide\n",
    "CLS_TOKEN_1 = sequence_1_embeddings[0]\n",
    "CLS_TOKEN_WORDS_1 = np.mean(sequence_1_embeddings[[1, 2, 3, 4]], axis=0)\n",
    "print(\n",
    "    f\"Cosine Similatiry between CLS token and the average of \\n'{sequence_1}'\"\n",
    "    f\" tokens: {cosine_similarity(CLS_TOKEN_1.reshape(1, -1), CLS_TOKEN_WORDS_1.reshape(1, -1))[0][0]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting since as stated by the paper, the CLS token _seems to be meaninfulless_. Then, let's analyze the similarity between the average tokens embeddings of each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similatiry between average of embedding tokens of\n",
      "'He will get scared'and 'She will get the drinks' tokens :0.6591895222663879\n"
     ]
    }
   ],
   "source": [
    "# hide-input\n",
    "print(\n",
    "    f\"Cosine Similatiry between average of embedding tokens of\\n'{sequence_0}'and '{sequence_1}'\"\n",
    "    f\" tokens :{cosine_similarity(CLS_TOKEN_WORDS_0.reshape(1, -1), CLS_TOKEN_WORDS_1.reshape(1, -1))[0][0]}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As expected**, despite the fact that _similar_ words were used, their contexts were totally different and therefore, their embeddings similarities were less than the plain word vectors :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Congratulations! You have learned the main concepts behind the BERT model :) Please stay tuned, tor future blog posts :) I intend adding distillation about some BERT fine tuning as well as dissecting it from scratch!\n",
    "\n",
    "However, if you want to have a higher level approach about how this works, I [highly recommend this blog post](https://huggingface.co/blog/how-to-train)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources that have inspired me\n",
    "\n",
    "Besides all other papers that I have referenced through this post, I would like to emphaisze the following:\n",
    "\n",
    "- http://jalammar.github.io/illustrated-bert/\n",
    "- https://jalammar.github.io/illustrated-transformer/\n",
    "- http://nlp.seas.harvard.edu/2018/04/03/attention.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Acknowledgments\n",
    "\n",
    "I would really like to appreciate the effort made by some colleagues that provided a fantastic technical review for this blog post :)\n",
    "\n",
    "\n",
    "In alphabetical order:\n",
    "\n",
    "- [Alan Barzilay](https://www.linkedin.com/in/alan-barzilay-58754855/)\n",
    "- [Alvaro Marques](https://www.linkedin.com/in/alvaro-marques-9a10aa131/)\n",
    "- [Igor Hoelscher](https://www.linkedin.com/in/ighoelscher/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.2 64-bit ('citadel')",
   "metadata": {
    "interpreter": {
     "hash": "0635a80df3c0ba425046ad05be36aa3494c9972f2ddd763659334119e4736704"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2-final"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}