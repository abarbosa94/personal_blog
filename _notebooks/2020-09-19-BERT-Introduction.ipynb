{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"BERT Introduction\"\n",
    "> \"Inspired by Jay Alamar Ilustrated BERT blog post, I have decided to explain it using my own words :)\"\n",
    "\n",
    "- toc: true\n",
    "- branch: master\n",
    "- author: Andre Barbosa\n",
    "- badges: true\n",
    "- hide_binder_badge: true\n",
    "- hide_colab_badge: true\n",
    "- comments: true\n",
    "- categories: [masters, nlp]\n",
    "- hide: false\n",
    "- search_exclude: false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A quick review\n",
    "\n",
    "I remember some day of 2016 while I was starting my carrer as a Data Scientist when I've stumped into [Chirs McCormick  blog about Word2Vec](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/). Honestly, I think that [Tomas Mikolov paper](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) was one of the most elegant and simple idea that I have ever found so far {% fn 1 %} :) \n",
    "\n",
    "{{ 'Fun Fact: Whereas nowadays [Miklov LinkedIn profile](https://www.linkedin.com/in/tomas-mikolov-59831188/?originalSubdomain=cz) points out that he has worked for Microsoft, Google and Facebook; another of W2V authors, [Ilya Sutskever](http://www.cs.toronto.edu/~ilya/) worked with some of the prestigious researchers in the recent AI area, such as [Geoffrey Hinton](https://www.cs.toronto.edu/~hinton/) and [Andrew Ng](https://www.andrewng.org/). Moreover, he is one of the founders of [Open AI](https://openai.com/)! ' | fndetail: 1 }}\n",
    "\n",
    "## What are Word Embeddings\n",
    "\n",
    "\n",
    "According to [Pytorch documentation](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html) an **Emnedding** can be defined as the following: \n",
    "\n",
    "   >A simple lookup table that stores embeddings of a fixed _dictionary_ and _size_.\n",
    "\n",
    "Then, we can interpret embeddings as a simple way to convert _integers_ into _vectors_ of a given size. Then, for **word embeddings**, we can interpret simply as words that are encoded as integers and then _these integers serves as inputs for a vector space.\n",
    "\n",
    "A have written some code with [manim](https://github.com/3b1b/manim) to illustrate this process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "import jupyter_manim\n",
    "from manimlib.imports import *\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "# %%manim -h\n",
    "# pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# %%manim EmbeddingText --low_quality\n",
    "\n",
    "# initial_text = \"The quick brown fox jumps over the lazy dog!!\"\n",
    "# post_process = initial_text.strip('!').lower().strip()\n",
    "# tokenized = post_process.split()\n",
    "# class EmbeddingText(Scene):\n",
    "#     def construct(self):\n",
    "#         title = TextMobject(\"Define some Text\")\n",
    "#         title.to_corner(UP + LEFT)\n",
    "#         first_step = TextMobject(initial_text)\n",
    "#         self.play(Write(title),\n",
    "#                   FadeInFrom(first_step, DOWN),\n",
    "#                  )\n",
    "#         self.wait(1)\n",
    "#         second_title = TextMobject(\"Preprocess it (optional)\")\n",
    "#         second_title.to_corner(UP + LEFT)\n",
    "#         second_step = TextMobject(post_process, color=BLUE)\n",
    "#         self.play(\n",
    "#             Transform(title,second_title),\n",
    "#             ReplacementTransform(first_step, second_step))\n",
    "        \n",
    "#         third_title = TextMobject(\"Tokenize it\")\n",
    "#         third_title.to_corner(UP + LEFT)\n",
    "#         first_arrow = Arrow(DOWN,3*DOWN,color=BLUE)\n",
    "#         first_arrow.next_to(second_step,DOWN)\n",
    "#         third_step = TextMobject(str(tokenized))\n",
    "#         third_step.next_to(first_arrow, DOWN)\n",
    "        \n",
    "#         self.play(GrowArrow(first_arrow))\n",
    "#         self.play(Transform(second_title,third_title), FadeOut(title), FadeIn(third_step))\n",
    "        \n",
    "\n",
    "#         fourth_step = TextMobject(str(tokenized))\n",
    "#         self.play(\n",
    "#                   FadeOut(second_step),\n",
    "#                   FadeOut(first_arrow),\n",
    "#                   FadeOut(third_step),\n",
    "#                   Transform(third_step, fourth_step))\n",
    "        \n",
    "#         fith_step_text = VGroup(\n",
    "#             TextMobject(\"the\"),\n",
    "#             TextMobject(\"quick\"),\n",
    "#             TextMobject(\"brown\"),\n",
    "#             TextMobject(\"fox\"),\n",
    "#             TextMobject(\"jumps\"),\n",
    "#             TextMobject(\"over\"),\n",
    "#             TextMobject(\"the\"),\n",
    "#             TextMobject(\"lazy\"),\n",
    "#             TextMobject(\"dog\"),\n",
    "#         ).arrange(DOWN, aligned_edge=LEFT)\n",
    "        \n",
    "#         self.play(ReplacementTransform(fourth_step, fith_step_text))\n",
    "        \n",
    "#         second_arrow = Arrow(RIGHT,3*RIGHT)\n",
    "#         second_arrow.next_to(fith_step_text, RIGHT)\n",
    "        \n",
    "#         fith_step_number = VGroup(\n",
    "#             TextMobject(\"0\"),\n",
    "#             TextMobject(\"1\"),\n",
    "#             TextMobject(\"2\"),\n",
    "#             TextMobject(\"3\"),\n",
    "#             TextMobject(\"4\"),\n",
    "#             TextMobject(\"5\"),\n",
    "#             TextMobject(\"0\"),\n",
    "#             TextMobject(\"6\"),\n",
    "#             TextMobject(\"7\"),\n",
    "#         ).arrange(DOWN, aligned_edge=LEFT)\n",
    "#         fith_step_number.next_to(second_arrow, RIGHT)\n",
    "#         fourth_title = TextMobject(\"Map each word to an Integer*\")\n",
    "#         second_line = TextMobject(\"*notice that both words\")\n",
    "#         third_line = TextMobject(\"the\", color=RED)\n",
    "#         fourth_line = TextMobject(\"  were mapped to number 0\")\n",
    "#         second_line.scale(.6)\n",
    "#         third_line.scale(.6)\n",
    "#         fourth_line.scale(.6)\n",
    "#         fourth_title.to_corner(UP + LEFT)\n",
    "#         #Position text\n",
    "#         second_line.next_to(fourth_title, DOWN)\n",
    "#         second_line.to_edge(LEFT)\n",
    "#         third_line.next_to(second_line, 0.8*RIGHT)\n",
    "#         fourth_line.to_edge(LEFT)\n",
    "#         fourth_line.next_to(second_line, 0.5*DOWN)\n",
    "#         self.wait()\n",
    "#         self.play(GrowArrow(second_arrow))\n",
    "#         self.play(Transform(third_title,fourth_title),\n",
    "#                   FadeOut(second_title),\n",
    "#                   FadeInFrom(second_line,DOWN),\n",
    "#                   FadeIn(third_line),\n",
    "#                   FadeIn(fourth_line),\n",
    "#                   FadeIn(fith_step_number))\n",
    "\n",
    "        \n",
    "#         sixth_step = VGroup(\n",
    "#             TextMobject(\"0\"),\n",
    "#             TextMobject(\"1\"),\n",
    "#             TextMobject(\"2\"),\n",
    "#             TextMobject(\"3\"),\n",
    "#             TextMobject(\"4\"),\n",
    "#             TextMobject(\"5\"),\n",
    "#             TextMobject(\"0\"),\n",
    "#             TextMobject(\"6\"),\n",
    "#             TextMobject(\"7\"),\n",
    "#         ).arrange(DOWN, aligned_edge=LEFT)\n",
    "        \n",
    "#         self.wait(2)\n",
    "#         self.play(FadeOut(fourth_line),\n",
    "#                    FadeOut(third_line),\n",
    "#                    FadeOut(second_line),\n",
    "#                    FadeOut(fith_step_text),\n",
    "#                   FadeOut(second_arrow),\n",
    "#                   Transform(fith_step_number, sixth_step))\n",
    "        \n",
    "#         seventh_step = VGroup(\n",
    "#             TextMobject(\"0\"),\n",
    "#             TextMobject(\"1\"),\n",
    "#             TextMobject(\"2\"),\n",
    "#             TextMobject(\"3\"),\n",
    "#             TextMobject(\"4\"),\n",
    "#             TextMobject(\"5\"),\n",
    "#             TextMobject(\"0\"),\n",
    "#             TextMobject(\"6\"),\n",
    "#             TextMobject(\"7\"),\n",
    "#         ).arrange(2*DOWN, aligned_edge=LEFT)\n",
    "#         seventh_step.next_to(sixth_step, 5*LEFT)\n",
    "        \n",
    "#         self.play(\n",
    "#           FadeOut(fith_step_number),\n",
    "#           FadeOut(third_title),\n",
    "#           FadeOut(fourth_title),\n",
    "#           Transform(sixth_step, seventh_step)\n",
    "#         )\n",
    "        \n",
    "#         third_arrow = Arrow(RIGHT,2*RIGHT)\n",
    "#         third_arrow.next_to(seventh_step, RIGHT)\n",
    "        \n",
    "#         embedding = torch.nn.Embedding(8,4)\n",
    "#         #single data \n",
    "#         input_data = torch.LongTensor([[0,1,2,3,4,5,0,6,7]])\n",
    "#         #get the first image batch\n",
    "#         emdedding = embedding(input_data).detach().numpy()[0].round(decimals=2).astype('str')\n",
    "        \n",
    "#         matrix_first = Matrix(emdedding)\n",
    "#         matrix_first.next_to(third_arrow, RIGHT)\n",
    "        \n",
    "#         fifth_title = TextMobject(\"Each integer becomes the index of a matrix*\")\n",
    "#         second_line = TextMobject(\"*Again, notice that both words\")\n",
    "#         third_line = TextMobject(\"the\", color=RED)\n",
    "#         fourth_line = TextMobject(\" were mapped to the same vector\")\n",
    "#         fifth_title.scale(.5)\n",
    "#         second_line.scale(.5)\n",
    "#         third_line.scale(.5)\n",
    "#         fourth_line.scale(.5)\n",
    "#         fifth_title.to_corner(UP + LEFT)\n",
    "#         second_line.to_edge(LEFT)\n",
    "#         second_line.next_to(fifth_title, DOWN)\n",
    "#         third_line.next_to(second_line, 0.8*RIGHT)\n",
    "#         fourth_line.to_edge(LEFT)\n",
    "#         fourth_line.next_to(second_line, 0.5*DOWN)\n",
    "#         self.play(GrowArrow(third_arrow))\n",
    "#         self.play(FadeIn(matrix_first),\n",
    "#                   FadeIn(fifth_title),\n",
    "#                   FadeInFrom(second_line,DOWN),\n",
    "#                   FadeIn(third_line),\n",
    "#                   FadeIn(fourth_line),)\n",
    "        \n",
    "#         matrix_second=Matrix(emdedding)\n",
    "#         self.wait(5)\n",
    "#         self.play(FadeOut(sixth_step),\n",
    "#                   FadeOut(third_arrow),\n",
    "#                   FadeOut(seventh_step),\n",
    "#                   FadeOut(fourth_line),\n",
    "#                    FadeOut(third_line),\n",
    "#                    FadeOut(second_line),\n",
    "#                   FadeOut(fifth_title),\n",
    "#                   Transform(matrix_first, matrix_second)\n",
    "#                  )\n",
    "#         self.wait(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/EmbeddingText.gif \"In this example, the embedding dimension is NxM, where N is the vocab size (8) and M is 4. The row representing word 'the' was duplicated for illustration purposes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then interpret each dimension as a single neuron of a hidden layer and then **these embedding numbers can be modified** from a learning algorithm through a neural network. This is the main motivation behind Word Embeddings algorithms such as [Word2Vec](https://patents.google.com/patent/US9037464B1/en); [GloVe](https://nlp.stanford.edu/projects/glove/) and [fastText](https://fasttext.cc/) {% fn 2 %} \n",
    "\n",
    "Nowadays, there are some libraries that provides already trained vectors based on a fixed and previously trained vocabulary. For instance, considerer the following [Spacy](https://spacy.io/models) code:\n",
    "\n",
    "{{ 'I am not going to cover word embeddings thourgh this blog post. If you are not familiarized with them, I highly recommend [this](http://jalammar.github.io/illustrated-word2vec/); [this](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/) and [this](https://www.youtube.com/watch?v=ASn7ExxLZws) as potential resources :)' | fndetail: 2 }}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coniderer the sentence 'The quick brown fox jumps over the lazy dog!!'\n",
      "'The' vector representation has size of 300. Its first five elements are: [ 0.27 -0.06 -0.19  0.02 -0.02]\n",
      "'quick' vector representation has size of 300. Its first five elements are: [-0.45  0.19 -0.25  0.47  0.16]\n",
      "'brown' vector representation has size of 300. Its first five elements are: [-0.37 -0.08  0.11  0.19  0.03]\n",
      "'fox' vector representation has size of 300. Its first five elements are: [-0.35 -0.08  0.18 -0.09 -0.45]\n",
      "'jumps' vector representation has size of 300. Its first five elements are: [-0.33  0.22 -0.35 -0.26  0.41]\n",
      "'over' vector representation has size of 300. Its first five elements are: [-0.3   0.01  0.04  0.1   0.12]\n",
      "'the' vector representation has size of 300. Its first five elements are: [ 0.27 -0.06 -0.19  0.02 -0.02]\n",
      "'lazy' vector representation has size of 300. Its first five elements are: [-0.35 -0.3  -0.18 -0.32 -0.39]\n",
      "'dog' vector representation has size of 300. Its first five elements are: [-0.4   0.37  0.02 -0.34  0.05]\n",
      "'!' vector representation has size of 300. Its first five elements are: [-0.27  0.34  0.22 -0.3  -0.06]\n",
      "'!' vector representation has size of 300. Its first five elements are: [-0.27  0.34  0.22 -0.3  -0.06]\n"
     ]
    }
   ],
   "source": [
    "#collapse-hide\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "print(\"Coniderer the sentence 'The quick brown fox jumps over the lazy dog!!'\")\n",
    "text = nlp(\"The quick brown fox jumps over the lazy dog!!\")\n",
    "for word in text:\n",
    "    print(f\"'{word.text}' vector representation has size of {word.vector.shape[0]}. Its first five elements are: {word.vector[:5].round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contains word representations that were trained on [Common Crawl data using GloVe algorithm](https://github.com/explosion/spacy-models/releases//tag/en_core_web_md-2.3.1). Different thant the example that I used at the beggining, the word '!' was encoded as well. Other interesting fact is that since GloVe probably passed thourgh a preprocessing step, both '_The_' and '_the_' got the same representation. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 values of word 'The' vector: [ 0.27 -0.06 -0.19  0.02 -0.02]\n",
      "First 5 values of word 'the' vector: [ 0.27 -0.06 -0.19  0.02 -0.02]\n"
     ]
    }
   ],
   "source": [
    "#collapse-hide\n",
    "print(f\"First 5 values of word 'The' vector: {nlp('The').vector[:5].round(2)}\")\n",
    "print(f\"First 5 values of word 'the' vector: {nlp('the').vector[:5].round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine different words to form the embedding of a phrase. According to [spacy documentation](https://spacy.io/usage/vectors-similarity#_title):\n",
    "> Models that come with built-in word vectors make them available as the Token.vector attribute. Doc.vector and Span.vector will default to an average of their token vectors. \n",
    "\n",
    "Then, the phrase the we are using as example has the following single representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 values of 'The quick brown fox jumps over the lazy dog!!': [-0.23  0.08 -0.03 -0.07 -0.02]\n"
     ]
    }
   ],
   "source": [
    "#hide_input\n",
    "print(f\"First 5 values of 'The quick brown fox jumps over the lazy dog!!': {text.vector[:5].round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limitations of Word Embeddings\n",
    "\n",
    "Despite the fact that Word Embeddings brings a lot of benefits in the realm of computational linguistics, it has some limitations. There is a linguistic phenomena called _polyseme_ where according to [wikipedia](https://en.wikipedia.org/wiki/Polysemy#:~:text=English%20has%20many%20polysemous%20words,a%20subset%20of%20the%20other.):\n",
    "> A polyseme is a word or phrase with different, but related senses.(...) English has many polysemous words. For example, the verb \"to get\" can mean \"procure\" (I'll get the drinks), \"become\" (she got scared), \"understand\" (I get it) etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So considering the example above, despite the fact that the verb has **different meaning** depending on the contexts, **it's word representation would always be the same**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 values of verb 'to get' vector: [ 0.03  0.12 -0.32  0.13  0.12]\n"
     ]
    }
   ],
   "source": [
    "#hide_input\n",
    "print(f\"First 5 values of verb 'to get' vector: {nlp('to get').vector[:5].round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, if we pick two phrases: `She got scared` and `She understand it`, we will get the following vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 values of verb 'She will get scared' vector: [-0.08  0.16 -0.22 -0.03  0.02]\n",
      "First 5 values of verb 'She will get the drinks' vector: [ 0.01  0.13 -0.04 -0.08  0.03]\n"
     ]
    }
   ],
   "source": [
    "text1 = nlp(\"She will get scared\")\n",
    "text2 = nlp(\"She will get the drinks\")\n",
    "\n",
    "print(f\"First 5 values of verb '{text1}' vector: {text1.vector[:5].round(2)}\")\n",
    "print(f\"First 5 values of verb '{text2}' vector: {text2.vector[:5].round(2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, if we take the cosine simlarity by taking the average of the word vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.88455245783805"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#collapse-hide\n",
    "text1.similarity(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates that both vectors would be a lot similar. However, the reason for that is the usage of _similar_ words, even considering that they were applied in different contexts! So there is the objective that BERT tries to solve.{% fn 3 %} \n",
    "\n",
    "\n",
    "\n",
    "{{ 'There are some BERT percursors such as [ELMo](https://allennlp.org/elmo); [ULMFit](https://arxiv.org/abs/1801.06146) and [Open AI Transformer](https://openai.com/blog/language-unsupervised/) that I am not going to cover here. Please reach out to [Illustrated BERT blog](http://jalammar.github.io/illustrated-bert/) to know more' | fndetail: 3 }}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "Useful resources for continuating\n",
    "- http://jalammar.github.io/illustrated-bert/\n",
    "- https://jalammar.github.io/illustrated-transformer/\n",
    "- http://nlp.seas.harvard.edu/2018/04/03/attention.html#decoder\n",
    "- https://github.com/malhotra5/Manim-Tutorial#Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masters",
   "language": "python",
   "name": "masters"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
