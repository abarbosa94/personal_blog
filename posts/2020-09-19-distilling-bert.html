<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.427">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Andre Barbosa">
<meta name="dcterms.date" content="2020-09-19">
<meta name="description" content="Step by step about its inner work from scratch :)">

<title>Andre Personal Blog :) - Distilling BERT Pre Training</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Andre Personal Blog :)</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Distilling BERT Pre Training</h1>
                  <div>
        <div class="description">
          Step by step about its inner work from scratch :)
        </div>
      </div>
                          <div class="quarto-categories">
                <div class="quarto-category">masters</div>
                <div class="quarto-category">nlp</div>
                <div class="quarto-category">knowledge-distill</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Andre Barbosa </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">September 19, 2020</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#a-quick-review" id="toc-a-quick-review" class="nav-link active" data-scroll-target="#a-quick-review">A quick review</a>
  <ul class="collapse">
  <li><a href="#what-are-word-embeddings" id="toc-what-are-word-embeddings" class="nav-link" data-scroll-target="#what-are-word-embeddings">What are Word Embeddings</a></li>
  <li><a href="#limitations-of-word-embeddings" id="toc-limitations-of-word-embeddings" class="nav-link" data-scroll-target="#limitations-of-word-embeddings">Limitations of Word Embeddings</a></li>
  </ul></li>
  <li><a href="#bert-model" id="toc-bert-model" class="nav-link" data-scroll-target="#bert-model">BERT Model</a>
  <ul class="collapse">
  <li><a href="#attention-is-all-you-need" id="toc-attention-is-all-you-need" class="nav-link" data-scroll-target="#attention-is-all-you-need">Attention is all you need</a>
  <ul class="collapse">
  <li><a href="#attention" id="toc-attention" class="nav-link" data-scroll-target="#attention">Attention?</a></li>
  <li><a href="#key-value-store" id="toc-key-value-store" class="nav-link" data-scroll-target="#key-value-store">Key-Value Store</a></li>
  <li><a href="#positional-encoding" id="toc-positional-encoding" class="nav-link" data-scroll-target="#positional-encoding">Positional Encoding</a></li>
  </ul></li>
  <li><a href="#the-bert-model" id="toc-the-bert-model" class="nav-link" data-scroll-target="#the-bert-model">The BERT model</a>
  <ul class="collapse">
  <li><a href="#the-multi-head-attention" id="toc-the-multi-head-attention" class="nav-link" data-scroll-target="#the-multi-head-attention">The Multi-Head Attention</a></li>
  <li><a href="#residual-conections" id="toc-residual-conections" class="nav-link" data-scroll-target="#residual-conections">Residual Conections</a></li>
  </ul></li>
  <li><a href="#embedding-representation" id="toc-embedding-representation" class="nav-link" data-scroll-target="#embedding-representation">Embedding Representation</a></li>
  </ul></li>
  <li><a href="#bert-pre-training" id="toc-bert-pre-training" class="nav-link" data-scroll-target="#bert-pre-training">BERT Pre Training</a>
  <ul class="collapse">
  <li><a href="#masked-language-model-mlm" id="toc-masked-language-model-mlm" class="nav-link" data-scroll-target="#masked-language-model-mlm">Masked Language Model (MLM)</a></li>
  <li><a href="#next-sentence-prediction-nsp" id="toc-next-sentence-prediction-nsp" class="nav-link" data-scroll-target="#next-sentence-prediction-nsp">Next Sentence Prediction (NSP)</a></li>
  </ul></li>
  <li><a href="#putting-all-together" id="toc-putting-all-together" class="nav-link" data-scroll-target="#putting-all-together">Putting all together</a></li>
  <li><a href="#working-in-practice" id="toc-working-in-practice" class="nav-link" data-scroll-target="#working-in-practice">Working in Practice</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  <li><a href="#resources-that-have-inspired-me" id="toc-resources-that-have-inspired-me" class="nav-link" data-scroll-target="#resources-that-have-inspired-me">Resources that have inspired me</a></li>
  <li><a href="#acknowledgments" id="toc-acknowledgments" class="nav-link" data-scroll-target="#acknowledgments">Acknowledgments</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>For a <strong>portuguese</strong> version of this post, please check <a href="https://abarbosa94.github.io/personal_blog/masters/nlp/knowledge-distill/2020/09/19/Distilling-BERT-pt.html">this</a></p>
</div>
</div>
<section id="a-quick-review" class="level1">
<h1>A quick review</h1>
<p>I remember someday of 2016 while I was starting my career as a Data Scientist when I’ve stumped into <a href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/">Chirs McCormick blog about Word2Vec</a>. Honestly, I think that <a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Tomas Mikolov paper</a> was one of the most elegant and simple idea that I have ever found so far {% fn 1 %} :)</p>
<p>{{ ‘Fun Fact: Whereas nowadays <a href="https://www.linkedin.com/in/tomas-mikolov-59831188/?originalSubdomain=cz">Miklov LinkedIn profile</a> points out that he has worked for Microsoft, Google and Facebook; another of W2V authors, <a href="http://www.cs.toronto.edu/~ilya/">Ilya Sutskever</a> worked with some of the prestigious researchers in the recent AI area, such as <a href="https://www.cs.toronto.edu/~hinton/">Geoffrey Hinton</a> and <a href="https://www.andrewng.org/">Andrew Ng</a>. Moreover, he is one of the founders of <a href="https://openai.com/">Open AI</a>!’ | fndetail: 1 }}</p>
<section id="what-are-word-embeddings" class="level2">
<h2 class="anchored" data-anchor-id="what-are-word-embeddings">What are Word Embeddings</h2>
<p>According to <a href="https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html">Pytorch documentation</a> an <strong>Embedding</strong> can be defined as the following:</p>
<blockquote class="blockquote">
<p>A simple lookup table (…) of a fixed <em>dictionary</em> and <em>size</em>.</p>
</blockquote>
<p>Then, we can interpret embeddings as a simple way to convert <em>integers</em> into <em>vectors</em> of a given size. Then, for <strong>word embeddings</strong>, we can interpret simply as words that are encoded as integers, and then <em>these</em> integers serve as inputs for a vector space.’</p>
<p>A have written some code with <a href="https://github.com/3b1b/manim">manim</a> to illustrate this process:</p>
<p><img src="images/media/videos/scene/720p30/EmbeddingExample.gif" title="In this example, the embedding dimension is NxM, where N is the vocab size (8) and M is 4." class="img-fluid"></p>
<p>We can then interpret each dimension as a single neuron of a hidden layer, and then <strong>these embedding numbers can be modified</strong> from a learning algorithm through a neural network. This is the main motivation behind Word Embeddings algorithms such as <a href="https://patents.google.com/patent/US9037464B1/en">Word2Vec</a> and <a href="https://fasttext.cc/">fastText</a> {% fn 2 %}</p>
<p>Nowadays, there are some libraries that provide already trained vectors based on a fixed and previously trained vocabulary. For instance, considerer the following <a href="https://spacy.io/models">Spacy</a> code:</p>
<p>{{ ‘I am not going to cover word embeddings through this blog post. If you are not familiarized with them, I highly recommend <a href="http://jalammar.github.io/illustrated-word2vec/">this</a>; <a href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/">this</a> and <a href="https://www.youtube.com/watch?v=ASn7ExxLZws">this</a> as potential resources :)’ | fndetail: 2 }}</p>
<div class="cell" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> spacy</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>nlp <span class="op">=</span> spacy.load(<span class="st">"en_core_web_md"</span>)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Coniderer the sentence 'The quick brown fox jumps over the lazy dog!!'"</span>)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> nlp(<span class="st">"The quick brown fox jumps over the lazy dog!!"</span>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> word <span class="kw">in</span> text:</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>        <span class="ss">f"'</span><span class="sc">{</span>word<span class="sc">.</span>text<span class="sc">}</span><span class="ss">' vector representation has size of </span><span class="sc">{</span>word<span class="sc">.</span>vector<span class="sc">.</span>shape[<span class="dv">0</span>]<span class="sc">}</span><span class="ss">. Its first five elements are: </span><span class="sc">{</span>word<span class="sc">.</span>vector[:<span class="dv">5</span>]<span class="sc">.</span><span class="bu">round</span>(<span class="dv">2</span>)<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Coniderer the sentence 'The quick brown fox jumps over the lazy dog!!'
'The' vector representation has size of 300. Its first five elements are: [ 0.27 -0.06 -0.19  0.02 -0.02]
'quick' vector representation has size of 300. Its first five elements are: [-0.45  0.19 -0.25  0.47  0.16]
'brown' vector representation has size of 300. Its first five elements are: [-0.37 -0.08  0.11  0.19  0.03]
'fox' vector representation has size of 300. Its first five elements are: [-0.35 -0.08  0.18 -0.09 -0.45]
'jumps' vector representation has size of 300. Its first five elements are: [-0.33  0.22 -0.35 -0.26  0.41]
'over' vector representation has size of 300. Its first five elements are: [-0.3   0.01  0.04  0.1   0.12]
'the' vector representation has size of 300. Its first five elements are: [ 0.27 -0.06 -0.19  0.02 -0.02]
'lazy' vector representation has size of 300. Its first five elements are: [-0.35 -0.3  -0.18 -0.32 -0.39]
'dog' vector representation has size of 300. Its first five elements are: [-0.4   0.37  0.02 -0.34  0.05]
'!' vector representation has size of 300. Its first five elements are: [-0.27  0.34  0.22 -0.3  -0.06]
'!' vector representation has size of 300. Its first five elements are: [-0.27  0.34  0.22 -0.3  -0.06]</code></pre>
</div>
</div>
<p>Contains word representations that were trained on <a href="https://github.com/explosion/spacy-models/releases//tag/en_core_web_md-2.3.1">Common Crawl data using GloVe algorithm</a>. Unlike the example that I used initially, the word ‘!’ was encoded as well.</p>
<p>We can combine different words to form the embedding of a phrase. According to <a href="https://spacy.io/usage/vectors-similarity#_title">spacy documentation</a>: &gt; Models that come with built-in word vectors make them available as the Token.vector attribute. Doc.vector and Span.vector will default to an average of their token vectors.</p>
<p>Then, the phrase the we are using as example has the following single representation:</p>
<div class="cell" data-execution_count="3">
<div class="cell-output cell-output-stdout">
<pre><code>First 5 values of 'The quick brown fox jumps over the lazy dog!!': [-0.23  0.08 -0.03 -0.07 -0.02]</code></pre>
</div>
</div>
</section>
<section id="limitations-of-word-embeddings" class="level2">
<h2 class="anchored" data-anchor-id="limitations-of-word-embeddings">Limitations of Word Embeddings</h2>
<p>Even though Word Embeddings brings many benefits in the realm of computational linguistics, they have some limitations. There is a linguistic phenomenon called <em>polysemy</em>. According to <a href="https://en.wikipedia.org/wiki/Polysemy#:~:text=English%20has%20many%20polysemous%20words,a%20subset%20of%20the%20other.">wikipedia</a>: &gt; A polyseme is a word or phrase with different, but related senses.(…) English has many polysemous words. For example, the verb “to get” can mean “procure” (I’ll get the drinks), “become” (she got scared), “understand” (I get it) etc.</p>
<p>So considering the example above, despite the fact that the verb has <strong>different meaning</strong> depending on the contexts, <strong>it’s word representation would always be the same</strong></p>
<div class="cell" data-execution_count="4">
<div class="cell-output cell-output-stdout">
<pre><code>First 5 values of verb 'to get' vector: [ 0.03  0.12 -0.32  0.13  0.12]</code></pre>
</div>
</div>
<p>Then, if we pick two phrases: <code>She got scared</code> and <code>She understand it</code>, we will get the following vectors</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>text1 <span class="op">=</span> nlp(<span class="st">"He will get scared"</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>text2 <span class="op">=</span> nlp(<span class="st">"She will get the drinks"</span>)</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"First 5 values of sentence '</span><span class="sc">{</span>text1<span class="sc">}</span><span class="ss">' vector: </span><span class="sc">{</span>text1<span class="sc">.</span>vector[:<span class="dv">5</span>]<span class="sc">.</span><span class="bu">round</span>(<span class="dv">2</span>)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"First 5 values of sentence '</span><span class="sc">{</span>text2<span class="sc">}</span><span class="ss">' vector: </span><span class="sc">{</span>text2<span class="sc">.</span>vector[:<span class="dv">5</span>]<span class="sc">.</span><span class="bu">round</span>(<span class="dv">2</span>)<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>First 5 values of verb 'He will get scared' vector: [-0.12  0.19 -0.21 -0.14  0.09]
First 5 values of verb 'She will get the drinks' vector: [ 0.01  0.13 -0.04 -0.08  0.03]</code></pre>
</div>
</div>
<p>Then, if we take the cosine similarity by taking the average of the word vectors:</p>
<div class="cell" data-execution_count="6">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics.pairwise <span class="im">import</span> cosine_similarity</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    <span class="ss">f"Similarity between:</span><span class="ch">\n</span><span class="ss"> '</span><span class="sc">{</span>text1<span class="sc">}</span><span class="ss">' and '</span><span class="sc">{</span>text2<span class="sc">}</span><span class="ss">': "</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    <span class="ss">f"</span><span class="sc">{</span>cosine_similarity(text1.vector.reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>),text2.vector.reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>))[<span class="dv">0</span>][<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Simlarity between:
 'He will get scared' and 'She will get the drinks': 0.8653444051742554</code></pre>
</div>
</div>
<p>This indicates that both vectors would be a lot similar. However, the reason for that is the usage of <em>similar</em> words, even considering that they were applied in different contexts! So there is the objective that BERT tries to solve.{% fn 3 %}</p>
<p>{{ ‘There are some BERT percursors such as <a href="https://allennlp.org/elmo">ELMo</a>; <a href="https://arxiv.org/abs/1801.06146">ULMFit</a> and <a href="https://openai.com/blog/language-unsupervised/">Open AI Transformer</a> that I am not going to cover here. Please reach out to <a href="http://jalammar.github.io/illustrated-bert/">Illustrated BERT blog</a> to know more’ | fndetail: 3 }}</p>
</section>
</section>
<section id="bert-model" class="level1">
<h1>BERT Model</h1>
<section id="attention-is-all-you-need" class="level2">
<h2 class="anchored" data-anchor-id="attention-is-all-you-need">Attention is all you need</h2>
<p>The <a href="https://arxiv.org/abs/1706.03762">Attention is all you need</a> paper have introduced the Transformer architeture for us :) In sense, it can be summarized as the picture below:</p>
<p><img src="images/transformer.png" title="The transformer- model architeture, taken from: https://arxiv.org/abs/1706.03762" class="img-fluid"></p>
<p>Strictly speaking, the motivation behind the paper is that <em>RNN</em>-like architetures are memory-expensive. The purpose behind Transformer models is that it you can achieve similar results using more computer efficient resources by applying <strong>just attention mechanisms</strong> (and exluding the CNN or RNN-like architetures) !{% fn 4 %} Despite the fact that the Transformer model was proposed to deal with translation problems, it turns out that we can also use variations of it to achieve awesome results in different tasks. This is the <strong>motivation behind BERT</strong>!</p>
<p>{{ ‘<a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">The NLP group from Harvard</a> has written a great blog post distilling the paper as well as implementing them in pytorch. If you have some interest in knowing details about the transformer architecture, I recommend looking at it!’ | fndetail: 4 }}</p>
<section id="attention" class="level3">
<h3 class="anchored" data-anchor-id="attention">Attention?</h3>
<p>According to the <a href="https://atcold.github.io/pytorch-Deep-Learning/en/week12/12-3/">Transformer and Attention lecture from NYU foundations of Deep Learning Course</a>:</p>
<blockquote class="blockquote">
<p>Transformers are made up of attention modules, which are mappings between sets, rather than sequences, which means we do not impose an ordering to our inputs/outputs.</p>
</blockquote>
<p>When we analyze the transformer architeture, we can see that both <em>Multi-Head Attention</em> and <em>Multi-Head Masked Attention</em> box have 3 Arrow Heads. Each one represents one of the following:</p>
<ul>
<li><em>Q</em> that stands for <strong>query</strong> vector with dimension <span class="math inline">\(d_k\)</span></li>
<li><em>K</em> that stands for <strong>key</strong> vector that also has dimension <span class="math inline">\(d_k\)</span></li>
<li><em>V</em> that stands for <strong>value</strong> vector that also has dimension <span class="math inline">\(d_v\)</span></li>
</ul>
<p><strong>KV</strong> pair can be understood as the encoded representation of the input whereas the <strong>Q</strong> is the output of a previous layer.</p>
</section>
<section id="key-value-store" class="level3">
<h3 class="anchored" data-anchor-id="key-value-store">Key-Value Store</h3>
<p>Again, from the <a href="https://atcold.github.io/pytorch-Deep-Learning/en/week12/12-3/">Deep Learning Foundations Course from NYU</a>:</p>
<blockquote class="blockquote">
<p>A key-value store is a paradigm designed for storing (saving), retrieving (querying), and managing associative arrays (dictionaries/hash tables)</p>
</blockquote>
<blockquote class="blockquote">
<p>For example, say we wanted to find a recipe to make lasagne. We have a recipe book and search for “lasagne” - this is the query. This query is checked against all possible keys in your dataset - in this case, this could be the titles of all the recipes in the book. We check how aligned the query is with each title to find the maximum matching score between the query and all the respective keys. If our output is the argmax function - we retrieve the single recipe with the highest score. Otherwise, if we use a soft argmax function, we would get a probability distribution and can retrieve in order from the most similar content to less and less relevant recipes matching the query.</p>
</blockquote>
<blockquote class="blockquote">
<p>Basically, the query is the question. Given one query, we check this query against every key and retrieve all matching content.</p>
</blockquote>
<div class="callout callout-style-default callout-warning callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Warning
</div>
</div>
<div class="callout-body-container callout-body">
<p>I have decided not to cover attention concepts in this post, giving just a higher-level introduction. As you might have noticed, NYU Deep Learning Foundations Course provides a really nice introduction about the topic that I recommend going through if you want to learn more :)</p>
</div>
</div>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Attention can be basically understood as measure of correlation of words between a set of sentences. For those interested to learn a little bit more, I <em>highly</em> recommend <a href="https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html">this</a> blog post</p>
</div>
</div>
</section>
<section id="positional-encoding" class="level3">
<h3 class="anchored" data-anchor-id="positional-encoding">Positional Encoding</h3>
<p>This was taken from <a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html#prelims">The annotated transformer blog</a> where you can find a cool pytorch implementation. It turns out that actually this is a quote from Attention is all you need <a href="https://arxiv.org/pdf/1706.03762.pdf">paper</a>:</p>
<blockquote class="blockquote">
<p>Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add “positional encodings” to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension <span class="math inline">\(d_{model}\)</span> as the embeddings, so that the two can be summed</p>
</blockquote>
<p><img src="images/positional_encoding.png" title="One example of a positional encoding that generates sine wave based on length. Notice that each dimension generates a sine wave with different frequency. Source: http://nlp.seas.harvard.edu/2018/04/03/attention.html" class="img-fluid"></p>
</section>
</section>
<section id="the-bert-model" class="level2">
<h2 class="anchored" data-anchor-id="the-bert-model">The BERT model</h2>
<p>BERT model itself is an <em>encoder model</em> only from the transformer model. Considering the models trained from the <a href="https://arxiv.org/pdf/1810.04805.pdf">paper</a>, the <strong>base</strong> model consists of 12 <em>encoder-stacked</em> layers and the <strong>large</strong> model consists of 24 <em>encoder-stacked</em> layers.</p>
<p>According to the <a href="https://arxiv.org/pdf/1706.03762.pdf">Attention is all you need paper</a>:</p>
<blockquote class="blockquote">
<p>The encoder is composed of a stack of <span class="math inline">\(N = 6\)</span> identical layers. Each layer has <strong>two sub-layers</strong>. The first is a <strong>multi-head self-attention mechanism</strong>, and the second is a simple, <strong>position wise fully connected feed-forward network</strong>. We employ a <a href="https://arxiv.org/abs/1512.03385">residual connection</a> around <strong>each</strong> of the two sub-layers, followed by <a href="https://arxiv.org/abs/1607.06450">layer normalization</a>.</p>
</blockquote>
<p><img src="images/sublayers.jpg" title="The encoder layer. Source: https://atcold.github.io/pytorch-Deep-Learning/en/week12/12-1/" class="img-fluid"></p>
<section id="the-multi-head-attention" class="level3">
<h3 class="anchored" data-anchor-id="the-multi-head-attention">The Multi-Head Attention</h3>
<p>Basically, the multi head attention is a <em>type</em> of an attention mechanism. It is a <em>concatenation</em> of another type of attention, the <em>scaled dot</em>. Both mechanisms works together as represented in the following image:</p>
<p><img src="images/attention_specific.png" title="(left) Scaled Dot-Product Attention followed by the Multi-Head Attention which consists of several attention layers running in parallel. Source: https://atcold.github.io/pytorch-Deep-Learning/en/week12/12-1/" class="img-fluid"></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>Scaled Dot-Product Attention is calculated by <span class="math inline">\(softmax(\frac{QK^T}{\sqrt{n}})V\)</span>, where <em>K</em>, <em>V</em> and <em>Q</em> are the same as the ones described in a previous section whereas <em>n</em> represents the number of elements in the set.</p>
</div>
</div>
<p>Here, <em>h</em>, or the number o attention heads (or layers) is equal to <span class="math inline">\(12\)</span> in the case of <span class="math inline">\(\text{BERT}_\text{base}\)</span> and <span class="math inline">\(16\)</span> in the case of <span class="math inline">\(\text{BERT}_\text{large}\)</span></p>
</section>
<section id="residual-conections" class="level3">
<h3 class="anchored" data-anchor-id="residual-conections">Residual Conections</h3>
<p>Each sublayer of the encoder stack contains a residual connection (the left curved arrow) added to the sublayer output before layer normalization. The <a href="https://arxiv.org/pdf/1512.03385.pdf">idea of Residual Conections</a> came from Computer Vision domain, and actually, it is a relatively simple technique that can be summarized by the following image:</p>
<p><img src="images/residual_connection.png" title="Residual Connection example. Source (https://arxiv.org/pdf/1512.03385.pdf)" class="img-fluid"></p>
<p>Considering the image above and the case of Encoder stack, each <span class="math inline">\(\mathcal{F}(x)\)</span> means either the <em>Multi-Head Attention</em> or <em>Feed Forward</em>. Therefore, quoting the paper:</p>
<blockquote class="blockquote">
<p>That is, the <strong>output of each sub-layer is LayerNorm(x + Sublayer(x))</strong>, where Sublayer(x) is the function implemented by the sub-layer itself. To <em>facilitate these residual connections</em>, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension <span class="math inline">\(d_{model} = 512\)</span> {% fn 5 %}.</p>
</blockquote>
<p>{{ ‘In the case of <a href="https://arxiv.org/pdf/1810.04805.pdf">BERT model</a>, please have in mind that <span class="math inline">\(N\)</span> is either <span class="math inline">\(12\)</span> (BERT<sub>base</sub>) or <span class="math inline">\(24\)</span> ((BERT<sub>large</sub>) and <em>d<sub>model</sub></em> is 768 for BERT base and 1024 for BERT large’ | fndetail: 5 }}</p>
<p>Then, what, in fact, is being encoded?</p>
</section>
</section>
<section id="embedding-representation" class="level2">
<h2 class="anchored" data-anchor-id="embedding-representation">Embedding Representation</h2>
<p>The authors would like to make BERT to perform well in different downstream tasks such as <em>binary and multi lablel classification</em>; <em>language modeling</em>; <em>question and answering</em>; <em>named entity recognition</em>; <em>etc</em>. Therefore, they said the following:</p>
<blockquote class="blockquote">
<p>our input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., h Question, Answer) in one token sequence. Throughout this work, a “sentence” can be an arbitrary span of contiguous text, rather than an actual linguistic sentence. A “sequence” refers to the input token sequence to BERT, which may be a single sentence or two sentences packed together</p>
</blockquote>
<p>In order to perform and create the sentence embeddings, <a href="https://arxiv.org/abs/1609.08144">WordPiece tokenize is applied</a>. Then, besides adding [CLS] token, pairs of sentence (e.g.&nbsp;sentence <em>A</em> and <em>B</em>) are concatenated into a single sentence, being separated with a special token [SEP] (e.g.&nbsp;<em>A</em> [SEP] <em>B</em>).</p>
<p>Then:</p>
<blockquote class="blockquote">
<p>For a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings.</p>
</blockquote>
<p><img src="images/token_embeddings.png" title="BERT input representation. Source: https://arxiv.org/pdf/1810.04805.pdf" class="img-fluid"></p>
</section>
</section>
<section id="bert-pre-training" class="level1">
<h1>BERT Pre Training</h1>
<p>The first part of BERT is a pre Training procedure that involved <strong>two</strong> objective functions</p>
<section id="masked-language-model-mlm" class="level2">
<h2 class="anchored" data-anchor-id="masked-language-model-mlm">Masked Language Model (MLM)</h2>
<p>As we are feeding the whole sentence into the model, it is possible to say that the model is bidirectional and hence as we are trying to predict the next word in a sentence, it would has access to it! Then, the idea behind this task is pretty simple. We can directly quote from the <a href="https://arxiv.org/pdf/1810.04805.pdf">paper</a>:</p>
<blockquote class="blockquote">
<p>Unfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirectional conditioning would allow each word to indirectly “see itself”, and the model could trivially predict the target word in a multi-layered context.</p>
</blockquote>
<blockquote class="blockquote">
<p>In order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. We refer to this procedure as a “masked LM” (MLM), although it is often referred to as a <em>Cloze task</em> in the <a href="https://journals.sagepub.com/doi/abs/10.1177/107769905303000401">literature</a>. In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM.</p>
</blockquote>
<p>In the case of BERT model, 15% of each sentence were masked during training.</p>
<p><img src="images/mlm.png" title="MLM task. Taken from here: http://jalammar.github.io/illustrated-bert/" class="img-fluid"></p>
</section>
<section id="next-sentence-prediction-nsp" class="level2">
<h2 class="anchored" data-anchor-id="next-sentence-prediction-nsp">Next Sentence Prediction (NSP)</h2>
<p>In order to learn relationships between pair of sentence (e.g.&nbsp;Question and Ansering tasks) the authors needed a different approach than plain Language Modeling. Then:</p>
<blockquote class="blockquote">
<p>In order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus. Specifically, when choosing the sentences A and B for each pretraining example, 50% of the time B is the actual next sentence that follows A (labeled as <code>IsNext</code>), and 50% of the time it is a random sentence from the corpus (labeled as <code>NotNext</code>).</p>
</blockquote>
<p>Once defined, both objected functions are used in BERT Pre training learning :)</p>
<p><img src="images/nsp.png" title="Next Sentence Preiction. Taken from here: http://jalammar.github.io/illustrated-bert/" class="img-fluid"></p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>The training loss is the sum of the mean masked LM (MLM) likelihood and the mean next sentence prediction (NSP) likelihood</p>
</div>
</div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>You may have noticed but this training procedure <strong>does not require labeling</strong>. As we are using the raw text inputs to generate the <em>labels</em> during training, e considerer this BERT Pre Training as a <em>self-surpervised</em> model!</p>
</div>
</div>
</section>
</section>
<section id="putting-all-together" class="level1">
<h1>Putting all together</h1>
<p>As we are dealing with <strong>sentence</strong> embeddings than <strong>word</strong> embeddings we need a clever way to, well, encode these sentences. Let’s see how BERT do it:</p>
<ul>
<li>We first take a text as input</li>
<li>We apply WordPiece Tokenizer</li>
<li>We fed the input into the Encoder stack</li>
<li>We train the network (Pre-Training step)</li>
<li>For those familiar with <em>CNN</em> we can say that [CLS] embedding works as a “pooled” representation (<a href="https://arxiv.org/pdf/2002.08909.pdf">ref</a>) of the sentence and then can be used as a <strong>contextual embedding feature</strong>. Hence, it can be fed into a Neural Net to solve classification tasks!</li>
<li>Depending on the downstreaming task (<em>Fine tuning task</em>) other token embeddings can be used as well</li>
</ul>
<blockquote class="blockquote">
<p>Important: without the fine-tuning task, CLS vector is not a meaninful representation since it was trained with NSP (<a href="https://arxiv.org/pdf/1810.04805.pdf">ref</a>)</p>
</blockquote>
<p>I have tried to summarize a foward pass of BERT thorugh the following gif:</p>
<p><img src="images/media/videos/scene/720p30/TransformerEncoderExample.gif" title="Entire Forward passing in BERT" class="img-fluid"></p>
</section>
<section id="working-in-practice" class="level1">
<h1>Working in Practice</h1>
<p>To show sentence embedding from BERT working, I usually rely on <a href="https://huggingface.co/transformers/">Hugging Face’s transformer library</a>. Here, since the <strong>Bert Model for Language Model</strong> was trained already, I will be using the bare BERT Model without any specific head (e.g., <code>LanguageModeling head</code> or <code>Sentence Classification head</code>) on top of it!</p>
<div class="cell" data-execution_count="7">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> transformers <span class="im">import</span> BertModel,BertTokenizer, BertForPreTraining</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>tokenizer <span class="op">=</span> BertTokenizer.from_pretrained(<span class="st">"bert-base-uncased"</span>)</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> BertModel.from_pretrained(<span class="st">"bert-base-uncased"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>sequence_0 <span class="op">=</span> <span class="st">"He will get scared"</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>sequence_1 <span class="op">=</span> <span class="st">"She will get the drinks"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="9">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>sequence_0_w2id <span class="op">=</span> tokenizer.encode(sequence_0) <span class="co"># we need to map words to id's :)</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>sequence_1_w2id <span class="op">=</span> tokenizer.encode(sequence_1)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Sequence 0 word2Id mapping: </span><span class="sc">{</span>sequence_0_w2id<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f"Sequence 1 word2Id mapping: </span><span class="sc">{</span>sequence_1_w2id<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Sequence 0 word2Id mapping: [101, 2002, 2097, 2131, 6015, 102]
Sequence 1 word2Id mapping: [101, 2016, 2097, 2131, 1996, 8974, 102]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="10">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>sequence_0_embeddings <span class="op">=</span> torch.tensor(sequence_0_w2id).unsqueeze(<span class="dv">0</span>)  <span class="co"># Batch size 1</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>sequence_0_embeddings <span class="op">=</span> model(sequence_0_embeddings, return_dict<span class="op">=</span><span class="va">True</span>)[</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"last_hidden_state"</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>].detach().numpy()</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>sequence_1_embeddings <span class="op">=</span> torch.tensor(sequence_1_w2id).unsqueeze(<span class="dv">0</span>)  <span class="co"># Batch size 1</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>sequence_1_embeddings <span class="op">=</span> model(sequence_1_embeddings, return_dict<span class="op">=</span><span class="va">True</span>)[</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    <span class="st">"last_hidden_state"</span></span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>].detach().numpy()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>sequence_0_embeddings.shape, sequence_1_embeddings.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="11">
<pre><code>((1, 6, 768), (1, 7, 768))</code></pre>
</div>
</div>
<p>Since the first dimension means the batch size, we can get rid of it!</p>
<div class="cell" data-execution_count="12">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>sequence_0_embeddings<span class="op">=</span>sequence_0_embeddings[<span class="dv">0</span>]</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>sequence_1_embeddings<span class="op">=</span>sequence_1_embeddings[<span class="dv">0</span>]</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>sequence_0_embeddings.shape, sequence_1_embeddings.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="12">
<pre><code>((6, 768), (7, 768))</code></pre>
</div>
</div>
<p>It turns out that this model generates one embedding for each word plus <code>CLS</code> and <code>SEP</code> tokens. This explains why sentence_0 and sentence_1 both start and end with the same token number! Let’s perform some cool math to analyze some patterns :)</p>
<p>First, let’s analyze the similarity between CLS and token words</p>
<div class="cell" data-execution_count="13">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>CLS_TOKEN_0 <span class="op">=</span> sequence_0_embeddings[<span class="dv">0</span>]</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>CLS_TOKEN_WORDS_0 <span class="op">=</span> np.mean(sequence_0_embeddings[[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>]], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>    <span class="ss">f"Cosine Similatiry between CLS token and the average of</span><span class="ch">\n</span><span class="ss">'</span><span class="sc">{</span>sequence_0<span class="sc">}</span><span class="ss">'"</span></span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    <span class="ss">f" tokens: </span><span class="sc">{</span>cosine_similarity(CLS_TOKEN_0.reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>), CLS_TOKEN_WORDS_0.reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>))[<span class="dv">0</span>][<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Cosine Similatiry between CLS token and the average of
'He will get scared' tokens: 0.29071152210235596</code></pre>
</div>
</div>
<div class="cell" data-execution_count="14">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>CLS_TOKEN_1 <span class="op">=</span> sequence_1_embeddings[<span class="dv">0</span>]</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>CLS_TOKEN_WORDS_1 <span class="op">=</span> np.mean(sequence_1_embeddings[[<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">4</span>]], axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>    <span class="ss">f"Cosine Similatiry between CLS token and the average of </span><span class="ch">\n</span><span class="ss">'</span><span class="sc">{</span>sequence_1<span class="sc">}</span><span class="ss">'"</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>    <span class="ss">f" tokens: </span><span class="sc">{</span>cosine_similarity(CLS_TOKEN_1.reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>), CLS_TOKEN_WORDS_1.reshape(<span class="dv">1</span>, <span class="op">-</span><span class="dv">1</span>))[<span class="dv">0</span>][<span class="dv">0</span>]<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Cosine Similatiry between CLS token and the average of 
'She will get the drinks' tokens: 0.32392317056655884</code></pre>
</div>
</div>
<p>It is interesting since as stated by the paper, the CLS token <em>seems to be meaninfulless</em>. Then, let’s analyze the similarity between the average tokens embeddings of each sentence</p>
<div class="cell" data-execution_count="15">
<div class="cell-output cell-output-stdout">
<pre><code>Cosine Similatiry between average of embedding tokens of
'He will get scared'and 'She will get the drinks' tokens :0.6591895222663879</code></pre>
</div>
</div>
<p><strong>As expected</strong>, despite the fact that <em>similar</em> words were used, their contexts were totally different and therefore, their embeddings similarities were less than the plain word vectors :)</p>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>Congratulations! You have learned the main concepts behind the BERT model :) Please stay tuned, tor future blog posts :) I intend adding distillation about some BERT fine tuning as well as dissecting it from scratch!</p>
<p>However, if you want to have a higher level approach about how this works, I <a href="https://huggingface.co/blog/how-to-train">highly recommend this blog post</a>!</p>
</section>
<section id="resources-that-have-inspired-me" class="level1">
<h1>Resources that have inspired me</h1>
<p>Besides all other papers that I have referenced through this post, I would like to emphaisze the following:</p>
<ul>
<li>http://jalammar.github.io/illustrated-bert/</li>
<li>https://jalammar.github.io/illustrated-transformer/</li>
<li>http://nlp.seas.harvard.edu/2018/04/03/attention.html</li>
</ul>
</section>
<section id="acknowledgments" class="level1">
<h1>Acknowledgments</h1>
<p>I would really like to appreciate the effort made by some colleagues that provided a fantastic technical review for this blog post :)</p>
<p>In alphabetical order:</p>
<ul>
<li><a href="https://www.linkedin.com/in/alan-barzilay-58754855/">Alan Barzilay</a></li>
<li><a href="https://www.linkedin.com/in/alvaro-marques-9a10aa131/">Alvaro Marques</a></li>
<li><a href="https://www.linkedin.com/in/ighoelscher/">Igor Hoelscher</a></li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>