{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "aliases:\n",
    "- /masters/nlp/knowledge-distill/2021/02/21/Downstream-BERT-QA\n",
    "author: Andre Barbosa\n",
    "badges: true\n",
    "branch: master\n",
    "categories:\n",
    "- masters\n",
    "- nlp\n",
    "- knowledge-distill\n",
    "date: '2021-02-21'\n",
    "description: Step by step about a specific fine tuning task:)\n",
    "hide: false\n",
    "hide_binder_badge: false\n",
    "hide_colab_badge: false\n",
    "output-file: 2021-02-21-downstream-bert-qa.html\n",
    "search_exclude: false\n",
    "title: BERT applied to Multiple Choice\n",
    "toc: true\n",
    "bibliography: references.bib\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drilling down Multiple Choice downstream task\n",
    "\n",
    "\n",
    "When I started studying Language Models, I remember when I've found the following image from Open AI transformer paper [@Radford2018ImprovingLU]:\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/downstream-gpt.png \"Example of fine-tuning tasks from GPT paper\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the only difference is that the **input data** should be _slightly_ different:\n",
    "\n",
    "> For these tasks, we are given a context\n",
    "document $z$, a question $q$, and a set of possible answers ${a_k}$. We concatenate the document context\n",
    "and question with each possible answer, adding a delimiter token in between to get [$z$; $q$; _\\$_ ; $a_k$]. Each of these sequences are **processed independently with our model and then normalized via a softmax layer to produce an output distribution over possible answers**."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, these inputs could be optimized via [Categorical Cross Entropy Loss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html), where $C$ is the number of options available. For a specific question.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From GPT to BERT\n",
    "\n",
    "As we will see with [Hugging Face's transformer library](https://huggingface.co/transformers/), when we considerer application from a fine tuning task, the approach of BERT can be derived directly from the tecnique presented by [@Radford2018ImprovingLU].\n",
    "It is possible to check it from [documentation](https://huggingface.co/transformers/model_doc/bert.html#transformers.BertForMultipleChoice)\n",
    "\n",
    "> Bert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a softmax) e.g. for RocStories/SWAG tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 232k/232k [00:01<00:00, 171kB/s]\n",
      "Downloading: 100%|██████████| 433/433 [00:00<00:00, 122kB/s]\n",
      "Downloading: 100%|██████████| 440M/440M [02:06<00:00, 3.48MB/s]\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultipleChoice: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#| code-fold: true\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForMultipleChoice\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertForMultipleChoice.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"George wants to warm his hands quickly by rubbing them. Which skin surface will produce the most heat?\"\n",
    "option_a = \"dry palms\"\n",
    "option_b = \"wet palms\"\n",
    "option_c = \"palms covered with oil\"\n",
    "option_d = \"palms covered with lotion\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, option A is the correct one. Furthermore, the batch size here would be 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = torch.tensor(0).unsqueeze(0) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the question is the same for each option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer(\n",
    "            [question, question, question, question],\n",
    "            [option_a, option_b, option_c, option_d],\n",
    "            return_tensors='pt',\n",
    "            padding=True\n",
    "           )\n",
    "\n",
    "outputs = model(**{k: v.unsqueeze(0) for k,v in encoding.items()}, labels=labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{.callout-important}\n",
    "\n",
    "Notice that if we have a dataset such as SquaD where each question comes with a context, we could append this context to either the question text or the option text and we would then have the tuple cited by [Open AI transformer paper](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)\n",
    "\n",
    ":::\n",
    "\n",
    "The output is a linear layer which would still be trained through a Cross Entropy loss. Then, as stated by the documentation, we still need to apply softmax to the logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = outputs.loss\n",
    "logits = outputs.logits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Linear Logits output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3457, -0.3295, -0.3271, -0.3342]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "logits"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logits after the softmax function. Since this model did not learn anything, the result below is expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2471, 0.2511, 0.2518, 0.2500]], grad_fn=<SoftmaxBackward>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "\n",
    "torch.nn.functional.softmax(logits)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Congratulations! Adding up with the [first part](https://abarbosa94.github.io/personal_blog/masters/nlp/knowledge-distill/2020/09/19/Distilling-BERT.html), you have learned the end-to-end BERT Flow :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
