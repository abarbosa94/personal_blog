[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi there! My name is André Barbosa and I’m currently a Master Student@Institute of Mathemathics and Statistics as well as a Data Scientist@QuintoAndar, a real state company. My current reserch interests include the following:\n\nExplainable AI\nNatural Language Processing\nCausal Inference\n\nFell free to follow me on social media and contact me :)"
  },
  {
    "objectID": "posts/2023-07-08-annotated-diverse-cfe.html#the-caveat",
    "href": "posts/2023-07-08-annotated-diverse-cfe.html#the-caveat",
    "title": "Distilling Diverse Counterfactual Explanations",
    "section": "The caveat",
    "text": "The caveat\nComing up with “what-if” scenarios that someone can actually do is tough. Let’s go back to the loan example. A counterfactual explanation might suggest “lower your rent”, but it doesn’t give other options or consider how hard different changes might be to pull off. That’s why we need a variety of counterfactual examples to help people wrap their heads around these complex machine learning models. Ideally, these examples should offer a range of suggestions and think about how doable those changes are.\n\n\n\n\n\n\nImportant\n\n\n\nRules should make sense with real-world rules, like, you can’t just downgrade your degree or change your race.\n\n\n\nBut what is wrong with Feature Importance methods?\nModels like those mentioned in the (Ribeiro, Singh, and Guestrin 2016) and (Lundberg and Lee 2017) papers are seen as “Feature Importance” methods. They explain things by ranking the most relevant features. But here’s the thing, these explanations aren’t totally honest about what’s going on with the machine learning models. As there is always this balancing act between being true to the model and making it understandable to humans when you’re explaining a complex model, an explanation methods that use simpler stand-in models are always approximatios the real model to some extent. Therefore, one big issue with this is that because the explanations come from these simpler models, there’s no guarantee that they’re actually representing the original model accurately."
  },
  {
    "objectID": "posts/2023-07-08-annotated-diverse-cfe.html#approaching-the-solution",
    "href": "posts/2023-07-08-annotated-diverse-cfe.html#approaching-the-solution",
    "title": "Distilling Diverse Counterfactual Explanations",
    "section": "Approaching the solution",
    "text": "Approaching the solution\nExtending the work from (Wachter, Mittelstadt, and Russell 2017), (Mothilal, Sharma, and Tan 2020) authors set up an optimization problem that looks at both the diversity of the “what-if” scenarios and how close they are to the original data, coming up with a method that generates a bunch of “what-if” scenarios. Also, they provide a set of interesting metrics for evaluating counterfactual explanations, which I believe is worthwhile to explore :)"
  },
  {
    "objectID": "posts/2023-07-08-annotated-diverse-cfe.html#going-formal",
    "href": "posts/2023-07-08-annotated-diverse-cfe.html#going-formal",
    "title": "Distilling Diverse Counterfactual Explanations",
    "section": "Going Formal",
    "text": "Going Formal\nSo, what we’re working with here is a trained machine learning model, \\(f\\), and a specific case, \\(x\\). Our aim is to generate a group of \\(k\\) counterfactuals, {\\(c_1, c_2, \\cdots{. . .}, c_k\\)}, that would lead to a different outcome than \\(x\\). Both \\(x\\) and all the CF examples (\\({c_1, c_2, \\cdots{. . .} , c_k}\\)) are \\(d\\text{-dimensional}\\). Throughout this paper, we’re assuming that the machine learning model is differentiable and static (it doesn’t change over time), and that the output is binary.\nOur goal is to generate an actionable counterfactual set, that is, the user should be able to find CF examples that they can act upon. To do so, we need individual CF examples to be feasible with respect to the original input, but also need diversity among the generated counterfactuals to provide different ways of changing the outcome class.\n\nCounterfactuals Generation Engine\nLet’s say you have an input feature \\(x\\) and an output from an ML model \\(f\\). A counterfactual explanation is a tweak to the input that causes a different output \\(y\\) from the same model. Specifically, Wachter and his team came up with this idea:\n\\[\n\\begin{aligned}\nx_{cf} = \\arg \\min_{x_{cf}} \\quad & yloss(f(x_{cf}), y) + |x - x_{cf}| \\\\\n\\text{s.t.} \\quad & f(x_{cf}) = y', y' \\neq y\n\\end{aligned}\n\\]\nIn plain terms, we want the counterfactual example \\(x_{cf}\\) that minimizes the loss such that this instance stays relatively close to the original data point. Thefore, while the first component (yloss) drives the counterfactual \\(x_{cf}\\) towards a prediction that’s different from the original data point the second one (\\(x - x_{cf}\\)) ensures that the counterfactual stays relatively close to \\(x\\).\nFor simplicity, for now on, lets assume that \\(x_{cf}=c\\)\n\n\nNew set of metrics\nAs mentioned, authors from (Mothilal, Sharma, and Tan 2020) implements a new set of metrics and definitions that I am going to explain further. In other words, while having a bunch of different counterfactuals might boost the odds of finding at least one a person can act on, the examples could end up tweaking a ton of features. Or, they might go for maximum diversity by suggesting big changes from the original data. This issue could get even trickier when you’re dealing with lots of features. So, we need a mix of diversity and feasibility, which I’ll lay out next.\n\nDiversity via Determinantal Point Processes\nWhen it comes to diversity, we’re borrowing from something called determinantal point processes (Kulesza and Taskar 2012). These have been used to tackle subset selection problems with diversity constraints. We measure diversity with this determinant of a kernel matrix that is based on the counterfactuals:\n\\[\n\\begin{aligned}\n\\text{dpp\\_diversity}(x) = \\det(K(x))\n\\end{aligned}\n\\]\nHere, \\(K_{i,j} = \\frac{1}{1+dist(x_i,c_j)}\\), where \\(dist(c_i,c_j)\\) is a way to measure the distance between two counterfactual examples. In practice, to dodge issues with undefined determinants, we add small random changes to the diagonal elements when we calculate the determinant.\nIf we want to have quick and dirty implementation about that, here it is 1:\n\n\nCode\nimport numpy as np\nimport scipy.spatial.distance as distance\nimport torch\nimport random\n\nseed = 42\ntorch.manual_seed(seed)\nnp.random.seed(seed)\nrandom.seed(seed)\ntorch.use_deterministic_algorithms(True)\n\n\ndef create_k_matrix(counterfactuals, distance_func, training_data, eps=1e-6):\n    \"\"\"\n    Creates a kernel matrix K based on counterfactuals and a given distance function.\n    \n    Parameters:\n    counterfactuals (list(torch.Tensor)): A list of counterfactual candidates\n    distance_func (function): A function to measure distance between two counterfactual examples.\n    eps (float): A small value added to the diagonal of the matrix to avoid determinant issues.\n    \n    Returns:\n    torch.Tensor: The kernel matrix K.\n    \"\"\"\n    \n    num_counterfactuals = len(counterfactuals)\n    K = torch.zeros((num_counterfactuals, num_counterfactuals))\n    \n    for i in range(num_counterfactuals):\n        for j in range(i+1, num_counterfactuals):\n            dist = distance_func(counterfactuals[i], counterfactuals[j], training_data)\n            K[i, j] = K[j, i] = 1 / (1 + dist)\n    \n    # Add small perturbations to the diagonal elements\n    # Clone the tensor first if gradient computation is needed\n    K = K.clone() \n    K.diagonal().add_(eps)\n    \n    return K\n\n\n\n\nProximity\nSo, the counterfactual examples that are closest to the original data are usually the most helpful for users. We calculate proximity as the (negative) vector distance between the original data and the features of the counterfactual example. This can be figured out using a distance metric like the \\(\\ell_{1}\\text{-distance}\\) (and you can weight this by feature if you want). The proximity of a set of counterfactual examples is just the average proximity across the set.\n\\[\n\\begin{aligned}\n-\\frac{1}{k}\\sum^{k}_{i=1}\\text{dist}(c_i,x)\n\\end{aligned}\n\\]\nAs expected, here is an attempt of implementation\n\n\nCode\nfrom scipy.spatial import distance\n\ndef compute_proximity(x, c, dist_function='euclidean'):\n    # Calculate the distance between x and each counterfactual in c\n    distances = np.array([distance.cdist([x], [c_i], dist_function) for c_i in c])\n\n    # Calculate the average proximity across the set\n    proximity = -np.mean(distances)\n\n    return proximity\n\n\n\n\nSparsity\nClosely linked to proximity is the concept of sparsity: it’s about determining the number of features someone needs to alter to reach the counterfactual class. It’s intuitive that a counterfactual example will be more feasible if it modifies fewer features. As this constraint is non-convex (it doesn’t have a simple, consistent shape), we don’t include it in the loss function. Instead, we manage it by modifying the counterfactuals we generate.\n\\[\n\\begin{aligned}\n1 - \\frac{1}{kd}\\sum^{k}_{i=1}\\sum^{d}_{l=1}\\mathbb{1}_[c^l_i \\neq x^l_i]\n\\end{aligned}\n\\]\n\n\nCode\ndef sparsity(x, c):\n    \"\"\"\n    Calculate the sparsity based on the given equation.\n\n    Parameters:\n    x (list): Input features.\n    c (list): Set of counterfactuals. It should be a 2D list where each inner list is a counterfactual.\n\n    Returns:\n    float: Sparsity value.\n    \"\"\"\n    k = len(c)  # Number of counterfactuals\n    d = len(x)  # Number of input features\n    assert all(len(cf) == d for cf in c), \"The size of input features and counterfactuals should match.\"\n\n    # Calculate the sum of the indicator function\n    indicator_sum = 0\n    for cf in c:\n        for xi, cfi in zip(x, cf):\n            if xi != cfi:\n                indicator_sum += 1\n\n    # Calculate the sparsity\n    sparsity = 1 - (1 / (k * d)) * indicator_sum\n\n    return sparsity\n\n\n\n\nUser Constraints\nA counterfactual example might seem feasible if you’re just looking at the features, but real-world restrictions could make it a no-go. So, it’s a good idea to let users set some constraints on how features can be changed. These can be defined in two ways. First, as box constraints that set feasible ranges for each feature—this is where we need to look for counterfactual examples. Like, “income can’t go above 200,000”. Or, a user could just list the variables they’re okay with changing.\n\n\n\nOptimizing everything\nThe authors then propose a combined loss function, \\(C(x)\\), that considers all generated counterfactuals:\n\\[\\begin{aligned}\nC(x) = \\arg \\min_{c_1, \\cdots, c_k} \\quad & \\frac{1}{k} \\sum^{k}_{i=1} yloss(f(c_i), y) + \\frac{\\lambda_{1}}{k} \\sum^{k}_{i=1} dist(c_i, x) - \\lambda_{2} \\text{dpp\\_diversity}(c_1, \\cdots, c_k)\n\\end{aligned}\\]\nIn this equation, \\(c_i\\) represents a counterfactual example (CF), \\(k\\) is the total number of CFs we want to make, \\(f(\\cdot)\\) is the ML model, \\(yloss(\\cdot)\\) is a metric that shrinks the distance between \\(f(\\cdot)\\)’s prediction for \\(c_i\\) and the preferred outcome \\(y\\), \\(x\\) is the initial input, and \\(\\text{dpp\\_diversity}(\\cdot)\\) is the diversity metric. \\(\\lambda1\\) and \\(\\lambda_2\\) are hyperparameters.\nSince that \\(y\\) is the preferable class could also be the opposite of \\(f(x)\\), so we can use the model’s prediction for the instance we want to explain instead of its label, but then we would need to flip its result. Moreover, author’s state that they initialize \\(c_i\\) randomly and optimize it via gradient descent.\nOne way to see this is that given a \\(c_i\\) random array with the same \\(d\\) dimensions of \\(x\\) and we want to learn the best set of \\(k\\) \\(c\\) arrays, we can “think” of this as some sort of single layer neural network, with \\(x\\) and \\(f(x)\\) being the label. For example, if \\(k=1\\), we are optmizing some sort of single “hidden layer” so that in the end we would have the counterfactuals.\n\n\n\n\n\n\nImportant\n\n\n\nAs the loss is going to be optimized via gradient descent and we want to learn the counterfactuals through it as we are passing it through \\(f(c_i)\\), the model inference function has to be differentiable. This, the blackbox model has to be something like a Neural Network or Logistic Function but not a boosted decision tree or a random forest, as decision trees are not differentiable\n\n\nLet’s breakdown each element of this loss function:\n\n\\(yloss(f(c\\_i), y)\\): this part will try to make \\(f(c_i)\\) as close as possible to \\(y\\), that is the desired counterfactual\n\\(dist(c_i, x)\\): will make sure that \\(c_i\\) is close to \\(x\\)\n\\(\\text{dpp\\_diversity}(c_1, \\cdots, c_k)\\): this part will try to diversify \\((c_1, \\cdots, c_k)\\)\n\n\n\n\n\n\n\nNote\n\n\n\nThe authors suggest that we implement a stop criteria based on the following rulles: - The loss stops improving at some threshold - The class found is the counterfactual one\n\n\nHere is pytorch for implementing this.\n\nimport torch\nfrom torch.autograd import Variable\nimport numpy as np\nfrom tqdm.auto import tqdm\n\ndef detect_convergence(model_classifier, current_loss, previous_loss, y, counterfactuals, threshold = 1e-3):\n    # Calculate the change in loss\n    change_in_loss = torch.abs(current_loss - previous_loss)\n    \n    # Check if the instance_to_analyze's prediction is different from all counterfactuals\n    y_diff_all = all([y != model_classifier.predict(c_i) for c_i in counterfactuals])\n    # Check if the change in loss is less than threshold and y is different from all counterfactuals\n    if change_in_loss &lt; threshold and y_diff_all:\n        print(f\"Loss converged at: {change_in_loss}, stop criteria reached\")\n        return True\n    return False\n\n\ndef learn_counterfactuals(instance_to_analyze, \n                            lambda_1, lambda_2, yloss, \n                            distance_function, model_classifier,\n                            k, training_dataset, num_epochs=100, learning_rate=0.01):\n    # Convert inputs to PyTorch tensors\n    counterfactual_candidates = [torch.randn_like(instance_to_analyze, requires_grad=True) for _ in range(k)]\n    \n    # Define the optimizer\n    optimizer = torch.optim.AdamW(counterfactual_candidates, lr=learning_rate)\n    \n    # Gradient descent\n    prev_loss = torch.tensor(float('inf'))\n    for epoch in tqdm(range(num_epochs), desc=\"Number of epochs\"):\n        y = model_classifier.predict(instance_to_analyze)\n        counterfactual_class = torch.tensor([1]) if y == 0 else torch.tensor([0])\n        optimizer.zero_grad()\n        \n        # Compute average yloss\n        yloss_per_ci = torch.stack([yloss(counterfactual_class, model_classifier(c_i)) for c_i in counterfactual_candidates])\n        avg_yloss = torch.mean(yloss_per_ci)\n                \n        # Compute average distance\n        dist_per_ci = torch.stack([distance_function(c_i, instance_to_analyze, training_dataset) for c_i in counterfactual_candidates])\n        avg_dist = torch.mean(dist_per_ci)\n        \n        # Compute dpp_diversity\n        K = create_k_matrix(counterfactual_candidates, distance_function, training_dataset)\n        dpp_diversity = torch.det(K)\n        \n        # Normalize dpp_diversity by the size of K to avoid determinant being too large\n        normalized_dpp_diversity = dpp_diversity / K.numel()\n\n        \n        # Compute the final loss\n        loss = avg_yloss + lambda_1*avg_dist - lambda_2*normalized_dpp_diversity\n        if detect_convergence(model_classifier, loss, prev_loss, y, counterfactual_candidates, threshold = 1e-3):\n            # this is still not working\n            break\n        prev_loss = loss\n        \n        # Backpropagation\n        loss.backward()\n        \n        # Update the counterfactuals\n        optimizer.step()\n        if epoch%10==0:\n            print(f\"Current loss: {loss}\")\n\n    \n    # Return the optimized counterfactuals\n    return [c_i for c_i in counterfactual_candidates]"
  },
  {
    "objectID": "posts/2023-07-08-annotated-diverse-cfe.html#a-few-practical-considerations",
    "href": "posts/2023-07-08-annotated-diverse-cfe.html#a-few-practical-considerations",
    "title": "Distilling Diverse Counterfactual Explanations",
    "section": "A few practical considerations",
    "text": "A few practical considerations\nAuthors also describe a few practical considerations that I’ll be using for understanding the main idea behind the paper. Let’s dive into them\n\nChoice of loss\nAt a first glance it might seem intuitive to use \\(\\ell_1\\text{-loss}\\) (\\(|f(x_{cf}) − f (c)|\\)) or \\(\\ell_2\\text{-loss}\\) ((\\(f(x_{cf}) - f(c))^2\\)) as the yloss, but this isn’t actually the best idea.\nThese loss functions take into account the difference between \\(f(c)\\) and our desired \\(f(x_{cf})\\) or \\(y\\), but what we really want for a valid counterfactual is for \\(f(c)\\) to be either more close than the threshold set by \\(f\\) such that it flips the class outcome, not necessarily the closest to our ideal \\(y\\) (either \\(1\\) or \\(0\\)). In fact, trying to optimize \\(f(c)\\) to be close to either \\(0\\) or \\(1\\) encourages bigger changes to \\(x\\) to fit the counterfactual class, which can result in a counterfactual that’s less feasible for a user. Because of this, we prefer to use a hinge-loss function that has no penalty as long as \\(f(c)\\) is above a certain threshold when the preferred class is 1 (or below a certain threshold when the preferred class is 0). It also adds a penalty that’s proportional to the difference between \\(f(c)\\) and the threshold when the classifier gets it right (but within the threshold), and a larger penalty when \\(f(c)\\) doesn’t show the desired counterfactual class. More precisely, the hinge-loss is: \\[\n\\begin{aligned}\n\\text{hinge\\_yloss}=max(0,1 - z*logit(f(c)))\n\\end{aligned}\n\\]\nWhere \\(z\\) is -1 when \\(y = 0\\) and \\(1\\) when \\(y=1\\), and \\(logit(f(c))\\) is the unscaled output from the ML model (e.g., final logits that enter a softmax layer for making predictions in a neural network).\n\nimport numpy as np\n\ndef hinge_yloss(y, probabilities, eps=1e-7):\n    \"\"\"\n    Computes the hinge loss.\n\n    Parameters:\n    eps (float): A small value to prevent log(0) when f_c is 0.\n\n    \"\"\"\n    # Ensures f_c is in (0, 1) interval to prevent log(0)\n    probabilities = probabilities.clamp(eps, 1)\n\n    # Compute logit\n    prediction_logits = torch.log(probabilities / (1 - probabilities))\n    # Determine z based on y\n    z = torch.where(y == 0, -torch.ones_like(y), torch.ones_like(y))\n\n    # Compute hinge loss\n    hinge_loss = torch.maximum(torch.tensor(0.0), 1 - z * prediction_logits)\n    \n    return hinge_loss\n\nI decided to have a quick test about it.\nNotice that as we need to differentiate the loss, the LogisticRegression method from sklearn wouldnt work since I dont have access to it’s derivatives or gradients (and they would be lost as I would need to convert torch.Tensor to numpy.array if I had to perform something like model.predict_proba(instance_tensor))\nTherefore, I’m creating a simple LogsiticRegression in Pytorch\n\n\nCode\nimport torch\nimport torch.nn as nn\n\nclass LogisticRegressionModel(nn.Module):\n    def __init__(self, input_dim):\n        super(LogisticRegressionModel, self).__init__()\n        self.linear = nn.Linear(input_dim, 1)\n\n    def predict(self, x, threshold=0.5):\n        self.eval()\n        label_probability = self(x)\n        y = (label_probability &gt; threshold).float()\n        return y\n\n    def forward(self, x):\n        out = torch.sigmoid(self.linear(x))\n        return out\n\n\n\nAnd then training it in a really dummy dataset, just for testing :)\n\n\nCode\nfrom sklearn.datasets import make_classification\nfrom sklearn.model_selection import train_test_split\n\n# Generate a binary classification dataset\nX, y = make_classification(n_samples=100, n_features=20, n_informative=2, n_redundant=10, random_state=1)\n\n# Split into train/test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1)\n\nX_train = torch.FloatTensor(X_train)\nX_test = torch.FloatTensor(X_test)\ny_train = torch.FloatTensor(y_train).view(-1, 1)\ny_test = torch.FloatTensor(y_test).view(-1, 1)\n\ninput_dim = X_train.shape[1]  # takes variable 'x' \nmodel = LogisticRegressionModel(input_dim)\n\ncriterion = torch.nn.BCELoss()  # Binary Cross Entropy loss\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n\nepochs = 2000\nfor epoch in tqdm(range(epochs), desc=\"Training Logistic Regression\"):\n    model.train()\n    optimizer.zero_grad()\n    # Forward pass\n    y_pred = model(X_train)\n    # Compute Loss\n    loss = criterion(y_pred, y_train)\n\n    # Backward pass and update\n    loss.backward()\n    optimizer.step()\n\n    # print progress\n    if (epoch+1) % 500 == 0:\n        print(f'epoch: {epoch+1}, loss = {loss.item()}')\n\n\n\n\n\nepoch: 500, loss = 0.06291789561510086\nepoch: 1000, loss = 0.04198703169822693\nepoch: 1500, loss = 0.031998444348573685\nepoch: 2000, loss = 0.025996342301368713\n\n\n\n\nCode\n# Create a sample instance\nx = X_test[0].reshape(1, -1)\nresult = hinge_yloss(model.predict(x), model(x))\n\n# Call the hinge_yloss function\nprint(f\"Hinge loss for a test case: {result}\")\n\n\nHinge loss for a test case: tensor([[0.9653]], grad_fn=&lt;MaximumBackward0&gt;)\n\n\nIt seems to have worked :)\n\n\nDistance function\nWhen dealing with continuous features, we define the ‘dist’ function as the average of feature-wise \\(\\ell_1\\) distances (or Manhattan distance) between the counterfactual example and the original input, as suggested by (Wachter, Mittelstadt, and Russell 2017). However, it’s worth noting that features can cover different ranges. So, to make sure we account for this, we divide each feature-wise distance by the median absolute deviation (MAD) of that feature’s values from the training set. This approach follows what was done in the paper by (Wachter, Mittelstadt, and Russell 2017). By looking at how much a feature deviates from the median, we can get a reliable measure of how variable a feature’s values are. So, dividing by the MAD lets us see how common it is to find the feature at a specific value. Mathematically, this can be represented as follows:\n\\[\n\\begin{aligned}\n\\text{dist\\_continuous}(c,x)=\\frac{1}{d_{cont}}\\sum^{d_{cont}}_{p=1}\\frac{|c^p - x^p|}{MAD_p}\n\\end{aligned}\n\\]\nHere, \\(d_{cont}\\) stands for the number of continuous variables and \\(MAD_p\\) is the median absolute deviation for the \\(p\\text{-th}\\) continuous variable, |c^p - x^p| is the Manhattan distance (\\(\\ell_1\\) distance) between a counterfactual vector and the instance we are analyzing\n\nimport torch\n\ndef median_abs_deviation(x):\n    \"\"\"\n    Compute the Median Absolute Deviation (MAD) of a tensor along an axis.\n    As I need to have access through tensors, this is necessary to be used instead\n    of scipy function\n\n    Parameters:\n    x (torch.Tensor): The input tensor.\n\n    Returns:\n    torch.Tensor: The computed MAD values.\n    \"\"\"\n    median = torch.median(x, axis=0)[0]\n    mad = torch.median(torch.abs(x - median), axis=0)[0]\n    return mad\n\ndef dist_continuous(c, x, training_data):\n    \"\"\"\n    Compute the distance function for continuous features. \n\n    We are assuming that training data contains only continuous features \n\n    Parameters:\n    c (torch.Tensor): A counterfactual example.\n    x (torch.Tensor): The original input instance.\n    training_data (torch.Tensor): The training dataset.\n\n    Returns:\n    float: The computed distance.\n    \"\"\"\n    \n    d_cont = x.shape[0]\n    \n    # Compute MAD values for each feature\n    mad_values = median_abs_deviation(training_data)\n    \n    abs_diff = torch.abs(c - x)\n    normalized_diff = abs_diff / mad_values\n    \n    return torch.sum(normalized_diff) / d_cont\n\nAnd then a quick test :)\n\n\nCode\nfrom scipy.stats import median_abs_deviation as mad_np\n\n#first lest see if the MAD implementation is correct\ndata_np = np.random.rand(100, 5)\ndata_torch = torch.tensor(data_np, dtype=torch.float)\n\n# Calculate MAD with both functions\nmad_scipy = mad_np(data_np, axis=0)\nmad_torch = median_abs_deviation(data_torch).numpy()\nassert np.allclose(mad_scipy, mad_torch, atol=1e-1)\n\n# Get the number of features\nnum_features = X_train.shape[1]\n\n# Define a counterfactual and an original instance with the correct number of features\nc = np.random.rand(num_features)\nx = np.random.rand(num_features)\n\n# Compute the distance\ndistance = dist_continuous(torch.tensor(c, requires_grad=True), torch.tensor(x), X_train)\n\nprint(distance)\n\n\ntensor(0.6583, dtype=torch.float64, grad_fn=&lt;DivBackward0&gt;)\n\n\n\n\n\n\n\n\nImportant\n\n\n\nAuthors also defined metrics and evaluations for categorical features, but I decided to skip them since this would give more details that could deviate from the main algorithm. If you want to check, please see the original paper (Mothilal, Sharma, and Tan 2020)\n\n\n\n\nScaling features\nTypically, continuous features can take on a broad variety of values, while categorical features are often constrained to a one-hot binary representation. It’s important to keep in mind that the scale of a feature can significantly influence its importance in our objective function. The authors provide to the users interactive interfaces where they can express their preferences for different features. However, as a reasonable starting point, we opt to transform all features to fall within the range of [0, 1].\nTherefore, for implementation purposes, I’ll be using StandardScaler for this example\n\n\nHyperparameters\nThe process of generating counterfactuals happens after the machine learning model has been trained, and so, as suggested in (Wachter, Mittelstadt, and Russell 2017), it’s not strictly necessary to use the same hyperparameters for each original input. However, because hyperparameters can significantly impact the counterfactuals that are produced, it could be problematic to present users with counterfactuals that were generated using different hyperparameters. In this study, the authors opted for \\(\\lambda_1 = 0.5\\) and \\(\\lambda_2 = 1\\).\n\n\nGetting real\nLet’s use the same dummy dataset that I used for Anchors example. There are a few differences, however: - We will be creating a SimpleNeuralNetwork classifier and we will use it as our blackbox model - We will be using StandardScaler to scale our features\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nclass SimpleNet(nn.Module):\n    def __init__(self, input_size, hidden_size):\n        super(SimpleNet, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(hidden_size, 1)\n        self.sigmoid = nn.Sigmoid()\n    \n    def predict(self, x, threshold=0.5):\n        self.eval()\n        # Ensure x is a 2D tensor: add an extra dimension if x is a 1D tensor\n        if len(x.shape) == 1:\n            x = x.unsqueeze(0) \n        label_probability = self(x)\n        y = (label_probability &gt; threshold).float()\n        return y\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return x\n\n\n# Generate a 2D dataset with two classes\nX, y = make_blobs(n_samples=200, centers=2, random_state=42, cluster_std=2.0)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nstd_features = StandardScaler()\nstd_features.fit(X_train)\n\n\nStandardScaler()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.StandardScalerStandardScaler()\n\n\nAnd then train the network\n\n\nCode\nimport torch.optim as optim\n\n# Define network parameters\ninput_size = X_train.shape[1]\nhidden_size = 5\n\n# Instantiate the network, loss function, and optimizer\nnet = SimpleNet(input_size, hidden_size)\ncriterion = nn.BCELoss()\noptimizer = optim.SGD(net.parameters(), lr=0.1)\nepochs = 2000\nfor epoch in tqdm(range(epochs), desc=\"Training Neural Net\"):\n    net.train()\n    optimizer.zero_grad()\n    # Forward pass\n    y_pred = net(torch.from_numpy(std_features.transform(X_train).astype(np.float32)))\n    # Compute Loss\n    loss = criterion(y_pred, torch.from_numpy(y_train.reshape(-1, 1).astype(np.float32)))\n\n    # Backward pass and update\n    loss.backward()\n    optimizer.step()\n\n    # print progress\n    if (epoch+1) % 500 == 0:\n        print(f'epoch: {epoch+1}, loss = {loss.item()}')\n\n\n\n\n\nepoch: 500, loss = 0.009603435173630714\nepoch: 1000, loss = 0.005159247666597366\nepoch: 1500, loss = 0.0036031161434948444\nepoch: 2000, loss = 0.0027885879389941692\n\n\n\nfrom sklearn.metrics import accuracy_score\npredictions = net.predict(torch.from_numpy(std_features.transform(X_test).astype(np.float32))).numpy().reshape(-1)\naccuracy_score(y_test, predictions)\n\n1.0\n\n\nIt was pretty easy to get a perfect model, which was expected.\nThen, now I want to explain a single instance from test set. Let’s take the first element :)\n\n\nCode\ninstance_to_explain = np.where(X==X_test[0])[0][0]\n\nassert (X[instance_to_explain]==X_test[0]).all()\n\ndef plot_dataset_with_instance(X, y, highlight_row=None, counterfactual_instance_list=None, title=\"Dataset\"):\n    # Assuming you have two classes 0 and 1\n    class_0 = X[y == 0]\n    class_1 = X[y == 1]\n    \n    # Create a scatter plot for each class\n    plt.scatter(class_0[:, 0], class_0[:, 1], c='blue', label='Class 0', alpha=0.3)\n    plt.scatter(class_1[:, 0], class_1[:, 1], c='red', label='Class 1', alpha=0.3)\n    \n    if highlight_row is not None:\n        plt.scatter(X[highlight_row, 0], X[highlight_row, 1], c='green', label='Instance', alpha=1, marker='o', edgecolors='k')\n    \n    if counterfactual_instance_list is not None and isinstance(counterfactual_instance_list, list):\n        for counterfactual_instance in counterfactual_instance_list:\n            plt.scatter(counterfactual_instance[0], counterfactual_instance[1], \n                        c='cyan', label='Counterfactual Instance', alpha=1, marker='o', edgecolors='k')\n        \n    plt.xlabel('Feature 1')\n    plt.ylabel('Feature 2')\n    plt.title(title)\n    \n    # Add a legend to the plot\n    plt.legend()\n    \n    plt.show()\n\n# Pass in the index of the row you want to highlight\nplot_dataset_with_instance(X, y, highlight_row=instance_to_explain)\n\n\n\n\n\nTherefore, we want to find the set of candidates that makes the green instance becomes red. For that, then let’s use the learn_counterfactuals defined earlier with the following consierations:\n\n\\(x=\\) the same instance that we are trying to get the counterfactuals, transformed from std_features\n\\(\\text{yloss}\\) is the hinge_loss that we defined earlier\n\\(\\text{distance\\_function}\\) is the \\(\\text{distance\\_continuous}\\) that was defined earlier\n\\(\\text{lambda\\_1}=\\lambda_1=0.5\\)\n\\(\\text{lambda\\_2}=\\lambda_2=2.0\\)\n\\(\\text{model\\_predict}\\) will be our net model which we just trained\n\\(k=3\\), so we are trying to learn a three counterfactual\n\n\ninstance_to_explain = X_test[0]\nreference_instance = np.where(X==instance_to_explain)[0][0]\nx = torch.from_numpy(std_features.transform(instance_to_explain.reshape(1,-1)).astype(np.float32))\nlambda_1 = 0.5\nlambda_2 = 2.0\nnumber_of_counterfactuals = 3\n\n\nlearnt_counterfactuals = learn_counterfactuals(\n    instance_to_analyze=x,\n    lambda_1=torch.tensor(lambda_1),\n    lambda_2=torch.tensor(lambda_2),\n    yloss=hinge_yloss,\n    distance_function=dist_continuous,\n    model_classifier=net,\n    k=number_of_counterfactuals,\n    training_dataset=torch.FloatTensor(X_train),\n    num_epochs=50, \n    learning_rate=0.05\n)\n\n\n\n\nCurrent loss: 0.6136817932128906\nCurrent loss: 0.20534832775592804\nCurrent loss: 0.1759258359670639\nLoss converged at: 0.0008479952812194824, stop criteria reached\n\n\n\ncounterfactual_instances_scaled_back = []\nfor single_counterfactual in learnt_counterfactuals:\n    counterfactual_instances_scaled_back.append(std_features.inverse_transform(single_counterfactual.detach().numpy())[0])\n\n\nplot_dataset_with_instance(X, y, highlight_row=reference_instance, counterfactual_instance_list=counterfactual_instances_scaled_back, title=\"Initial Dataset\")\n\n\n\n\n\n\nDataset with Learnt Counterfactual Instances"
  },
  {
    "objectID": "posts/2023-07-08-annotated-diverse-cfe.html#performing-adjustment",
    "href": "posts/2023-07-08-annotated-diverse-cfe.html#performing-adjustment",
    "title": "Distilling Diverse Counterfactual Explanations",
    "section": "Performing Adjustment",
    "text": "Performing Adjustment\n\nEnhancing Sparcity\nThe loss function aims to minimize the distance between the input and the generated counterfactuals. However, an ideal counterfactual should change as few features as possible to maintain its sparsity considering the feasibility definition. To promote this sparsity, the authors implemented a post-processing step where they revert the values of continuous features back to their original values in \\(x\\), proceeding greedily until the predicted class \\(f(c)\\) changes. For this step, they consider all continuous features \\(c^j\\) where the difference from \\(x^j\\) is below a chosen threshold. Although the median absolute distance (\\(MAD\\)) may seem like an intuitive threshold, it can be rather large for features with high variance. Thus, for each feature, they choose the lower value between the \\(MAD\\) and the 10 percentile of the absolute difference between non-identical values from the median.\n\nimport torch\n\ndef post_hoc_adjustment(c_list_original, instance_to_explain, model, X_train, std_features):\n    \"\"\"\n    Post-hoc filtering to promote sparsity in the counterfactuals.\n\n    Parameters:\n    c_list_original (List[torch.Tensor]): A list of counterfactual unscaled.\n    instance_to_explain (torch.Tensor): The original input instance unscaled.\n    model: The machine learning model that we are trying to explain.\n    X_train_scaled (torch.Tensor): The training dataset unscaled.\n\n    Returns:\n    List[torch.Tensor]: The list of sparse counterfactuals unscaled.\n    \"\"\"\n\n    # Calculate MAD values for each feature\n    mad_values = median_abs_deviation(X_train)\n\n    # Calculate the 10th percentile of absolute differences from the median\n    p10_values = torch.quantile(input=torch.abs(X_train - torch.median(X_train)), q=0.10, dim=0)\n\n    # Choose the lower value between the MAD and the 10th percentile for each feature\n    thresholds = torch.minimum(mad_values, p10_values)\n    # Prepare a container for sparse counterfactuals\n    c_sparse_list = []\n\n    for original_counterfactual in c_list_original:\n        # Calculate the absolute differences between the counterfactual and the original instance\n        scale_c = std_features.transform(original_counterfactual.detach().numpy().reshape(1,-1)).astype(np.float32)\n        scale_x = std_features.transform(instance_to_explain.detach().numpy().reshape(1,-1)).astype(np.float32)\n        abs_diff = torch.abs(torch.from_numpy(scale_c) - torch.from_numpy(scale_x))\n\n        # Create a list of indices ordered by the absolute differences\n        ordered_indices = torch.argsort(abs_diff)\n\n        # Copy the counterfactual to avoid modifying the original\n        c_sparse = original_counterfactual.clone()\n        # Revert each feature until the predicted class changes\n        for idx in ordered_indices[0]:\n            if abs_diff[0][idx] &lt; thresholds[idx]:\n                c_sparse_tmp = scale_c\n                c_sparse_tmp[0][idx] = scale_x[0][idx]\n                if model.predict(torch.from_numpy(c_sparse_tmp)) != model.predict(torch.from_numpy(scale_c)):\n                    break\n                c_sparse[0][idx] = instance_to_explain[idx]\n        c_sparse_list.append(c_sparse)\n\n    return [counterfactual_sparse[0].detach().numpy() for counterfactual_sparse in c_sparse_list]\n\nLets do the post processing and then compare the plots\n\n\nCode\nc_sparse = post_hoc_adjustment(\n      learnt_counterfactuals,\n      torch.Tensor(instance_to_explain), \n      net, \n      torch.FloatTensor(X_train), \n      std_features\n)\n\n\n\n\n\n\n\n\n\n\n\n\nWe can see from this plot that if we just move Feature 2 and keep Feature 1 constant, we can achieve three different counterfactuals, that is, we have found the feature changes that modify the class result! :)"
  },
  {
    "objectID": "posts/2023-07-08-annotated-diverse-cfe.html#footnotes",
    "href": "posts/2023-07-08-annotated-diverse-cfe.html#footnotes",
    "title": "Distilling Diverse Counterfactual Explanations",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAs always, ChatGPT is being a true “friend” when creating these boilerplate code :)↩︎"
  },
  {
    "objectID": "posts/2023-06-24-annotated-anchors.html",
    "href": "posts/2023-06-24-annotated-anchors.html",
    "title": "Distilling Anchors",
    "section": "",
    "text": "Inspired by The Annotated Transformer blog post, which is an amazing educational resource, I decided to start this series of posts where I will try to drill down some papers that I’ve been reading :) The focus is to provide some dummy explanation, giving preference to use code and some cool vizualizations alongside with it!\nThis is the first blog, where I’m going to explain in details the Anchor’s paper (Ribeiro, Singh, and Guestrin 2018) 1."
  },
  {
    "objectID": "posts/2023-06-24-annotated-anchors.html#necessary-jargon",
    "href": "posts/2023-06-24-annotated-anchors.html#necessary-jargon",
    "title": "Distilling Anchors",
    "section": "Necessary Jargon",
    "text": "Necessary Jargon\nI’ll take the definitions from Alibi’s documentation, which are concise and really well defined :)\n\nPredicate\n\nRepresents an expression involving a single feature. Some examples of predicates for a tabular dataset having features such as Age, Relationship, and Occupation are: - 28 &lt; Age &lt; 50 - Relationship = Husband - Occupation = Blue-Collar\n\n\n\nRule\n\nA rule represents a set of predicates connected by the AND operator. Considering all the predicate examples above, we can construct the following rule:\n\n\n28 &lt; Age &lt; 50 AND Relationship = Husband AND Occupation = Blue-Collar\n\nNote that a rule selects/refers to a particular subpopulation from the given dataset.\n\n\nAnchor\n\nAn anchor explanation is a rule that sufficiently anchors the prediction locally – such that changes to the rest of the feature values of the instance do not matter (Ribeiro, Singh, and Guestrin 2018)\n\n\n\nPrecision\n\nPrecision represents the probability of receiving the same classification label of the explained input if we query the model on other instances that satisfy the anchor predicates. The expected precision range is the interval \\([t,1]\\), where \\(t\\) is the user-specified precision threshold.\n\n\nFor example, if \\(t=0.95\\) and we have two instances, \\(a\\) and \\(b\\), that contains the same anchor predicates, the probability that a given model will predict the same result for both is \\(0.95\\).\n\n\n\nCoverage\n\nCoverage represents the proportion of the population which satisfy the anchor predicates. It is a positive number \\(\\leq 1\\), where a value of corresponds \\(1\\) to the empty anchor."
  },
  {
    "objectID": "posts/2023-06-24-annotated-anchors.html#going-formal",
    "href": "posts/2023-06-24-annotated-anchors.html#going-formal",
    "title": "Distilling Anchors",
    "section": "Going Formal",
    "text": "Going Formal\nLet’s say we have a set of conditions or rules, which we’ll call \\(A\\). \\(A\\) works on an easily understandable representation and it goes like this - if all conditions of \\(A\\) are met for a specific instance (let’s call it \\(x\\)), then \\(A(x)\\) will return 1.\nIn our earlier example, \\(x\\) has these feature values: Age is between 28 and 50, Relationship status is 'Husband', and Occupation is 'Blue-Collar'. If these are the conditions \\(A\\) is checking for, in this case, \\(A(x)\\) is \\(1\\) because all the conditions are met.\nNow, suppose we have a complex model (we’ll refer to it as a ‘black box model’), which we’ll denote as \\(f\\), that takes input \\(X\\) and generates output \\(Y\\). Now, if we have a specific instance \\(x \\in X\\), our objective with local model-agnostic interpretability is to make it easy for a user to understand why \\(f(x)\\) - the prediction for the instance \\(x\\) - is what it is.\nThe fundamental assumption here is that even if the model is too intricate to explain in a brief manner globally, if we “zoom in” on individual predictions, the task of explaining becomes feasible. In order to achieve this, most model-agnostic techniques use a process called ‘perturbation’, modifying the instance \\(x\\) following a specific “perturbation distribution” which we’ll denote as \\(\\mathcal{D}\\).\nAssuming the definition of \\(A\\) from before, we denote as \\(\\mathcal{D}(\\cdot|A)\\) the case when conditional distribution contains the rule \\(A\\) being applied. \\(A\\) is called an anchor if it holds true for \\(x\\) (i.e., \\(A(x)=1\\)) and is a sufficient condition for \\(f(x)\\) with high probability (the \\(\\tau\\) introduced before). If we sample an instance \\(z\\) from \\(\\mathcal{D}(z|A)\\) and it’s predicted as Positive in the same way as \\(x\\) (meaning \\(f(x) = f(z)\\)), then \\(A\\) is an anchor.\nIn other words:\n\nGiven we apply the anchor \\(A\\) to \\(x\\), the expected probability that \\(f(x) = f(z)\\) under the distribution \\(\\mathcal{D}\\) conditioned on \\(A\\) is equal or greater then some threshold \\(\\tau\\).\n\nMathematically,\n\\(\\mathbb{E}_{\\mathcal{D}(z|A)}[\\mathbb{1}_{f(x)=f(z)}] \\geq \\tau,A(x)=1\\)\n\nComputing Anchors\nAs stated before, an anchor \\(A\\) is identified as a collection of feature predicates related to \\(x\\), with a precision \\(prec(A)\\) that is equal to or greater than \\(\\tau\\). This precision can be defined as follows:\n\\(prec(A) = \\mathbb{E}_{D(z|A)}[\\mathbb{1}{f(x) = f(z)}]\\)\nThe authors define precision in a probabilistic manner:\n\nAn anchor meets the precision condition with a high level of certainty.\n\n\\(P(prec(A) \\geq \\tau) \\geq 1 - \\delta\\)\nIf there are multiple anchors that satisfy these criteria, preference is given to those that represent a larger portion of the input space, or in other words, those with the greatest coverage.\nThe coverage of an anchor \\(A\\), \\(\\text{cov}(A)\\), is formally defined as the probability that it is applicable to samples derived from the distribution \\(D\\). We can represent it as follows:\n\\(\\text{cov}(A) = \\mathbb{E}_{D(z)}[A(z)]\\).\n\nSearching the set of Anchors\nThus, our goal is to identify the set of anchors, \\(A\\), that ensures maximum coverage. This can be achieved through the following combinatorial optimization problem:\n\\[\\begin{align*}\n\\underset{A \\text{ s.t. } , P(prec(A) \\geq \\tau) \\geq 1 - \\delta}{\\text{max}}  \\text{cov}(A)\n\\end{align*}\\]\nTo accomplish this, we don’t utilize a pre-existing dataset. Rather, we employ perturbation distributions along with a black box model, which are instrumental in estimating precision and coverage bounds under the distribution \\(\\mathcal{D}\\).\nFor a good (and trackable) exploration of the model’s behavior within the perturbation space, we map the problem to a multi-armed bandit formulation."
  },
  {
    "objectID": "posts/2023-06-24-annotated-anchors.html#the-dataset",
    "href": "posts/2023-06-24-annotated-anchors.html#the-dataset",
    "title": "Distilling Anchors",
    "section": "The Dataset",
    "text": "The Dataset\nFor learning purposes, let’s use a simple dataset generator from scikit-learn which I trained a LGBMClassifier\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import train_test_split\n\n# Generate a 2D dataset with two classes\nnp.random.seed(42)\nX, y = make_blobs(n_samples=200, centers=2, random_state=42, cluster_std=2.0)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Train the LightGBM classifier\nclf = LGBMClassifier(random_state=42)\nclf.fit(X_train, y_train)\n\n\nLGBMClassifier(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LGBMClassifierLGBMClassifier(random_state=42)\n\n\nAlso, I want to explain a single instance from test set. Let’s take the first element :)\n\ninstance_to_explain = np.where(X==X_test[0])[0][0]\n\nassert (X[instance_to_explain]==X_test[0]).all()\n\ndef plot_dataset_with_instance(X, y, highlight_row=None, title=\"Dataset\"):\n    # Assuming you have two classes 0 and 1\n    class_0 = X[y == 0]\n    class_1 = X[y == 1]\n    \n    # Create a scatter plot for each class\n    plt.scatter(class_0[:, 0], class_0[:, 1], c='blue', label='Class 0', alpha=0.3)\n    plt.scatter(class_1[:, 0], class_1[:, 1], c='red', label='Class 1', alpha=0.3)\n    \n    if highlight_row is not None:\n        plt.scatter(X[highlight_row, 0], X[highlight_row, 1], c='green', label='Instance', alpha=1, marker='o', edgecolors='k')\n\n    plt.xlabel('Feature 1')\n    plt.ylabel('Feature 2')\n    plt.title(title)\n    \n    # Add a legend to the plot\n    plt.legend()\n    \n    plt.show()\n\n# Pass in the index of the row you want to highlight\nplot_dataset_with_instance(X, y, highlight_row=instance_to_explain)\n\n\n\n\nTherefore, I want to provide an explaination through Anchors for the green point, which comes from the test set.\nIn the paper, the authors separates the candidate generation from the Anchor algorithm itself. I think it is better to threat them altogether, then in other hands we can say that the whole algorithm is three fold: 1. Candidate Generation 2. Best candidate identification"
  },
  {
    "objectID": "posts/2023-06-24-annotated-anchors.html#identifying-candidates",
    "href": "posts/2023-06-24-annotated-anchors.html#identifying-candidates",
    "title": "Distilling Anchors",
    "section": "Identifying Candidates",
    "text": "Identifying Candidates\nWe start by creating an ‘anchor’ \\(A\\), which we initially set up as an empty rule that can apply to every instance. As we iterate through our process, we come up with new potential rules that expand \\(A\\) by adding another feature predicate, denoted as \\(\\{a_i\\}\\), to its definition. In other words, with each iteration, our set of potential rules looks like \\(\\{A \\wedge a_i, A \\wedge a_{i+1}, A \\wedge a_{i+2}, \\dots\\}\\), each one just adding a new feature predicate to the last.\nMathematically, the algorithm is defined as follows (taken from the paper): \n\nShow me the code!\nBefore that we have to define some intermediate functions that will help us to simplify things2\n\nCreating Predicates\nSince anchors are a set of predicates, we need to define what a Predicate is\n\n\nCode\nimport pandas as pd\nfrom collections import namedtuple\nimport numpy as np\nfrom tqdm import tqdm\n\nclass Predicate:\n    def __init__(self, feature, operator, threshold):\n        self.feature = feature\n        self.operator = operator\n        self.threshold = threshold\n\n    def __call__(self, x):\n        if self.operator == \"&lt;=\":\n            return x[:, self.feature] &lt;= self.threshold\n        elif self.operator == \"&gt;\":\n            return x[:, self.feature] &gt; self.threshold\n\n    def __str__(self):\n        return f'x[{self.feature}] {self.operator} {self.threshold}'\n    \ndef generate_predicates(X_train):\n    predicates = []\n    for feature in range(X_train.shape[1]):\n        for threshold in np.unique(X_train[:, feature]):\n            predicates.append(Predicate(feature, \"&lt;=\", threshold))\n            predicates.append(Predicate(feature, \"&gt;\", threshold))\n    return predicates\n\n\n\n\n\n\n\n\nImportant\n\n\n\nPlease note that the generate_predicates method that I implemented uses unique values from dataset train to set the rules. I decided to keep it this way to simplify things and focus on what matters. However, if we considerer real implementations, we should create rules based on Discretization of continuous features to reduce the search space. Also, authors claim to have used a validation dataset for \\(\\mathcal{D}\\) instead of the training set.\n\n\n\n\nCode\ndef calculate_coverage(anchor, D):\n    D_sample = D.copy()\n    for predicate in anchor:\n        mask = predicate(D_sample)\n        indices_match_anchor = np.argwhere(mask == True).flatten()\n        D_sample = D_sample[indices_match_anchor]\n    # D_sample is equal to the number of instances where anchor rules hold\n    return len(D_sample)/len(D)\n\ndef generate_cands(anchor_set, c, X_train):\n    A_r = []\n    predicates = generate_predicates(X_train)\n    if len(anchor_set) == 0:\n        # each predicate is a potential anchor in the first round\n        for predicate in predicates:\n            anchor = [predicate]\n            cov = calculate_coverage(anchor, X_train)\n            if cov &gt; c:\n                A_r.append(anchor)\n    else:\n        for anchor in anchor_set:\n            anchor_features = {pred.feature for pred in anchor}\n            for predicate in predicates:\n                # Skip predicates already in the anchor\n                if predicate.feature in anchor_features:\n                    continue\n\n                # Create a new candidate anchor by adding the predicate\n                A_new = anchor.copy()\n                A_new.append(predicate)\n                \n                cov = calculate_coverage(anchor, X_train)\n                if cov &gt; c:\n                    # If coverage is above threshold, add to results\n                    A_r.append(A_new)\n    print(f\"Found {len(A_r)} new anchors with coverage above {c}\")\n    return A_r\n\n\nLets see a quick and simple example where we want to explain the first element of test set and the desired coverage is 0.95\n\n\n\n\n\n\nImportant\n\n\n\nAlso, realize that the way that we defined \\(\\mathcal{D}\\) is specific for Tabular cases. For Images or Text these might difer. If you want to learn more, check this out\n\n\n\n# Apply the function\nA = []  # start with an empty anchor\nc = 0.95  # coverage threshold, set as desired\nx = X_test[0]\nA_r = generate_cands(A, c, X_train)\nlen(A_r)\n\nFound 26 new anchors with coverage above 0.95\n\n\n26\n\n\nFor example, these are some candidates:\n\nprint(A_r[0][0])\nprint(A_r[10][0])\nprint(A_r[20][0])\n\nx[0] &gt; -5.959033288078816\nx[0] &lt;= 8.884191230253368\nx[1] &lt;= 11.128530580636152"
  },
  {
    "objectID": "posts/2023-06-24-annotated-anchors.html#selecting-the-best-candidate",
    "href": "posts/2023-06-24-annotated-anchors.html#selecting-the-best-candidate",
    "title": "Distilling Anchors",
    "section": "Selecting the Best Candidate",
    "text": "Selecting the Best Candidate\nIn order to determine the most suitable candidate from a given pool, the authors approach the problem by formulating it as an exploration of multi-armed bandits (Kaufmann and Kalyanakrishnan 2013). Here, each candidate \\(A\\) is considered as an arm, and the hidden reward is represented by the true precision of \\(A\\) on \\(\\mathcal{D}(\\cdot|A)\\). Evaluating the expression \\(\\mathbb{1}_{f(x)=f(z)}\\) on a sample drawn from \\(\\mathcal{D}(z|A)\\) corresponds to pulling the arm \\(A\\).\nThe authors utilized a method called KL-LUCB (short for Kullback-Leibler Lower Confidence Bound), to determine the rule with the highest precision. However, explaining the intricacies of KL-LUCB in a brief blog post may be overly complex. Since our main focus is on Anchor’s theory rather than Reinforcement Learning, I will simplify this part by employing an \\(\\epsilon\\)-greedy approach.\n\nDifferences into algorithm approaches\nI asked ChatGPT to define this one and considering it is correct, I thought it would worth to put it here:\nThe Kullback-Leibler Lower Confidence Bound (KL-LUCB) and the \\(\\epsilon\\text{-greedy}\\) algorithms are two different approaches to the multi-armed bandit problem, which is a classical problem in probability theory and statistics that models the trade-off between exploration and exploitation in sequential decision-making.\nAn \\(\\epsilon\\text{-greedy}\\) algorithm is a simple approach where, with probability epsilon, the algorithm chooses an arm at random (exploration). With probability \\(1-\\epsilon\\), it selects the arm with the highest estimated reward (exploitation). The value of epsilon is usually set to a small value, and it can be decreased over time to favor exploitation over exploration.\nOn the other hand, the KL-LUCB calculates the confidence bounds (an upper one and a lower one on the expected rewards for each option using the Kullback-Leibler divergence, which quantifies the difference between two probability distributions. This allows KL-LUCB to make more informed decisions based on the uncertainty of the rewards.\nTherefore, for implementing the exploration vs. exploitation algorithm, we will not need to calculate the upper confidence and lower confidence bounds for precision, but rather we will always take the maximum precision or a random one, depending on \\(\\epsilon\\) value.\n\n\n\n\n\n\nImportant\n\n\n\nI’ll not paste the pseudo code for the other algorithms (BestCands and BeamSearch) shown in paper since I implemented variations of them by using \\(\\epsilon \\text{- greedy}\\) instead of KL-LUCB\n\n\n\n# Function to get a sample which satisfies the rules in an anchor\ndef sample_conditionally(X, anchor):\n    D_sample = X.copy()\n    for predicate in anchor:\n        mask = predicate(D_sample)\n        # Use the mask to sample from the training set\n        D_sample = D_sample[mask]\n    #we are interested in obtaining samples from minimum size of 2\n    if len(D_sample)&lt;=1: \n        return None\n    #D_sample will contain the instances where A(x)=1, where A is the anchor\n    # I decided to sample half of it, but we could use other values\n    rnd_indices = np.random.choice(len(D_sample), size=len(D_sample)//2)\n    D_sample = D_sample[rnd_indices]\n    return D_sample\n\n# Function to estimate the precision of an anchor\ndef estimate_precision(clf, X_train, x, anchor):\n    fx = clf.predict([x])\n    z = sample_conditionally(X_train, anchor)\n    if z is None: # No sample satisfies the anchor\n        return 0.0\n    fz = clf.predict(z)\n    return (fz==fx).mean()\n\n# Main function to find the best candidate to add to the anchor\ndef best_cand(anchor_candidates, X_train, x, clf, epsilon, num_samples=1000):\n    if len(anchor_candidates) == 0:\n        return None\n    num_arms = len(anchor_candidates)\n    Q = np.zeros(num_arms)\n    N = np.zeros(num_arms, dtype=int)\n    for _ in tqdm(range(num_samples), desc=\"Performing Epsilon Greedy approach\"):\n        if np.random.rand() &lt; epsilon: # Explore\n            arm = np.random.randint(num_arms)\n        else: # Exploit\n            arm = np.argmax(Q)\n        anchor = anchor_candidates[arm]\n        reward = estimate_precision(clf, X_train, x, anchor)\n        N[arm] += 1\n        Q[arm] += (reward - Q[arm]) / N[arm]\n    return anchor_candidates[np.argmax(Q)]\n\n\nbest_anchor = best_cand(A_r, X_train, x, clf, 0.3, num_samples=1000)\n\nPerforming Epsilon Greedy approach: 100%|██████████| 1000/1000 [00:16&lt;00:00, 59.52it/s]\n\n\n\n\nCode\nprint(\"Anchors final rule: \" + \"AND \".join(str(predicate) for predicate in best_anchor))\n\n\nAnchors final rule: x[0] &lt;= 7.2508364505367595"
  },
  {
    "objectID": "posts/2023-06-24-annotated-anchors.html#issues-with-greedy-search",
    "href": "posts/2023-06-24-annotated-anchors.html#issues-with-greedy-search",
    "title": "Distilling Anchors",
    "section": "Issues with Greedy Search",
    "text": "Issues with Greedy Search\nAs we can see, due to the greedy nature of the algorithm, it is impossible to search two different anchors “at the same time”. At every time, it will select the best candidate and incrementally augment it (so we can’t backtrack, for example). Also, the greedy search does not consider coverage, so it is biased to return the shortest anchor that respects the precision \\(\\tau\\) threshold.\nWe can extend the greedy approach to address these issues so that instead of taking the best candidate, we take the best \\(B\\) ones. Therefore, we turn a greedy search problem into beam-search. Given this set of \\(B\\) candidates, we search for the one with the highest coverage."
  },
  {
    "objectID": "posts/2023-06-24-annotated-anchors.html#footnotes",
    "href": "posts/2023-06-24-annotated-anchors.html#footnotes",
    "title": "Distilling Anchors",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDepending on the effort that I will need to put on that I will decide if I would continue or not↩︎\nChatGPT helped me to create this whole. It is be being a true ally for writing my posts as well :)↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Andre’s Personal Blog :)",
    "section": "",
    "text": "Distilling Diverse Counterfactual Explanations\n\n\n\n\n\n\n\nmasters\n\n\nknowledge-distill\n\n\nxai\n\n\n\n\nDistilling Diverse Counterfactual Explanations\n\n\n\n\n\n\nJul 8, 2023\n\n\nAndre Barbosa\n\n\n\n\n\n\n  \n\n\n\n\nDistilling Anchors\n\n\n\n\n\n\n\nmasters\n\n\nknowledge-distill\n\n\nxai\n\n\n\n\nAn attempt to distill Anchors paper through code\n\n\n\n\n\n\nJun 24, 2023\n\n\nAndre Barbosa\n\n\n\n\n\n\n  \n\n\n\n\nBERT applied to Multiple Choice\n\n\n\n\n\n\n\nmasters\n\n\nnlp\n\n\nknowledge-distill\n\n\n\n\nStep by step about a specific fine tuning task:)\n\n\n\n\n\n\nFeb 21, 2021\n\n\nAndre Barbosa\n\n\n\n\n\n\n  \n\n\n\n\nDistilling BERT Pre Training\n\n\n\n\n\n\n\nmasters\n\n\nnlp\n\n\nknowledge-distill\n\n\n\n\nStep by step about its inner work from scratch :)\n\n\n\n\n\n\nSep 19, 2020\n\n\nAndre Barbosa\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2021-02-21-downstream-bert-qa.html",
    "href": "posts/2021-02-21-downstream-bert-qa.html",
    "title": "BERT applied to Multiple Choice",
    "section": "",
    "text": "Drilling down Multiple Choice downstream task\nWhen I started studying Language Models, I remember when I’ve found the following image from Open AI transformer paper (Radford and Narasimhan 2018):\n\nHowever, the only difference is that the input data should be slightly different:\n\nFor these tasks, we are given a context document \\(z\\), a question \\(q\\), and a set of possible answers \\({a_k}\\). We concatenate the document context and question with each possible answer, adding a delimiter token in between to get [\\(z\\); \\(q\\); $ ; \\(a_k\\)]. Each of these sequences are processed independently with our model and then normalized via a softmax layer to produce an output distribution over possible answers.\n\nTherefore, these inputs could be optimized via Categorical Cross Entropy Loss, where \\(C\\) is the number of options available. For a specific question.\n\n\nFrom GPT to BERT\nAs we will see with Hugging Face’s transformer library, when we considerer application from a fine tuning task, the approach of BERT can be derived directly from the tecnique presented by (Radford and Narasimhan 2018). It is possible to check it from documentation\n\nBert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a softmax) e.g. for RocStories/SWAG tasks.\n\n\n\nCode\nimport numpy as np\nimport torch\nfrom transformers import BertTokenizer, BertForMultipleChoice\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = BertForMultipleChoice.from_pretrained(\"bert-base-uncased\")\n\n\nDownloading: 100%|██████████| 232k/232k [00:01&lt;00:00, 171kB/s]\nDownloading: 100%|██████████| 433/433 [00:00&lt;00:00, 122kB/s]\nDownloading: 100%|██████████| 440M/440M [02:06&lt;00:00, 3.48MB/s]\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultipleChoice: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\nquestion = \"George wants to warm his hands quickly by rubbing them. Which skin surface will produce the most heat?\"\noption_a = \"dry palms\"\noption_b = \"wet palms\"\noption_c = \"palms covered with oil\"\noption_d = \"palms covered with lotion\"\n\nIn this case, option A is the correct one. Furthermore, the batch size here would be 1\n\nlabels = torch.tensor(0).unsqueeze(0) \n\nNotice that the question is the same for each option\n\nencoding = tokenizer(\n            [question, question, question, question],\n            [option_a, option_b, option_c, option_d],\n            return_tensors='pt',\n            padding=True\n           )\n\noutputs = model(**{k: v.unsqueeze(0) for k,v in encoding.items()}, labels=labels)\n\n\n\n\n\n\n\nImportant\n\n\n\nNotice that if we have a dataset such as SquaD where each question comes with a context, we could append this context to either the question text or the option text and we would then have the tuple cited by Open AI transformer paper\n\n\nThe output is a linear layer which would still be trained through a Cross Entropy loss. Then, as stated by the documentation, we still need to apply softmax to the logits\n\nloss = outputs.loss\nlogits = outputs.logits\n\nLinear Logits output:\n\n\ntensor([[-0.3457, -0.3295, -0.3271, -0.3342]], grad_fn=&lt;ViewBackward&gt;)\n\n\nLogits after the softmax function. Since this model did not learn anything, the result below is expected:\n\n\ntensor([[0.2471, 0.2511, 0.2518, 0.2500]], grad_fn=&lt;SoftmaxBackward&gt;)\n\n\n\n\nConclusion\nCongratulations! Adding up with the first part, you have learned the end-to-end BERT Flow :)\n\n\n\n\n\nReferences\n\nRadford, Alec, and Karthik Narasimhan. 2018. “Improving Language Understanding by Generative Pre-Training.” In."
  },
  {
    "objectID": "posts/2020-09-19-distilling-bert.html",
    "href": "posts/2020-09-19-distilling-bert.html",
    "title": "Distilling BERT Pre Training",
    "section": "",
    "text": "I remember someday of 2016 while I was starting my career as a Data Scientist when I’ve stumped into Chirs McCormick blog about Word2Vec. Honestly, I think that Tomas Mikolov paper was one of the most elegant and simple idea that I have ever found so far1 :)\n\n\nAccording to Pytorch documentation an Embedding can be defined as the following:\n\nA simple lookup table (…) of a fixed dictionary and size.\n\nThen, we can interpret embeddings as a simple way to convert integers into vectors of a given size. Then, for word embeddings, we can interpret simply as words that are encoded as integers, and then these integers serve as inputs for a vector space.’\nA have written some code with manim to illustrate this process:\n\nWe can then interpret each dimension as a single neuron of a hidden layer, and then these embedding numbers can be modified from a learning algorithm through a neural network. This is the main motivation behind Word Embeddings algorithms such as Word2Vec and fastText2\nNowadays, there are some libraries that provide already trained vectors based on a fixed and previously trained vocabulary. For instance, considerer the following Spacy code:\n\n\nCode\nimport spacy\n\nnlp = spacy.load(\"en_core_web_md\")\nprint(\"Coniderer the sentence 'The quick brown fox jumps over the lazy dog!!'\")\ntext = nlp(\"The quick brown fox jumps over the lazy dog!!\")\nfor word in text:\n    print(\n        f\"'{word.text}' vector representation has size of {word.vector.shape[0]}. Its first five elements are: {word.vector[:5].round(2)}\"\n    )\n\n\nConiderer the sentence 'The quick brown fox jumps over the lazy dog!!'\n'The' vector representation has size of 300. Its first five elements are: [ 0.27 -0.06 -0.19  0.02 -0.02]\n'quick' vector representation has size of 300. Its first five elements are: [-0.45  0.19 -0.25  0.47  0.16]\n'brown' vector representation has size of 300. Its first five elements are: [-0.37 -0.08  0.11  0.19  0.03]\n'fox' vector representation has size of 300. Its first five elements are: [-0.35 -0.08  0.18 -0.09 -0.45]\n'jumps' vector representation has size of 300. Its first five elements are: [-0.33  0.22 -0.35 -0.26  0.41]\n'over' vector representation has size of 300. Its first five elements are: [-0.3   0.01  0.04  0.1   0.12]\n'the' vector representation has size of 300. Its first five elements are: [ 0.27 -0.06 -0.19  0.02 -0.02]\n'lazy' vector representation has size of 300. Its first five elements are: [-0.35 -0.3  -0.18 -0.32 -0.39]\n'dog' vector representation has size of 300. Its first five elements are: [-0.4   0.37  0.02 -0.34  0.05]\n'!' vector representation has size of 300. Its first five elements are: [-0.27  0.34  0.22 -0.3  -0.06]\n'!' vector representation has size of 300. Its first five elements are: [-0.27  0.34  0.22 -0.3  -0.06]\n\n\nContains word representations that were trained on Common Crawl data using GloVe algorithm. Unlike the example that I used initially, the word ‘!’ was encoded as well.\nWe can combine different words to form the embedding of a phrase. According to spacy documentation: &gt; Models that come with built-in word vectors make them available as the Token.vector attribute. Doc.vector and Span.vector will default to an average of their token vectors.\nThen, the phrase the we are using as example has the following single representation:\n\n\nFirst 5 values of 'The quick brown fox jumps over the lazy dog!!': [-0.23  0.08 -0.03 -0.07 -0.02]\n\n\n\n\n\nEven though Word Embeddings brings many benefits in the realm of computational linguistics, they have some limitations. There is a linguistic phenomenon called polysemy. According to wikipedia: &gt; A polyseme is a word or phrase with different, but related senses.(…) English has many polysemous words. For example, the verb “to get” can mean “procure” (I’ll get the drinks), “become” (she got scared), “understand” (I get it) etc.\nSo considering the example above, despite the fact that the verb has different meaning depending on the contexts, it’s word representation would always be the same\n\n\nFirst 5 values of verb 'to get' vector: [ 0.03  0.12 -0.32  0.13  0.12]\n\n\nThen, if we pick two phrases: She got scared and She understand it, we will get the following vectors\n\ntext1 = nlp(\"He will get scared\")\ntext2 = nlp(\"She will get the drinks\")\n\nprint(f\"First 5 values of sentence '{text1}' vector: {text1.vector[:5].round(2)}\")\nprint(f\"First 5 values of sentence '{text2}' vector: {text2.vector[:5].round(2)}\")\n\nFirst 5 values of verb 'He will get scared' vector: [-0.12  0.19 -0.21 -0.14  0.09]\nFirst 5 values of verb 'She will get the drinks' vector: [ 0.01  0.13 -0.04 -0.08  0.03]\n\n\nThen, if we take the cosine similarity by taking the average of the word vectors:\n\n\nCode\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nprint(\n    f\"Similarity between:\\n '{text1}' and '{text2}': \"\n    f\"{cosine_similarity(text1.vector.reshape(1, -1),text2.vector.reshape(1, -1))[0][0]}\"\n)\n\n\nSimlarity between:\n 'He will get scared' and 'She will get the drinks': 0.8653444051742554\n\n\nThis indicates that both vectors would be a lot similar. However, the reason for that is the usage of similar words, even considering that they were applied in different contexts! So there is the objective that BERT tries to solve.{% fn 3 %}\n{{ ‘There are some BERT percursors such as ELMo; ULMFit and Open AI Transformer that I am not going to cover here. Please reach out to Illustrated BERT blog to know more’ | fndetail: 3 }}"
  },
  {
    "objectID": "posts/2020-09-19-distilling-bert.html#what-are-word-embeddings",
    "href": "posts/2020-09-19-distilling-bert.html#what-are-word-embeddings",
    "title": "Distilling BERT Pre Training",
    "section": "",
    "text": "According to Pytorch documentation an Embedding can be defined as the following:\n\nA simple lookup table (…) of a fixed dictionary and size.\n\nThen, we can interpret embeddings as a simple way to convert integers into vectors of a given size. Then, for word embeddings, we can interpret simply as words that are encoded as integers, and then these integers serve as inputs for a vector space.’\nA have written some code with manim to illustrate this process:\n\nWe can then interpret each dimension as a single neuron of a hidden layer, and then these embedding numbers can be modified from a learning algorithm through a neural network. This is the main motivation behind Word Embeddings algorithms such as Word2Vec and fastText2\nNowadays, there are some libraries that provide already trained vectors based on a fixed and previously trained vocabulary. For instance, considerer the following Spacy code:\n\n\nCode\nimport spacy\n\nnlp = spacy.load(\"en_core_web_md\")\nprint(\"Coniderer the sentence 'The quick brown fox jumps over the lazy dog!!'\")\ntext = nlp(\"The quick brown fox jumps over the lazy dog!!\")\nfor word in text:\n    print(\n        f\"'{word.text}' vector representation has size of {word.vector.shape[0]}. Its first five elements are: {word.vector[:5].round(2)}\"\n    )\n\n\nConiderer the sentence 'The quick brown fox jumps over the lazy dog!!'\n'The' vector representation has size of 300. Its first five elements are: [ 0.27 -0.06 -0.19  0.02 -0.02]\n'quick' vector representation has size of 300. Its first five elements are: [-0.45  0.19 -0.25  0.47  0.16]\n'brown' vector representation has size of 300. Its first five elements are: [-0.37 -0.08  0.11  0.19  0.03]\n'fox' vector representation has size of 300. Its first five elements are: [-0.35 -0.08  0.18 -0.09 -0.45]\n'jumps' vector representation has size of 300. Its first five elements are: [-0.33  0.22 -0.35 -0.26  0.41]\n'over' vector representation has size of 300. Its first five elements are: [-0.3   0.01  0.04  0.1   0.12]\n'the' vector representation has size of 300. Its first five elements are: [ 0.27 -0.06 -0.19  0.02 -0.02]\n'lazy' vector representation has size of 300. Its first five elements are: [-0.35 -0.3  -0.18 -0.32 -0.39]\n'dog' vector representation has size of 300. Its first five elements are: [-0.4   0.37  0.02 -0.34  0.05]\n'!' vector representation has size of 300. Its first five elements are: [-0.27  0.34  0.22 -0.3  -0.06]\n'!' vector representation has size of 300. Its first five elements are: [-0.27  0.34  0.22 -0.3  -0.06]\n\n\nContains word representations that were trained on Common Crawl data using GloVe algorithm. Unlike the example that I used initially, the word ‘!’ was encoded as well.\nWe can combine different words to form the embedding of a phrase. According to spacy documentation: &gt; Models that come with built-in word vectors make them available as the Token.vector attribute. Doc.vector and Span.vector will default to an average of their token vectors.\nThen, the phrase the we are using as example has the following single representation:\n\n\nFirst 5 values of 'The quick brown fox jumps over the lazy dog!!': [-0.23  0.08 -0.03 -0.07 -0.02]"
  },
  {
    "objectID": "posts/2020-09-19-distilling-bert.html#limitations-of-word-embeddings",
    "href": "posts/2020-09-19-distilling-bert.html#limitations-of-word-embeddings",
    "title": "Distilling BERT Pre Training",
    "section": "",
    "text": "Even though Word Embeddings brings many benefits in the realm of computational linguistics, they have some limitations. There is a linguistic phenomenon called polysemy. According to wikipedia: &gt; A polyseme is a word or phrase with different, but related senses.(…) English has many polysemous words. For example, the verb “to get” can mean “procure” (I’ll get the drinks), “become” (she got scared), “understand” (I get it) etc.\nSo considering the example above, despite the fact that the verb has different meaning depending on the contexts, it’s word representation would always be the same\n\n\nFirst 5 values of verb 'to get' vector: [ 0.03  0.12 -0.32  0.13  0.12]\n\n\nThen, if we pick two phrases: She got scared and She understand it, we will get the following vectors\n\ntext1 = nlp(\"He will get scared\")\ntext2 = nlp(\"She will get the drinks\")\n\nprint(f\"First 5 values of sentence '{text1}' vector: {text1.vector[:5].round(2)}\")\nprint(f\"First 5 values of sentence '{text2}' vector: {text2.vector[:5].round(2)}\")\n\nFirst 5 values of verb 'He will get scared' vector: [-0.12  0.19 -0.21 -0.14  0.09]\nFirst 5 values of verb 'She will get the drinks' vector: [ 0.01  0.13 -0.04 -0.08  0.03]\n\n\nThen, if we take the cosine similarity by taking the average of the word vectors:\n\n\nCode\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nprint(\n    f\"Similarity between:\\n '{text1}' and '{text2}': \"\n    f\"{cosine_similarity(text1.vector.reshape(1, -1),text2.vector.reshape(1, -1))[0][0]}\"\n)\n\n\nSimlarity between:\n 'He will get scared' and 'She will get the drinks': 0.8653444051742554\n\n\nThis indicates that both vectors would be a lot similar. However, the reason for that is the usage of similar words, even considering that they were applied in different contexts! So there is the objective that BERT tries to solve.{% fn 3 %}\n{{ ‘There are some BERT percursors such as ELMo; ULMFit and Open AI Transformer that I am not going to cover here. Please reach out to Illustrated BERT blog to know more’ | fndetail: 3 }}"
  },
  {
    "objectID": "posts/2020-09-19-distilling-bert.html#attention-is-all-you-need",
    "href": "posts/2020-09-19-distilling-bert.html#attention-is-all-you-need",
    "title": "Distilling BERT Pre Training",
    "section": "Attention is all you need",
    "text": "Attention is all you need\nThe Attention is all you need paper have introduced the Transformer architeture for us :) In sense, it can be summarized as the picture below:\n\nStrictly speaking, the motivation behind the paper is that RNN-like architetures are memory-expensive. The purpose behind Transformer models is that it you can achieve similar results using more computer efficient resources by applying just attention mechanisms (and exluding the CNN or RNN-like architetures) !{% fn 4 %} Despite the fact that the Transformer model was proposed to deal with translation problems, it turns out that we can also use variations of it to achieve awesome results in different tasks. This is the motivation behind BERT!\n{{ ‘The NLP group from Harvard has written a great blog post distilling the paper as well as implementing them in pytorch. If you have some interest in knowing details about the transformer architecture, I recommend looking at it!’ | fndetail: 4 }}\n\nAttention?\nAccording to the Transformer and Attention lecture from NYU foundations of Deep Learning Course:\n\nTransformers are made up of attention modules, which are mappings between sets, rather than sequences, which means we do not impose an ordering to our inputs/outputs.\n\nWhen we analyze the transformer architeture, we can see that both Multi-Head Attention and Multi-Head Masked Attention box have 3 Arrow Heads. Each one represents one of the following:\n\nQ that stands for query vector with dimension \\(d_k\\)\nK that stands for key vector that also has dimension \\(d_k\\)\nV that stands for value vector that also has dimension \\(d_v\\)\n\nKV pair can be understood as the encoded representation of the input whereas the Q is the output of a previous layer.\n\n\nKey-Value Store\nAgain, from the Deep Learning Foundations Course from NYU:\n\nA key-value store is a paradigm designed for storing (saving), retrieving (querying), and managing associative arrays (dictionaries/hash tables)\n\n\nFor example, say we wanted to find a recipe to make lasagne. We have a recipe book and search for “lasagne” - this is the query. This query is checked against all possible keys in your dataset - in this case, this could be the titles of all the recipes in the book. We check how aligned the query is with each title to find the maximum matching score between the query and all the respective keys. If our output is the argmax function - we retrieve the single recipe with the highest score. Otherwise, if we use a soft argmax function, we would get a probability distribution and can retrieve in order from the most similar content to less and less relevant recipes matching the query.\n\n\nBasically, the query is the question. Given one query, we check this query against every key and retrieve all matching content.\n\n\n\n\n\n\n\nWarning\n\n\n\nI have decided not to cover attention concepts in this post, giving just a higher-level introduction. As you might have noticed, NYU Deep Learning Foundations Course provides a really nice introduction about the topic that I recommend going through if you want to learn more :)\n\n\n\n\n\n\n\n\nNote\n\n\n\nAttention can be basically understood as measure of correlation of words between a set of sentences. For those interested to learn a little bit more, I highly recommend this blog post\n\n\n\n\nPositional Encoding\nThis was taken from The annotated transformer blog where you can find a cool pytorch implementation. It turns out that actually this is a quote from Attention is all you need paper:\n\nSince our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add “positional encodings” to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension \\(d_{model}\\) as the embeddings, so that the two can be summed"
  },
  {
    "objectID": "posts/2020-09-19-distilling-bert.html#the-bert-model",
    "href": "posts/2020-09-19-distilling-bert.html#the-bert-model",
    "title": "Distilling BERT Pre Training",
    "section": "The BERT model",
    "text": "The BERT model\nBERT model itself is an encoder model only from the transformer model. Considering the models trained from the paper, the base model consists of 12 encoder-stacked layers and the large model consists of 24 encoder-stacked layers.\nAccording to the Attention is all you need paper:\n\nThe encoder is composed of a stack of \\(N = 6\\) identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position wise fully connected feed-forward network. We employ a residual connection around each of the two sub-layers, followed by layer normalization.\n\n\n\nThe Multi-Head Attention\nBasically, the multi head attention is a type of an attention mechanism. It is a concatenation of another type of attention, the scaled dot. Both mechanisms works together as represented in the following image:\n\n\n\n\n\n\n\nNote\n\n\n\nScaled Dot-Product Attention is calculated by \\(softmax(\\frac{QK^T}{\\sqrt{n}})V\\), where K, V and Q are the same as the ones described in a previous section whereas n represents the number of elements in the set.\n\n\nHere, h, or the number o attention heads (or layers) is equal to \\(12\\) in the case of \\(\\text{BERT}_\\text{base}\\) and \\(16\\) in the case of \\(\\text{BERT}_\\text{large}\\)\n\n\nResidual Conections\nEach sublayer of the encoder stack contains a residual connection (the left curved arrow) added to the sublayer output before layer normalization. The idea of Residual Conections came from Computer Vision domain, and actually, it is a relatively simple technique that can be summarized by the following image:\n\nConsidering the image above and the case of Encoder stack, each \\(\\mathcal{F}(x)\\) means either the Multi-Head Attention or Feed Forward. Therefore, quoting the paper:\n\nThat is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension \\(d_{model} = 512\\) {% fn 5 %}.\n\n{{ ‘In the case of BERT model, please have in mind that \\(N\\) is either \\(12\\) (BERTbase) or \\(24\\) ((BERTlarge) and dmodel is 768 for BERT base and 1024 for BERT large’ | fndetail: 5 }}\nThen, what, in fact, is being encoded?"
  },
  {
    "objectID": "posts/2020-09-19-distilling-bert.html#embedding-representation",
    "href": "posts/2020-09-19-distilling-bert.html#embedding-representation",
    "title": "Distilling BERT Pre Training",
    "section": "Embedding Representation",
    "text": "Embedding Representation\nThe authors would like to make BERT to perform well in different downstream tasks such as binary and multi lablel classification; language modeling; question and answering; named entity recognition; etc. Therefore, they said the following:\n\nour input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., h Question, Answer) in one token sequence. Throughout this work, a “sentence” can be an arbitrary span of contiguous text, rather than an actual linguistic sentence. A “sequence” refers to the input token sequence to BERT, which may be a single sentence or two sentences packed together\n\nIn order to perform and create the sentence embeddings, WordPiece tokenize is applied. Then, besides adding [CLS] token, pairs of sentence (e.g. sentence A and B) are concatenated into a single sentence, being separated with a special token [SEP] (e.g. A [SEP] B).\nThen:\n\nFor a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings."
  },
  {
    "objectID": "posts/2020-09-19-distilling-bert.html#masked-language-model-mlm",
    "href": "posts/2020-09-19-distilling-bert.html#masked-language-model-mlm",
    "title": "Distilling BERT Pre Training",
    "section": "Masked Language Model (MLM)",
    "text": "Masked Language Model (MLM)\nAs we are feeding the whole sentence into the model, it is possible to say that the model is bidirectional and hence as we are trying to predict the next word in a sentence, it would has access to it! Then, the idea behind this task is pretty simple. We can directly quote from the paper:\n\nUnfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirectional conditioning would allow each word to indirectly “see itself”, and the model could trivially predict the target word in a multi-layered context.\n\n\nIn order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. We refer to this procedure as a “masked LM” (MLM), although it is often referred to as a Cloze task in the literature. In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM.\n\nIn the case of BERT model, 15% of each sentence were masked during training."
  },
  {
    "objectID": "posts/2020-09-19-distilling-bert.html#next-sentence-prediction-nsp",
    "href": "posts/2020-09-19-distilling-bert.html#next-sentence-prediction-nsp",
    "title": "Distilling BERT Pre Training",
    "section": "Next Sentence Prediction (NSP)",
    "text": "Next Sentence Prediction (NSP)\nIn order to learn relationships between pair of sentence (e.g. Question and Ansering tasks) the authors needed a different approach than plain Language Modeling. Then:\n\nIn order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus. Specifically, when choosing the sentences A and B for each pretraining example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext).\n\nOnce defined, both objected functions are used in BERT Pre training learning :)\n\n\n\n\n\n\n\nNote\n\n\n\nThe training loss is the sum of the mean masked LM (MLM) likelihood and the mean next sentence prediction (NSP) likelihood\n\n\n\n\n\n\n\n\nImportant\n\n\n\nYou may have noticed but this training procedure does not require labeling. As we are using the raw text inputs to generate the labels during training, e considerer this BERT Pre Training as a self-surpervised model!"
  },
  {
    "objectID": "posts/2020-09-19-distilling-bert.html#footnotes",
    "href": "posts/2020-09-19-distilling-bert.html#footnotes",
    "title": "Distilling BERT Pre Training",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFun Fact: Whereas nowadays Miklov LinkedIn profile points out that he has worked for Microsoft, Google and Facebook; another of W2V authors, Ilya Sutskever worked with some of the prestigious researchers in the recent AI area, such as Geoffrey Hinton and Andrew Ng. Moreover, he is one of the founders of Open AI!↩︎\nI am not going to cover word embeddings through this blog post. If you are not familiarized with them, I highly recommend this; this and this as potential resources :)↩︎"
  }
]