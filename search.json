[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi there! My name is André Barbosa and I’m currently a Master Student@Institute of Mathemathics and Statistics as well as a Data Scientist@QuintoAndar, a real state company. My current reserch interests include the following:\n\nExplainable AI\nNatural Language Processing\nCausal Inference\n\nFell free to follow me on social media and contact me :)"
  },
  {
    "objectID": "posts/2021-02-21-downstream-bert-qa.html",
    "href": "posts/2021-02-21-downstream-bert-qa.html",
    "title": "BERT applied to Multiple Choice",
    "section": "",
    "text": "Drilling down Multiple Choice downstream task\nWhen I started studying Language Models, I remember when I’ve found the following image from Open AI transformer paper (Radford and Narasimhan 2018):\n\nHowever, the only difference is that the input data should be slightly different:\n\nFor these tasks, we are given a context document \\(z\\), a question \\(q\\), and a set of possible answers \\({a_k}\\). We concatenate the document context and question with each possible answer, adding a delimiter token in between to get [\\(z\\); \\(q\\); $ ; \\(a_k\\)]. Each of these sequences are processed independently with our model and then normalized via a softmax layer to produce an output distribution over possible answers.\n\nTherefore, these inputs could be optimized via Categorical Cross Entropy Loss, where \\(C\\) is the number of options available. For a specific question.\n\n\nFrom GPT to BERT\nAs we will see with Hugging Face’s transformer library, when we considerer application from a fine tuning task, the approach of BERT can be derived directly from the tecnique presented by (Radford and Narasimhan 2018). It is possible to check it from documentation\n\nBert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a softmax) e.g. for RocStories/SWAG tasks.\n\n\n\nCode\nimport numpy as np\nimport torch\nfrom transformers import BertTokenizer, BertForMultipleChoice\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = BertForMultipleChoice.from_pretrained(\"bert-base-uncased\")\n\n\nDownloading: 100%|██████████| 232k/232k [00:01&lt;00:00, 171kB/s]\nDownloading: 100%|██████████| 433/433 [00:00&lt;00:00, 122kB/s]\nDownloading: 100%|██████████| 440M/440M [02:06&lt;00:00, 3.48MB/s]\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultipleChoice: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\nquestion = \"George wants to warm his hands quickly by rubbing them. Which skin surface will produce the most heat?\"\noption_a = \"dry palms\"\noption_b = \"wet palms\"\noption_c = \"palms covered with oil\"\noption_d = \"palms covered with lotion\"\n\nIn this case, option A is the correct one. Furthermore, the batch size here would be 1\n\nlabels = torch.tensor(0).unsqueeze(0) \n\nNotice that the question is the same for each option\n\nencoding = tokenizer(\n            [question, question, question, question],\n            [option_a, option_b, option_c, option_d],\n            return_tensors='pt',\n            padding=True\n           )\n\noutputs = model(**{k: v.unsqueeze(0) for k,v in encoding.items()}, labels=labels)\n\n\n\n\n\n\n\nImportant\n\n\n\nNotice that if we have a dataset such as SquaD where each question comes with a context, we could append this context to either the question text or the option text and we would then have the tuple cited by Open AI transformer paper\n\n\nThe output is a linear layer which would still be trained through a Cross Entropy loss. Then, as stated by the documentation, we still need to apply softmax to the logits\n\nloss = outputs.loss\nlogits = outputs.logits\n\nLinear Logits output:\n\n\ntensor([[-0.3457, -0.3295, -0.3271, -0.3342]], grad_fn=&lt;ViewBackward&gt;)\n\n\nLogits after the softmax function. Since this model did not learn anything, the result below is expected:\n\n\ntensor([[0.2471, 0.2511, 0.2518, 0.2500]], grad_fn=&lt;SoftmaxBackward&gt;)\n\n\n\n\nConclusion\nCongratulations! Adding up with the first part, you have learned the end-to-end BERT Flow :)\n\n\n\n\n\nReferences\n\nRadford, Alec, and Karthik Narasimhan. 2018. “Improving Language Understanding by Generative Pre-Training.” In."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Andre’s Personal Blog :)",
    "section": "",
    "text": "Distilling Anchors\n\n\n\n\n\n\n\nmasters\n\n\nknowledge-distill\n\n\nxai\n\n\n\n\nAn attempt to distill Anchors paper through code\n\n\n\n\n\n\nJun 24, 2023\n\n\nAndre Barbosa\n\n\n\n\n\n\n  \n\n\n\n\nBERT applied to Multiple Choice\n\n\n\n\n\n\n\nmasters\n\n\nnlp\n\n\nknowledge-distill\n\n\n\n\nStep by step about a specific fine tuning task:)\n\n\n\n\n\n\nFeb 21, 2021\n\n\nAndre Barbosa\n\n\n\n\n\n\n  \n\n\n\n\nDistilling BERT Pre Training\n\n\n\n\n\n\n\nmasters\n\n\nnlp\n\n\nknowledge-distill\n\n\n\n\nStep by step about its inner work from scratch :)\n\n\n\n\n\n\nSep 19, 2020\n\n\nAndre Barbosa\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-06-24-annotated-anchors.html",
    "href": "posts/2023-06-24-annotated-anchors.html",
    "title": "Distilling Anchors",
    "section": "",
    "text": "Inspired by The Annotated Transformer blog post, which is an amazing educational resource, I decided to start this series of posts where I will try to drill down some papers that I’ve been reading :) The focus is to provide some dummy explanation, giving preference to use code and some cool vizualizations alongside with it!\nThis is the first blog, where I’m going to explain in details the Anchor’s paper (Ribeiro, Singh, and Guestrin 2018) 1."
  },
  {
    "objectID": "posts/2023-06-24-annotated-anchors.html#necessary-jargon",
    "href": "posts/2023-06-24-annotated-anchors.html#necessary-jargon",
    "title": "Distilling Anchors",
    "section": "Necessary Jargon",
    "text": "Necessary Jargon\nI’ll take the definitions from Alibi’s documentation, which are concise and really well defined :)\n\nPredicate\n\nRepresents an expression involving a single feature. Some examples of predicates for a tabular dataset having features such as Age, Relationship, and Occupation are: - 28 &lt; Age &lt; 50 - Relationship = Husband - Occupation = Blue-Collar\n\n\n\nRule\n\nA rule represents a set of predicates connected by the AND operator. Considering all the predicate examples above, we can construct the following rule:\n\n\n28 &lt; Age &lt; 50 AND Relationship = Husband AND Occupation = Blue-Collar\n\nNote that a rule selects/refers to a particular subpopulation from the given dataset.\n\n\nAnchor\n\nAn anchor explanation is a rule that sufficiently anchors the prediction locally – such that changes to the rest of the feature values of the instance do not matter (Ribeiro, Singh, and Guestrin 2018)\n\n\n\nPrecision\n\nPrecision represents the probability of receiving the same classification label of the explained input if we query the model on other instances that satisfy the anchor predicates. The expected precision range is the interval \\([t,1]\\), where \\(t\\) is the user-specified precision threshold.\n\n\nFor example, if \\(t=0.95\\) and we have two instances, \\(a\\) and \\(b\\), that contains the same anchor predicates, the probability that a given model will predict the same result for both is \\(0.95\\).\n\n\n\nCoverage\n\nCoverage represents the proportion of the population which satisfy the anchor predicates. It is a positive number \\(\\leq 1\\), where a value of corresponds \\(1\\) to the empty anchor."
  },
  {
    "objectID": "posts/2023-06-24-annotated-anchors.html#going-formal",
    "href": "posts/2023-06-24-annotated-anchors.html#going-formal",
    "title": "Distilling Anchors",
    "section": "Going Formal",
    "text": "Going Formal\nLet’s say we have a set of conditions or rules, which we’ll call \\(A\\). \\(A\\) works on an easily understandable representation and it goes like this - if all conditions of \\(A\\) are met for a specific instance (let’s call it \\(x\\)), then \\(A(x)\\) will return 1.\nIn our earlier example, \\(x\\) has these feature values: Age is between 28 and 50, Relationship status is 'Husband', and Occupation is 'Blue-Collar'. If these are the conditions \\(A\\) is checking for, in this case, \\(A(x)\\) is \\(1\\) because all the conditions are met.\nNow, suppose we have a complex model (we’ll refer to it as a ‘black box model’), which we’ll denote as \\(f\\), that takes input \\(X\\) and generates output \\(Y\\). Now, if we have a specific instance \\(x \\in X\\), our objective with local model-agnostic interpretability is to make it easy for a user to understand why \\(f(x)\\) - the prediction for the instance \\(x\\) - is what it is.\nThe fundamental assumption here is that even if the model is too intricate to explain in a brief manner globally, if we “zoom in” on individual predictions, the task of explaining becomes feasible. In order to achieve this, most model-agnostic techniques use a process called ‘perturbation’, modifying the instance \\(x\\) following a specific “perturbation distribution” which we’ll denote as \\(\\mathcal{D}\\).\nAssuming the definition of \\(A\\) from before, we denote as \\(\\mathcal{D}(\\cdot|A)\\) the case when conditional distribution contains the rule \\(A\\) being applied. \\(A\\) is called an anchor if it holds true for \\(x\\) (i.e., \\(A(x)=1\\)) and is a sufficient condition for \\(f(x)\\) with high probability (the \\(\\tau\\) introduced before). If we sample an instance \\(z\\) from \\(\\mathcal{D}(z|A)\\) and it’s predicted as Positive in the same way as \\(x\\) (meaning \\(f(x) = f(z)\\)), then \\(A\\) is an anchor.\nIn other words:\n\nGiven we apply the anchor \\(A\\) to \\(x\\), the expected probability that \\(f(x) = f(z)\\) under the distribution \\(\\mathcal{D}\\) conditioned on \\(A\\) is equal or greater then some threshold \\(\\tau\\).\n\nMathematically,\n\\(\\mathbb{E}_{\\mathcal{D}(z|A)}[\\mathbb{1}_{f(x)=f(z)}] \\geq \\tau,A(x)=1\\)\n\nComputing Anchors\nAs stated before, an anchor \\(A\\) is identified as a collection of feature predicates related to \\(x\\), with a precision \\(prec(A)\\) that is equal to or greater than \\(\\tau\\). This precision can be defined as follows:\n\\(prec(A) = \\mathbb{E}_{D(z|A)}[\\mathbb{1}{f(x) = f(z)}]\\)\nThe authors define precision in a probabilistic manner:\n\nAn anchor meets the precision condition with a high level of certainty.\n\n\\(P(prec(A) \\geq \\tau) \\geq 1 - \\delta\\)\nIf there are multiple anchors that satisfy these criteria, preference is given to those that represent a larger portion of the input space, or in other words, those with the greatest coverage.\nThe coverage of an anchor \\(A\\), \\(\\text{cov}(A)\\), is formally defined as the probability that it is applicable to samples derived from the distribution \\(D\\). We can represent it as follows:\n\\(\\text{cov}(A) = \\mathbb{E}_{D(z)}[A(z)]\\).\n\nSearching the set of Anchors\nThus, our goal is to identify the set of anchors, \\(A\\), that ensures maximum coverage. This can be achieved through the following combinatorial optimization problem:\n\\[\\begin{align*}\n\\underset{A \\text{ s.t. } , P(prec(A) \\geq \\tau) \\geq 1 - \\delta}{\\text{max}}  \\text{cov}(A)\n\\end{align*}\\]\nTo accomplish this, we don’t utilize a pre-existing dataset. Rather, we employ perturbation distributions along with a black box model, which are instrumental in estimating precision and coverage bounds under the distribution \\(\\mathcal{D}\\).\nFor a good (and trackable) exploration of the model’s behavior within the perturbation space, we map the problem to a multi-armed bandit formulation."
  },
  {
    "objectID": "posts/2023-06-24-annotated-anchors.html#the-dataset",
    "href": "posts/2023-06-24-annotated-anchors.html#the-dataset",
    "title": "Distilling Anchors",
    "section": "The Dataset",
    "text": "The Dataset\nFor learning purposes, let’s use a simple dataset generator from scikit-learn which I trained a LGBMClassifier\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import make_blobs\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import train_test_split\n\n# Generate a 2D dataset with two classes\nnp.random.seed(42)\nX, y = make_blobs(n_samples=200, centers=2, random_state=42, cluster_std=2.0)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Train the LightGBM classifier\nclf = LGBMClassifier(random_state=42)\nclf.fit(X_train, y_train)\n\n\nLGBMClassifier(random_state=42)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LGBMClassifierLGBMClassifier(random_state=42)\n\n\nAlso, I want to explain a single instance from test set. Let’s take the first element :)\n\ninstance_to_explain = np.where(X==X_test[0])[0][0]\n\nassert (X[instance_to_explain]==X_test[0]).all()\n\ndef plot_dataset_with_instance(X, y, highlight_row=None, title=\"Dataset\"):\n    # Assuming you have two classes 0 and 1\n    class_0 = X[y == 0]\n    class_1 = X[y == 1]\n    \n    # Create a scatter plot for each class\n    plt.scatter(class_0[:, 0], class_0[:, 1], c='blue', label='Class 0', alpha=0.3)\n    plt.scatter(class_1[:, 0], class_1[:, 1], c='red', label='Class 1', alpha=0.3)\n    \n    if highlight_row is not None:\n        plt.scatter(X[highlight_row, 0], X[highlight_row, 1], c='green', label='Instance', alpha=1, marker='o', edgecolors='k')\n\n    plt.xlabel('Feature 1')\n    plt.ylabel('Feature 2')\n    plt.title(title)\n    \n    # Add a legend to the plot\n    plt.legend()\n    \n    plt.show()\n\n# Pass in the index of the row you want to highlight\nplot_dataset_with_instance(X, y, highlight_row=instance_to_explain)\n\n\n\n\nTherefore, I want to provide an explaination through Anchors for the green point, which comes from the test set.\nIn the paper, the authors separates the candidate generation from the Anchor algorithm itself. I think it is better to threat them altogether, then in other hands we can say that the whole algorithm is three fold: 1. Candidate Generation 2. Best candidate identification"
  },
  {
    "objectID": "posts/2023-06-24-annotated-anchors.html#identifying-candidates",
    "href": "posts/2023-06-24-annotated-anchors.html#identifying-candidates",
    "title": "Distilling Anchors",
    "section": "Identifying Candidates",
    "text": "Identifying Candidates\nWe start by creating an ‘anchor’ \\(A\\), which we initially set up as an empty rule that can apply to every instance. As we iterate through our process, we come up with new potential rules that expand \\(A\\) by adding another feature predicate, denoted as \\(\\{a_i\\}\\), to its definition. In other words, with each iteration, our set of potential rules looks like \\(\\{A \\wedge a_i, A \\wedge a_{i+1}, A \\wedge a_{i+2}, \\dots\\}\\), each one just adding a new feature predicate to the last.\nMathematically, the algorithm is defined as follows (taken from the paper): \n\nShow me the code!\nBefore that we have to define some intermediate functions that will help us to simplify things2\n\nCreating Predicates\nSince anchors are a set of predicates, we need to define what a Predicate is\n\n\nCode\nimport pandas as pd\nfrom collections import namedtuple\nimport numpy as np\nfrom tqdm import tqdm\n\nclass Predicate:\n    def __init__(self, feature, operator, threshold):\n        self.feature = feature\n        self.operator = operator\n        self.threshold = threshold\n\n    def __call__(self, x):\n        if self.operator == \"&lt;=\":\n            return x[:, self.feature] &lt;= self.threshold\n        elif self.operator == \"&gt;\":\n            return x[:, self.feature] &gt; self.threshold\n\n    def __str__(self):\n        return f'x[{self.feature}] {self.operator} {self.threshold}'\n    \ndef generate_predicates(X_train):\n    predicates = []\n    for feature in range(X_train.shape[1]):\n        for threshold in np.unique(X_train[:, feature]):\n            predicates.append(Predicate(feature, \"&lt;=\", threshold))\n            predicates.append(Predicate(feature, \"&gt;\", threshold))\n    return predicates\n\n\n\n\n\n\n\n\nImportant\n\n\n\nPlease note that the generate_predicates method that I implemented uses unique values from dataset train to set the rules. I decided to keep it this way to simplify things and focus on what matters. However, if we considerer real implementations, we should create rules based on Discretization of continuous features to reduce the search space. Also, authors claim to have used a validation dataset for \\(\\mathcal{D}\\) instead of the training set.\n\n\n\n\nCode\ndef calculate_coverage(anchor, D):\n    D_sample = D.copy()\n    for predicate in anchor:\n        mask = predicate(D_sample)\n        indices_match_anchor = np.argwhere(mask == True).flatten()\n        D_sample = D_sample[indices_match_anchor]\n    # D_sample is equal to the number of instances where anchor rules hold\n    return len(D_sample)/len(D)\n\ndef generate_cands(anchor_set, c, X_train):\n    A_r = []\n    predicates = generate_predicates(X_train)\n    if len(anchor_set) == 0:\n        # each predicate is a potential anchor in the first round\n        for predicate in predicates:\n            anchor = [predicate]\n            cov = calculate_coverage(anchor, X_train)\n            if cov &gt; c:\n                A_r.append(anchor)\n    else:\n        for anchor in anchor_set:\n            anchor_features = {pred.feature for pred in anchor}\n            for predicate in predicates:\n                # Skip predicates already in the anchor\n                if predicate.feature in anchor_features:\n                    continue\n\n                # Create a new candidate anchor by adding the predicate\n                A_new = anchor.copy()\n                A_new.append(predicate)\n                \n                cov = calculate_coverage(anchor, X_train)\n                if cov &gt; c:\n                    # If coverage is above threshold, add to results\n                    A_r.append(A_new)\n    print(f\"Found {len(A_r)} new anchors with coverage above {c}\")\n    return A_r\n\n\nLets see a quick and simple example where we want to explain the first element of test set and the desired coverage is 0.95\n\n\n\n\n\n\nImportant\n\n\n\nAlso, realize that the way that we defined \\(\\mathcal{D}\\) is specific for Tabular cases. For Images or Text these might difer. If you want to learn more, check this out\n\n\n\n# Apply the function\nA = []  # start with an empty anchor\nc = 0.95  # coverage threshold, set as desired\nx = X_test[0]\nA_r = generate_cands(A, c, X_train)\nlen(A_r)\n\nFound 26 new anchors with coverage above 0.95\n\n\n26\n\n\nFor example, these are some candidates:\n\nprint(A_r[0][0])\nprint(A_r[10][0])\nprint(A_r[20][0])\n\nx[0] &gt; -5.959033288078816\nx[0] &lt;= 8.884191230253368\nx[1] &lt;= 11.128530580636152"
  },
  {
    "objectID": "posts/2023-06-24-annotated-anchors.html#selecting-the-best-candidate",
    "href": "posts/2023-06-24-annotated-anchors.html#selecting-the-best-candidate",
    "title": "Distilling Anchors",
    "section": "Selecting the Best Candidate",
    "text": "Selecting the Best Candidate\nIn order to determine the most suitable candidate from a given pool, the authors approach the problem by formulating it as an exploration of multi-armed bandits (Kaufmann and Kalyanakrishnan 2013). Here, each candidate \\(A\\) is considered as an arm, and the hidden reward is represented by the true precision of \\(A\\) on \\(\\mathcal{D}(\\cdot|A)\\). Evaluating the expression \\(\\mathbb{1}_{f(x)=f(z)}\\) on a sample drawn from \\(\\mathcal{D}(z|A)\\) corresponds to pulling the arm \\(A\\).\nThe authors utilized a method called KL-LUCB (short for Kullback-Leibler Lower Confidence Bound), to determine the rule with the highest precision. However, explaining the intricacies of KL-LUCB in a brief blog post may be overly complex. Since our main focus is on Anchor’s theory rather than Reinforcement Learning, I will simplify this part by employing an \\(\\epsilon\\)-greedy approach.\n\nDifferences into algorithm approaches\nI asked ChatGPT to define this one and considering it is correct, I thought it would worth to put it here:\nThe Kullback-Leibler Lower Confidence Bound (KL-LUCB) and the \\(\\epsilon\\text{-greedy}\\) algorithms are two different approaches to the multi-armed bandit problem, which is a classical problem in probability theory and statistics that models the trade-off between exploration and exploitation in sequential decision-making.\nAn \\(\\epsilon\\text{-greedy}\\) algorithm is a simple approach where, with probability epsilon, the algorithm chooses an arm at random (exploration). With probability \\(1-\\epsilon\\), it selects the arm with the highest estimated reward (exploitation). The value of epsilon is usually set to a small value, and it can be decreased over time to favor exploitation over exploration.\nOn the other hand, the KL-LUCB calculates the confidence bounds (an upper one and a lower one on the expected rewards for each option using the Kullback-Leibler divergence, which quantifies the difference between two probability distributions. This allows KL-LUCB to make more informed decisions based on the uncertainty of the rewards.\nTherefore, for implementing the exploration vs. exploitation algorithm, we will not need to calculate the upper confidence and lower confidence bounds for precision, but rather we will always take the maximum precision or a random one, depending on \\(\\epsilon\\) value.\n\n\n\n\n\n\nImportant\n\n\n\nI’ll not paste the pseudo code for the other algorithms (BestCands and BeamSearch) shown in paper since I implemented variations of them by using \\(\\epsilon \\text{- greedy}\\) instead of KL-LUCB\n\n\n\n# Function to get a sample which satisfies the rules in an anchor\ndef sample_conditionally(X, anchor):\n    D_sample = X.copy()\n    for predicate in anchor:\n        mask = predicate(D_sample)\n        # Use the mask to sample from the training set\n        D_sample = D_sample[mask]\n    #we are interested in obtaining samples from minimum size of 2\n    if len(D_sample)&lt;=1: \n        return None\n    #D_sample will contain the instances where A(x)=1, where A is the anchor\n    # I decided to sample half of it, but we could use other values\n    rnd_indices = np.random.choice(len(D_sample), size=len(D_sample)//2)\n    D_sample = D_sample[rnd_indices]\n    return D_sample\n\n# Function to estimate the precision of an anchor\ndef estimate_precision(clf, X_train, x, anchor):\n    fx = clf.predict([x])\n    z = sample_conditionally(X_train, anchor)\n    if z is None: # No sample satisfies the anchor\n        return 0.0\n    fz = clf.predict(z)\n    return (fz==fx).mean()\n\n# Main function to find the best candidate to add to the anchor\ndef best_cand(anchor_candidates, X_train, x, clf, epsilon, num_samples=1000):\n    if len(anchor_candidates) == 0:\n        return None\n    num_arms = len(anchor_candidates)\n    Q = np.zeros(num_arms)\n    N = np.zeros(num_arms, dtype=int)\n    for _ in tqdm(range(num_samples), desc=\"Performing Epsilon Greedy approach\"):\n        if np.random.rand() &lt; epsilon: # Explore\n            arm = np.random.randint(num_arms)\n        else: # Exploit\n            arm = np.argmax(Q)\n        anchor = anchor_candidates[arm]\n        reward = estimate_precision(clf, X_train, x, anchor)\n        N[arm] += 1\n        Q[arm] += (reward - Q[arm]) / N[arm]\n    return anchor_candidates[np.argmax(Q)]\n\n\nbest_anchor = best_cand(A_r, X_train, x, clf, 0.3, num_samples=1000)\n\nPerforming Epsilon Greedy approach: 100%|██████████| 1000/1000 [00:16&lt;00:00, 59.52it/s]\n\n\n\n\nCode\nprint(\"Anchors final rule: \" + \"AND \".join(str(predicate) for predicate in best_anchor))\n\n\nAnchors final rule: x[0] &lt;= 7.2508364505367595"
  },
  {
    "objectID": "posts/2023-06-24-annotated-anchors.html#issues-with-greedy-search",
    "href": "posts/2023-06-24-annotated-anchors.html#issues-with-greedy-search",
    "title": "Distilling Anchors",
    "section": "Issues with Greedy Search",
    "text": "Issues with Greedy Search\nAs we can see, due to the greedy nature of the algorithm, it is impossible to search two different anchors “at the same time”. At every time, it will select the best candidate and incrementally augment it (so we can’t backtrack, for example). Also, the greedy search does not consider coverage, so it is biased to return the shortest anchor that respects the precision \\(\\tau\\) threshold.\nWe can extend the greedy approach to address these issues so that instead of taking the best candidate, we take the best \\(B\\) ones. Therefore, we turn a greedy search problem into beam-search. Given this set of \\(B\\) candidates, we search for the one with the highest coverage."
  },
  {
    "objectID": "posts/2023-06-24-annotated-anchors.html#footnotes",
    "href": "posts/2023-06-24-annotated-anchors.html#footnotes",
    "title": "Distilling Anchors",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDepending on the effort that I will need to put on that I will decide if I would continue or not↩︎\nChatGPT helped me to create this whole. It is be being a true ally for writing my posts as well :)↩︎"
  },
  {
    "objectID": "posts/2020-09-19-distilling-bert.html",
    "href": "posts/2020-09-19-distilling-bert.html",
    "title": "Distilling BERT Pre Training",
    "section": "",
    "text": "I remember someday of 2016 while I was starting my career as a Data Scientist when I’ve stumped into Chirs McCormick blog about Word2Vec. Honestly, I think that Tomas Mikolov paper was one of the most elegant and simple idea that I have ever found so far1 :)\n\n\nAccording to Pytorch documentation an Embedding can be defined as the following:\n\nA simple lookup table (…) of a fixed dictionary and size.\n\nThen, we can interpret embeddings as a simple way to convert integers into vectors of a given size. Then, for word embeddings, we can interpret simply as words that are encoded as integers, and then these integers serve as inputs for a vector space.’\nA have written some code with manim to illustrate this process:\n\nWe can then interpret each dimension as a single neuron of a hidden layer, and then these embedding numbers can be modified from a learning algorithm through a neural network. This is the main motivation behind Word Embeddings algorithms such as Word2Vec and fastText2\nNowadays, there are some libraries that provide already trained vectors based on a fixed and previously trained vocabulary. For instance, considerer the following Spacy code:\n\n\nCode\nimport spacy\n\nnlp = spacy.load(\"en_core_web_md\")\nprint(\"Coniderer the sentence 'The quick brown fox jumps over the lazy dog!!'\")\ntext = nlp(\"The quick brown fox jumps over the lazy dog!!\")\nfor word in text:\n    print(\n        f\"'{word.text}' vector representation has size of {word.vector.shape[0]}. Its first five elements are: {word.vector[:5].round(2)}\"\n    )\n\n\nConiderer the sentence 'The quick brown fox jumps over the lazy dog!!'\n'The' vector representation has size of 300. Its first five elements are: [ 0.27 -0.06 -0.19  0.02 -0.02]\n'quick' vector representation has size of 300. Its first five elements are: [-0.45  0.19 -0.25  0.47  0.16]\n'brown' vector representation has size of 300. Its first five elements are: [-0.37 -0.08  0.11  0.19  0.03]\n'fox' vector representation has size of 300. Its first five elements are: [-0.35 -0.08  0.18 -0.09 -0.45]\n'jumps' vector representation has size of 300. Its first five elements are: [-0.33  0.22 -0.35 -0.26  0.41]\n'over' vector representation has size of 300. Its first five elements are: [-0.3   0.01  0.04  0.1   0.12]\n'the' vector representation has size of 300. Its first five elements are: [ 0.27 -0.06 -0.19  0.02 -0.02]\n'lazy' vector representation has size of 300. Its first five elements are: [-0.35 -0.3  -0.18 -0.32 -0.39]\n'dog' vector representation has size of 300. Its first five elements are: [-0.4   0.37  0.02 -0.34  0.05]\n'!' vector representation has size of 300. Its first five elements are: [-0.27  0.34  0.22 -0.3  -0.06]\n'!' vector representation has size of 300. Its first five elements are: [-0.27  0.34  0.22 -0.3  -0.06]\n\n\nContains word representations that were trained on Common Crawl data using GloVe algorithm. Unlike the example that I used initially, the word ‘!’ was encoded as well.\nWe can combine different words to form the embedding of a phrase. According to spacy documentation: &gt; Models that come with built-in word vectors make them available as the Token.vector attribute. Doc.vector and Span.vector will default to an average of their token vectors.\nThen, the phrase the we are using as example has the following single representation:\n\n\nFirst 5 values of 'The quick brown fox jumps over the lazy dog!!': [-0.23  0.08 -0.03 -0.07 -0.02]\n\n\n\n\n\nEven though Word Embeddings brings many benefits in the realm of computational linguistics, they have some limitations. There is a linguistic phenomenon called polysemy. According to wikipedia: &gt; A polyseme is a word or phrase with different, but related senses.(…) English has many polysemous words. For example, the verb “to get” can mean “procure” (I’ll get the drinks), “become” (she got scared), “understand” (I get it) etc.\nSo considering the example above, despite the fact that the verb has different meaning depending on the contexts, it’s word representation would always be the same\n\n\nFirst 5 values of verb 'to get' vector: [ 0.03  0.12 -0.32  0.13  0.12]\n\n\nThen, if we pick two phrases: She got scared and She understand it, we will get the following vectors\n\ntext1 = nlp(\"He will get scared\")\ntext2 = nlp(\"She will get the drinks\")\n\nprint(f\"First 5 values of sentence '{text1}' vector: {text1.vector[:5].round(2)}\")\nprint(f\"First 5 values of sentence '{text2}' vector: {text2.vector[:5].round(2)}\")\n\nFirst 5 values of verb 'He will get scared' vector: [-0.12  0.19 -0.21 -0.14  0.09]\nFirst 5 values of verb 'She will get the drinks' vector: [ 0.01  0.13 -0.04 -0.08  0.03]\n\n\nThen, if we take the cosine similarity by taking the average of the word vectors:\n\n\nCode\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nprint(\n    f\"Similarity between:\\n '{text1}' and '{text2}': \"\n    f\"{cosine_similarity(text1.vector.reshape(1, -1),text2.vector.reshape(1, -1))[0][0]}\"\n)\n\n\nSimlarity between:\n 'He will get scared' and 'She will get the drinks': 0.8653444051742554\n\n\nThis indicates that both vectors would be a lot similar. However, the reason for that is the usage of similar words, even considering that they were applied in different contexts! So there is the objective that BERT tries to solve.{% fn 3 %}\n{{ ‘There are some BERT percursors such as ELMo; ULMFit and Open AI Transformer that I am not going to cover here. Please reach out to Illustrated BERT blog to know more’ | fndetail: 3 }}"
  },
  {
    "objectID": "posts/2020-09-19-distilling-bert.html#what-are-word-embeddings",
    "href": "posts/2020-09-19-distilling-bert.html#what-are-word-embeddings",
    "title": "Distilling BERT Pre Training",
    "section": "",
    "text": "According to Pytorch documentation an Embedding can be defined as the following:\n\nA simple lookup table (…) of a fixed dictionary and size.\n\nThen, we can interpret embeddings as a simple way to convert integers into vectors of a given size. Then, for word embeddings, we can interpret simply as words that are encoded as integers, and then these integers serve as inputs for a vector space.’\nA have written some code with manim to illustrate this process:\n\nWe can then interpret each dimension as a single neuron of a hidden layer, and then these embedding numbers can be modified from a learning algorithm through a neural network. This is the main motivation behind Word Embeddings algorithms such as Word2Vec and fastText2\nNowadays, there are some libraries that provide already trained vectors based on a fixed and previously trained vocabulary. For instance, considerer the following Spacy code:\n\n\nCode\nimport spacy\n\nnlp = spacy.load(\"en_core_web_md\")\nprint(\"Coniderer the sentence 'The quick brown fox jumps over the lazy dog!!'\")\ntext = nlp(\"The quick brown fox jumps over the lazy dog!!\")\nfor word in text:\n    print(\n        f\"'{word.text}' vector representation has size of {word.vector.shape[0]}. Its first five elements are: {word.vector[:5].round(2)}\"\n    )\n\n\nConiderer the sentence 'The quick brown fox jumps over the lazy dog!!'\n'The' vector representation has size of 300. Its first five elements are: [ 0.27 -0.06 -0.19  0.02 -0.02]\n'quick' vector representation has size of 300. Its first five elements are: [-0.45  0.19 -0.25  0.47  0.16]\n'brown' vector representation has size of 300. Its first five elements are: [-0.37 -0.08  0.11  0.19  0.03]\n'fox' vector representation has size of 300. Its first five elements are: [-0.35 -0.08  0.18 -0.09 -0.45]\n'jumps' vector representation has size of 300. Its first five elements are: [-0.33  0.22 -0.35 -0.26  0.41]\n'over' vector representation has size of 300. Its first five elements are: [-0.3   0.01  0.04  0.1   0.12]\n'the' vector representation has size of 300. Its first five elements are: [ 0.27 -0.06 -0.19  0.02 -0.02]\n'lazy' vector representation has size of 300. Its first five elements are: [-0.35 -0.3  -0.18 -0.32 -0.39]\n'dog' vector representation has size of 300. Its first five elements are: [-0.4   0.37  0.02 -0.34  0.05]\n'!' vector representation has size of 300. Its first five elements are: [-0.27  0.34  0.22 -0.3  -0.06]\n'!' vector representation has size of 300. Its first five elements are: [-0.27  0.34  0.22 -0.3  -0.06]\n\n\nContains word representations that were trained on Common Crawl data using GloVe algorithm. Unlike the example that I used initially, the word ‘!’ was encoded as well.\nWe can combine different words to form the embedding of a phrase. According to spacy documentation: &gt; Models that come with built-in word vectors make them available as the Token.vector attribute. Doc.vector and Span.vector will default to an average of their token vectors.\nThen, the phrase the we are using as example has the following single representation:\n\n\nFirst 5 values of 'The quick brown fox jumps over the lazy dog!!': [-0.23  0.08 -0.03 -0.07 -0.02]"
  },
  {
    "objectID": "posts/2020-09-19-distilling-bert.html#limitations-of-word-embeddings",
    "href": "posts/2020-09-19-distilling-bert.html#limitations-of-word-embeddings",
    "title": "Distilling BERT Pre Training",
    "section": "",
    "text": "Even though Word Embeddings brings many benefits in the realm of computational linguistics, they have some limitations. There is a linguistic phenomenon called polysemy. According to wikipedia: &gt; A polyseme is a word or phrase with different, but related senses.(…) English has many polysemous words. For example, the verb “to get” can mean “procure” (I’ll get the drinks), “become” (she got scared), “understand” (I get it) etc.\nSo considering the example above, despite the fact that the verb has different meaning depending on the contexts, it’s word representation would always be the same\n\n\nFirst 5 values of verb 'to get' vector: [ 0.03  0.12 -0.32  0.13  0.12]\n\n\nThen, if we pick two phrases: She got scared and She understand it, we will get the following vectors\n\ntext1 = nlp(\"He will get scared\")\ntext2 = nlp(\"She will get the drinks\")\n\nprint(f\"First 5 values of sentence '{text1}' vector: {text1.vector[:5].round(2)}\")\nprint(f\"First 5 values of sentence '{text2}' vector: {text2.vector[:5].round(2)}\")\n\nFirst 5 values of verb 'He will get scared' vector: [-0.12  0.19 -0.21 -0.14  0.09]\nFirst 5 values of verb 'She will get the drinks' vector: [ 0.01  0.13 -0.04 -0.08  0.03]\n\n\nThen, if we take the cosine similarity by taking the average of the word vectors:\n\n\nCode\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nprint(\n    f\"Similarity between:\\n '{text1}' and '{text2}': \"\n    f\"{cosine_similarity(text1.vector.reshape(1, -1),text2.vector.reshape(1, -1))[0][0]}\"\n)\n\n\nSimlarity between:\n 'He will get scared' and 'She will get the drinks': 0.8653444051742554\n\n\nThis indicates that both vectors would be a lot similar. However, the reason for that is the usage of similar words, even considering that they were applied in different contexts! So there is the objective that BERT tries to solve.{% fn 3 %}\n{{ ‘There are some BERT percursors such as ELMo; ULMFit and Open AI Transformer that I am not going to cover here. Please reach out to Illustrated BERT blog to know more’ | fndetail: 3 }}"
  },
  {
    "objectID": "posts/2020-09-19-distilling-bert.html#attention-is-all-you-need",
    "href": "posts/2020-09-19-distilling-bert.html#attention-is-all-you-need",
    "title": "Distilling BERT Pre Training",
    "section": "Attention is all you need",
    "text": "Attention is all you need\nThe Attention is all you need paper have introduced the Transformer architeture for us :) In sense, it can be summarized as the picture below:\n\nStrictly speaking, the motivation behind the paper is that RNN-like architetures are memory-expensive. The purpose behind Transformer models is that it you can achieve similar results using more computer efficient resources by applying just attention mechanisms (and exluding the CNN or RNN-like architetures) !{% fn 4 %} Despite the fact that the Transformer model was proposed to deal with translation problems, it turns out that we can also use variations of it to achieve awesome results in different tasks. This is the motivation behind BERT!\n{{ ‘The NLP group from Harvard has written a great blog post distilling the paper as well as implementing them in pytorch. If you have some interest in knowing details about the transformer architecture, I recommend looking at it!’ | fndetail: 4 }}\n\nAttention?\nAccording to the Transformer and Attention lecture from NYU foundations of Deep Learning Course:\n\nTransformers are made up of attention modules, which are mappings between sets, rather than sequences, which means we do not impose an ordering to our inputs/outputs.\n\nWhen we analyze the transformer architeture, we can see that both Multi-Head Attention and Multi-Head Masked Attention box have 3 Arrow Heads. Each one represents one of the following:\n\nQ that stands for query vector with dimension \\(d_k\\)\nK that stands for key vector that also has dimension \\(d_k\\)\nV that stands for value vector that also has dimension \\(d_v\\)\n\nKV pair can be understood as the encoded representation of the input whereas the Q is the output of a previous layer.\n\n\nKey-Value Store\nAgain, from the Deep Learning Foundations Course from NYU:\n\nA key-value store is a paradigm designed for storing (saving), retrieving (querying), and managing associative arrays (dictionaries/hash tables)\n\n\nFor example, say we wanted to find a recipe to make lasagne. We have a recipe book and search for “lasagne” - this is the query. This query is checked against all possible keys in your dataset - in this case, this could be the titles of all the recipes in the book. We check how aligned the query is with each title to find the maximum matching score between the query and all the respective keys. If our output is the argmax function - we retrieve the single recipe with the highest score. Otherwise, if we use a soft argmax function, we would get a probability distribution and can retrieve in order from the most similar content to less and less relevant recipes matching the query.\n\n\nBasically, the query is the question. Given one query, we check this query against every key and retrieve all matching content.\n\n\n\n\n\n\n\nWarning\n\n\n\nI have decided not to cover attention concepts in this post, giving just a higher-level introduction. As you might have noticed, NYU Deep Learning Foundations Course provides a really nice introduction about the topic that I recommend going through if you want to learn more :)\n\n\n\n\n\n\n\n\nNote\n\n\n\nAttention can be basically understood as measure of correlation of words between a set of sentences. For those interested to learn a little bit more, I highly recommend this blog post\n\n\n\n\nPositional Encoding\nThis was taken from The annotated transformer blog where you can find a cool pytorch implementation. It turns out that actually this is a quote from Attention is all you need paper:\n\nSince our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add “positional encodings” to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension \\(d_{model}\\) as the embeddings, so that the two can be summed"
  },
  {
    "objectID": "posts/2020-09-19-distilling-bert.html#the-bert-model",
    "href": "posts/2020-09-19-distilling-bert.html#the-bert-model",
    "title": "Distilling BERT Pre Training",
    "section": "The BERT model",
    "text": "The BERT model\nBERT model itself is an encoder model only from the transformer model. Considering the models trained from the paper, the base model consists of 12 encoder-stacked layers and the large model consists of 24 encoder-stacked layers.\nAccording to the Attention is all you need paper:\n\nThe encoder is composed of a stack of \\(N = 6\\) identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position wise fully connected feed-forward network. We employ a residual connection around each of the two sub-layers, followed by layer normalization.\n\n\n\nThe Multi-Head Attention\nBasically, the multi head attention is a type of an attention mechanism. It is a concatenation of another type of attention, the scaled dot. Both mechanisms works together as represented in the following image:\n\n\n\n\n\n\n\nNote\n\n\n\nScaled Dot-Product Attention is calculated by \\(softmax(\\frac{QK^T}{\\sqrt{n}})V\\), where K, V and Q are the same as the ones described in a previous section whereas n represents the number of elements in the set.\n\n\nHere, h, or the number o attention heads (or layers) is equal to \\(12\\) in the case of \\(\\text{BERT}_\\text{base}\\) and \\(16\\) in the case of \\(\\text{BERT}_\\text{large}\\)\n\n\nResidual Conections\nEach sublayer of the encoder stack contains a residual connection (the left curved arrow) added to the sublayer output before layer normalization. The idea of Residual Conections came from Computer Vision domain, and actually, it is a relatively simple technique that can be summarized by the following image:\n\nConsidering the image above and the case of Encoder stack, each \\(\\mathcal{F}(x)\\) means either the Multi-Head Attention or Feed Forward. Therefore, quoting the paper:\n\nThat is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension \\(d_{model} = 512\\) {% fn 5 %}.\n\n{{ ‘In the case of BERT model, please have in mind that \\(N\\) is either \\(12\\) (BERTbase) or \\(24\\) ((BERTlarge) and dmodel is 768 for BERT base and 1024 for BERT large’ | fndetail: 5 }}\nThen, what, in fact, is being encoded?"
  },
  {
    "objectID": "posts/2020-09-19-distilling-bert.html#embedding-representation",
    "href": "posts/2020-09-19-distilling-bert.html#embedding-representation",
    "title": "Distilling BERT Pre Training",
    "section": "Embedding Representation",
    "text": "Embedding Representation\nThe authors would like to make BERT to perform well in different downstream tasks such as binary and multi lablel classification; language modeling; question and answering; named entity recognition; etc. Therefore, they said the following:\n\nour input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., h Question, Answer) in one token sequence. Throughout this work, a “sentence” can be an arbitrary span of contiguous text, rather than an actual linguistic sentence. A “sequence” refers to the input token sequence to BERT, which may be a single sentence or two sentences packed together\n\nIn order to perform and create the sentence embeddings, WordPiece tokenize is applied. Then, besides adding [CLS] token, pairs of sentence (e.g. sentence A and B) are concatenated into a single sentence, being separated with a special token [SEP] (e.g. A [SEP] B).\nThen:\n\nFor a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings."
  },
  {
    "objectID": "posts/2020-09-19-distilling-bert.html#masked-language-model-mlm",
    "href": "posts/2020-09-19-distilling-bert.html#masked-language-model-mlm",
    "title": "Distilling BERT Pre Training",
    "section": "Masked Language Model (MLM)",
    "text": "Masked Language Model (MLM)\nAs we are feeding the whole sentence into the model, it is possible to say that the model is bidirectional and hence as we are trying to predict the next word in a sentence, it would has access to it! Then, the idea behind this task is pretty simple. We can directly quote from the paper:\n\nUnfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirectional conditioning would allow each word to indirectly “see itself”, and the model could trivially predict the target word in a multi-layered context.\n\n\nIn order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. We refer to this procedure as a “masked LM” (MLM), although it is often referred to as a Cloze task in the literature. In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM.\n\nIn the case of BERT model, 15% of each sentence were masked during training."
  },
  {
    "objectID": "posts/2020-09-19-distilling-bert.html#next-sentence-prediction-nsp",
    "href": "posts/2020-09-19-distilling-bert.html#next-sentence-prediction-nsp",
    "title": "Distilling BERT Pre Training",
    "section": "Next Sentence Prediction (NSP)",
    "text": "Next Sentence Prediction (NSP)\nIn order to learn relationships between pair of sentence (e.g. Question and Ansering tasks) the authors needed a different approach than plain Language Modeling. Then:\n\nIn order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus. Specifically, when choosing the sentences A and B for each pretraining example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext).\n\nOnce defined, both objected functions are used in BERT Pre training learning :)\n\n\n\n\n\n\n\nNote\n\n\n\nThe training loss is the sum of the mean masked LM (MLM) likelihood and the mean next sentence prediction (NSP) likelihood\n\n\n\n\n\n\n\n\nImportant\n\n\n\nYou may have noticed but this training procedure does not require labeling. As we are using the raw text inputs to generate the labels during training, e considerer this BERT Pre Training as a self-surpervised model!"
  },
  {
    "objectID": "posts/2020-09-19-distilling-bert.html#footnotes",
    "href": "posts/2020-09-19-distilling-bert.html#footnotes",
    "title": "Distilling BERT Pre Training",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFun Fact: Whereas nowadays Miklov LinkedIn profile points out that he has worked for Microsoft, Google and Facebook; another of W2V authors, Ilya Sutskever worked with some of the prestigious researchers in the recent AI area, such as Geoffrey Hinton and Andrew Ng. Moreover, he is one of the founders of Open AI!↩︎\nI am not going to cover word embeddings through this blog post. If you are not familiarized with them, I highly recommend this; this and this as potential resources :)↩︎"
  }
]