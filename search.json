[
  {
    "objectID": "posts/2021-02-21-downstream-bert-qa.html",
    "href": "posts/2021-02-21-downstream-bert-qa.html",
    "title": "BERT applied to Multiple Choice",
    "section": "",
    "text": "Drilling down Multiple Choice downstream task\nWhen I started studying Language Models, I remember when I’ve found the following image from Open AI transformer paper (Radford and Narasimhan 2018):\n\nHowever, the only difference is that the input data should be slightly different:\n\nFor these tasks, we are given a context document \\(z\\), a question \\(q\\), and a set of possible answers \\({a_k}\\). We concatenate the document context and question with each possible answer, adding a delimiter token in between to get [\\(z\\); \\(q\\); $ $ $; \\(a_k\\)]. Each of these sequences are processed independently with our model and then normalized via a softmax layer to produce an output distribution over possible answers.\n\nTherefore, these inputs could be optimized via Categorical Cross Entropy Loss, where \\(C\\) is the number of options available. For a specific question.\n\n\nFrom GPT to BERT\nAs we will see with Hugging Face’s transformer library, when we considerer application from a fine tuning task, the approach of BERT can be derived directly from the tecnique presented by (Radford and Narasimhan 2018). It is possible to check it from documentation\n\nBert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a softmax) e.g. for RocStories/SWAG tasks.\n\n\n\nCode\nimport numpy as np\nimport torch\nfrom transformers import BertTokenizer, BertForMultipleChoice\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\nmodel = BertForMultipleChoice.from_pretrained(\"bert-base-uncased\")\n\n\nDownloading: 100%|██████████| 232k/232k [00:01&lt;00:00, 171kB/s]\nDownloading: 100%|██████████| 433/433 [00:00&lt;00:00, 122kB/s]\nDownloading: 100%|██████████| 440M/440M [02:06&lt;00:00, 3.48MB/s]\nSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultipleChoice: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n\n\n\nquestion = \"George wants to warm his hands quickly by rubbing them. Which skin surface will produce the most heat?\"\noption_a = \"dry palms\"\noption_b = \"wet palms\"\noption_c = \"palms covered with oil\"\noption_d = \"palms covered with lotion\"\n\nIn this case, option A is the correct one. Furthermore, the batch size here would be 1\n\nlabels = torch.tensor(0).unsqueeze(0) \n\nNotice that the question is the same for each option\n\nencoding = tokenizer(\n            [question, question, question, question],\n            [option_a, option_b, option_c, option_d],\n            return_tensors='pt',\n            padding=True\n           )\n\noutputs = model(**{k: v.unsqueeze(0) for k,v in encoding.items()}, labels=labels)\n\n\n\n\n\n\n\nImportant\n\n\n\nNotice that if we have a dataset such as SquaD where each question comes with a context, we could append this context to either the question text or the option text and we would then have the tuple cited by Open AI transformer paper\n\n\nThe output is a linear layer which would still be trained through a Cross Entropy loss. Then, as stated by the documentation, we still need to apply softmax to the logits\n\nloss = outputs.loss\nlogits = outputs.logits\n\nLinear Logits output:\n\n\ntensor([[-0.3457, -0.3295, -0.3271, -0.3342]], grad_fn=&lt;ViewBackward&gt;)\n\n\nLogits after the softmax function. Since this model did not learn anything, the result below is expected:\n\n\ntensor([[0.2471, 0.2511, 0.2518, 0.2500]], grad_fn=&lt;SoftmaxBackward&gt;)\n\n\n\n\nConclusion\nCongratulations! Adding up with the first part, you have learned the end-to-end BERT Flow :)\n\n\n\n\n\nReferences\n\nRadford, Alec, and Karthik Narasimhan. 2018. “Improving Language Understanding by Generative Pre-Training.” In."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Andre’s Personal Blog :)",
    "section": "",
    "text": "BERT applied to Multiple Choice\n\n\n\n\n\n\n\nmasters\n\n\nnlp\n\n\nknowledge-distill\n\n\n\n\nStep by step about a specific fine tuning task:)\n\n\n\n\n\n\nFeb 21, 2021\n\n\nAndre Barbosa\n\n\n\n\n\n\n  \n\n\n\n\nDistilling BERT Pre Training\n\n\n\n\n\n\n\nmasters\n\n\nnlp\n\n\nknowledge-distill\n\n\n\n\nStep by step about its inner work from scratch :)\n\n\n\n\n\n\nSep 19, 2020\n\n\nAndre Barbosa\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi there! My name is André Barbosa and I’m currently a Master Student@Institute of Mathemathics and Statistics as well as a Data Scientist@QuintoAndar, a real state company. My current reserch interests include the following:\n-Natural Language Processing -Causal Inference\nFell free to follow me on social media and contact me :)"
  },
  {
    "objectID": "posts/2020-09-19-distilling-bert.html",
    "href": "posts/2020-09-19-distilling-bert.html",
    "title": "Distilling BERT Pre Training",
    "section": "",
    "text": "Note\n\n\n\nFor a portuguese version of this post, please check this"
  },
  {
    "objectID": "posts/2020-09-19-distilling-bert.html#what-are-word-embeddings",
    "href": "posts/2020-09-19-distilling-bert.html#what-are-word-embeddings",
    "title": "Distilling BERT Pre Training",
    "section": "What are Word Embeddings",
    "text": "What are Word Embeddings\nAccording to Pytorch documentation an Embedding can be defined as the following:\n\nA simple lookup table (…) of a fixed dictionary and size.\n\nThen, we can interpret embeddings as a simple way to convert integers into vectors of a given size. Then, for word embeddings, we can interpret simply as words that are encoded as integers, and then these integers serve as inputs for a vector space.’\nA have written some code with manim to illustrate this process:\n\nWe can then interpret each dimension as a single neuron of a hidden layer, and then these embedding numbers can be modified from a learning algorithm through a neural network. This is the main motivation behind Word Embeddings algorithms such as Word2Vec and fastText2\nNowadays, there are some libraries that provide already trained vectors based on a fixed and previously trained vocabulary. For instance, considerer the following Spacy code:\n\n\nCode\nimport spacy\n\nnlp = spacy.load(\"en_core_web_md\")\nprint(\"Coniderer the sentence 'The quick brown fox jumps over the lazy dog!!'\")\ntext = nlp(\"The quick brown fox jumps over the lazy dog!!\")\nfor word in text:\n    print(\n        f\"'{word.text}' vector representation has size of {word.vector.shape[0]}. Its first five elements are: {word.vector[:5].round(2)}\"\n    )\n\n\nConiderer the sentence 'The quick brown fox jumps over the lazy dog!!'\n'The' vector representation has size of 300. Its first five elements are: [ 0.27 -0.06 -0.19  0.02 -0.02]\n'quick' vector representation has size of 300. Its first five elements are: [-0.45  0.19 -0.25  0.47  0.16]\n'brown' vector representation has size of 300. Its first five elements are: [-0.37 -0.08  0.11  0.19  0.03]\n'fox' vector representation has size of 300. Its first five elements are: [-0.35 -0.08  0.18 -0.09 -0.45]\n'jumps' vector representation has size of 300. Its first five elements are: [-0.33  0.22 -0.35 -0.26  0.41]\n'over' vector representation has size of 300. Its first five elements are: [-0.3   0.01  0.04  0.1   0.12]\n'the' vector representation has size of 300. Its first five elements are: [ 0.27 -0.06 -0.19  0.02 -0.02]\n'lazy' vector representation has size of 300. Its first five elements are: [-0.35 -0.3  -0.18 -0.32 -0.39]\n'dog' vector representation has size of 300. Its first five elements are: [-0.4   0.37  0.02 -0.34  0.05]\n'!' vector representation has size of 300. Its first five elements are: [-0.27  0.34  0.22 -0.3  -0.06]\n'!' vector representation has size of 300. Its first five elements are: [-0.27  0.34  0.22 -0.3  -0.06]\n\n\nContains word representations that were trained on Common Crawl data using GloVe algorithm. Unlike the example that I used initially, the word ‘!’ was encoded as well.\nWe can combine different words to form the embedding of a phrase. According to spacy documentation: &gt; Models that come with built-in word vectors make them available as the Token.vector attribute. Doc.vector and Span.vector will default to an average of their token vectors.\nThen, the phrase the we are using as example has the following single representation:\n\n\nFirst 5 values of 'The quick brown fox jumps over the lazy dog!!': [-0.23  0.08 -0.03 -0.07 -0.02]"
  },
  {
    "objectID": "posts/2020-09-19-distilling-bert.html#limitations-of-word-embeddings",
    "href": "posts/2020-09-19-distilling-bert.html#limitations-of-word-embeddings",
    "title": "Distilling BERT Pre Training",
    "section": "Limitations of Word Embeddings",
    "text": "Limitations of Word Embeddings\nEven though Word Embeddings brings many benefits in the realm of computational linguistics, they have some limitations. There is a linguistic phenomenon called polysemy. According to wikipedia: &gt; A polyseme is a word or phrase with different, but related senses.(…) English has many polysemous words. For example, the verb “to get” can mean “procure” (I’ll get the drinks), “become” (she got scared), “understand” (I get it) etc.\nSo considering the example above, despite the fact that the verb has different meaning depending on the contexts, it’s word representation would always be the same\n\n\nFirst 5 values of verb 'to get' vector: [ 0.03  0.12 -0.32  0.13  0.12]\n\n\nThen, if we pick two phrases: She got scared and She understand it, we will get the following vectors\n\ntext1 = nlp(\"He will get scared\")\ntext2 = nlp(\"She will get the drinks\")\n\nprint(f\"First 5 values of sentence '{text1}' vector: {text1.vector[:5].round(2)}\")\nprint(f\"First 5 values of sentence '{text2}' vector: {text2.vector[:5].round(2)}\")\n\nFirst 5 values of verb 'He will get scared' vector: [-0.12  0.19 -0.21 -0.14  0.09]\nFirst 5 values of verb 'She will get the drinks' vector: [ 0.01  0.13 -0.04 -0.08  0.03]\n\n\nThen, if we take the cosine similarity by taking the average of the word vectors:\n\n\nCode\nfrom sklearn.metrics.pairwise import cosine_similarity\n\nprint(\n    f\"Similarity between:\\n '{text1}' and '{text2}': \"\n    f\"{cosine_similarity(text1.vector.reshape(1, -1),text2.vector.reshape(1, -1))[0][0]}\"\n)\n\n\nSimlarity between:\n 'He will get scared' and 'She will get the drinks': 0.8653444051742554\n\n\nThis indicates that both vectors would be a lot similar. However, the reason for that is the usage of similar words, even considering that they were applied in different contexts! So there is the objective that BERT tries to solve.{% fn 3 %}\n{{ ‘There are some BERT percursors such as ELMo; ULMFit and Open AI Transformer that I am not going to cover here. Please reach out to Illustrated BERT blog to know more’ | fndetail: 3 }}"
  },
  {
    "objectID": "posts/2020-09-19-distilling-bert.html#attention-is-all-you-need",
    "href": "posts/2020-09-19-distilling-bert.html#attention-is-all-you-need",
    "title": "Distilling BERT Pre Training",
    "section": "Attention is all you need",
    "text": "Attention is all you need\nThe Attention is all you need paper have introduced the Transformer architeture for us :) In sense, it can be summarized as the picture below:\n\nStrictly speaking, the motivation behind the paper is that RNN-like architetures are memory-expensive. The purpose behind Transformer models is that it you can achieve similar results using more computer efficient resources by applying just attention mechanisms (and exluding the CNN or RNN-like architetures) !{% fn 4 %} Despite the fact that the Transformer model was proposed to deal with translation problems, it turns out that we can also use variations of it to achieve awesome results in different tasks. This is the motivation behind BERT!\n{{ ‘The NLP group from Harvard has written a great blog post distilling the paper as well as implementing them in pytorch. If you have some interest in knowing details about the transformer architecture, I recommend looking at it!’ | fndetail: 4 }}\n\nAttention?\nAccording to the Transformer and Attention lecture from NYU foundations of Deep Learning Course:\n\nTransformers are made up of attention modules, which are mappings between sets, rather than sequences, which means we do not impose an ordering to our inputs/outputs.\n\nWhen we analyze the transformer architeture, we can see that both Multi-Head Attention and Multi-Head Masked Attention box have 3 Arrow Heads. Each one represents one of the following:\n\nQ that stands for query vector with dimension \\(d_k\\)\nK that stands for key vector that also has dimension \\(d_k\\)\nV that stands for value vector that also has dimension \\(d_v\\)\n\nKV pair can be understood as the encoded representation of the input whereas the Q is the output of a previous layer.\n\n\nKey-Value Store\nAgain, from the Deep Learning Foundations Course from NYU:\n\nA key-value store is a paradigm designed for storing (saving), retrieving (querying), and managing associative arrays (dictionaries/hash tables)\n\n\nFor example, say we wanted to find a recipe to make lasagne. We have a recipe book and search for “lasagne” - this is the query. This query is checked against all possible keys in your dataset - in this case, this could be the titles of all the recipes in the book. We check how aligned the query is with each title to find the maximum matching score between the query and all the respective keys. If our output is the argmax function - we retrieve the single recipe with the highest score. Otherwise, if we use a soft argmax function, we would get a probability distribution and can retrieve in order from the most similar content to less and less relevant recipes matching the query.\n\n\nBasically, the query is the question. Given one query, we check this query against every key and retrieve all matching content.\n\n\n\n\n\n\n\nWarning\n\n\n\nI have decided not to cover attention concepts in this post, giving just a higher-level introduction. As you might have noticed, NYU Deep Learning Foundations Course provides a really nice introduction about the topic that I recommend going through if you want to learn more :)\n\n\n\n\n\n\n\n\nNote\n\n\n\nAttention can be basically understood as measure of correlation of words between a set of sentences. For those interested to learn a little bit more, I highly recommend this blog post\n\n\n\n\nPositional Encoding\nThis was taken from The annotated transformer blog where you can find a cool pytorch implementation. It turns out that actually this is a quote from Attention is all you need paper:\n\nSince our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add “positional encodings” to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension \\(d_{model}\\) as the embeddings, so that the two can be summed"
  },
  {
    "objectID": "posts/2020-09-19-distilling-bert.html#the-bert-model",
    "href": "posts/2020-09-19-distilling-bert.html#the-bert-model",
    "title": "Distilling BERT Pre Training",
    "section": "The BERT model",
    "text": "The BERT model\nBERT model itself is an encoder model only from the transformer model. Considering the models trained from the paper, the base model consists of 12 encoder-stacked layers and the large model consists of 24 encoder-stacked layers.\nAccording to the Attention is all you need paper:\n\nThe encoder is composed of a stack of \\(N = 6\\) identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position wise fully connected feed-forward network. We employ a residual connection around each of the two sub-layers, followed by layer normalization.\n\n\n\nThe Multi-Head Attention\nBasically, the multi head attention is a type of an attention mechanism. It is a concatenation of another type of attention, the scaled dot. Both mechanisms works together as represented in the following image:\n\n\n\n\n\n\n\nNote\n\n\n\nScaled Dot-Product Attention is calculated by \\(softmax(\\frac{QK^T}{\\sqrt{n}})V\\), where K, V and Q are the same as the ones described in a previous section whereas n represents the number of elements in the set.\n\n\nHere, h, or the number o attention heads (or layers) is equal to \\(12\\) in the case of \\(\\text{BERT}_\\text{base}\\) and \\(16\\) in the case of \\(\\text{BERT}_\\text{large}\\)\n\n\nResidual Conections\nEach sublayer of the encoder stack contains a residual connection (the left curved arrow) added to the sublayer output before layer normalization. The idea of Residual Conections came from Computer Vision domain, and actually, it is a relatively simple technique that can be summarized by the following image:\n\nConsidering the image above and the case of Encoder stack, each \\(\\mathcal{F}(x)\\) means either the Multi-Head Attention or Feed Forward. Therefore, quoting the paper:\n\nThat is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself. To facilitate these residual connections, all sub-layers in the model, as well as the embedding layers, produce outputs of dimension \\(d_{model} = 512\\) {% fn 5 %}.\n\n{{ ‘In the case of BERT model, please have in mind that \\(N\\) is either \\(12\\) (BERTbase) or \\(24\\) ((BERTlarge) and dmodel is 768 for BERT base and 1024 for BERT large’ | fndetail: 5 }}\nThen, what, in fact, is being encoded?"
  },
  {
    "objectID": "posts/2020-09-19-distilling-bert.html#embedding-representation",
    "href": "posts/2020-09-19-distilling-bert.html#embedding-representation",
    "title": "Distilling BERT Pre Training",
    "section": "Embedding Representation",
    "text": "Embedding Representation\nThe authors would like to make BERT to perform well in different downstream tasks such as binary and multi lablel classification; language modeling; question and answering; named entity recognition; etc. Therefore, they said the following:\n\nour input representation is able to unambiguously represent both a single sentence and a pair of sentences (e.g., h Question, Answer) in one token sequence. Throughout this work, a “sentence” can be an arbitrary span of contiguous text, rather than an actual linguistic sentence. A “sequence” refers to the input token sequence to BERT, which may be a single sentence or two sentences packed together\n\nIn order to perform and create the sentence embeddings, WordPiece tokenize is applied. Then, besides adding [CLS] token, pairs of sentence (e.g. sentence A and B) are concatenated into a single sentence, being separated with a special token [SEP] (e.g. A [SEP] B).\nThen:\n\nFor a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings."
  },
  {
    "objectID": "posts/2020-09-19-distilling-bert.html#masked-language-model-mlm",
    "href": "posts/2020-09-19-distilling-bert.html#masked-language-model-mlm",
    "title": "Distilling BERT Pre Training",
    "section": "Masked Language Model (MLM)",
    "text": "Masked Language Model (MLM)\nAs we are feeding the whole sentence into the model, it is possible to say that the model is bidirectional and hence as we are trying to predict the next word in a sentence, it would has access to it! Then, the idea behind this task is pretty simple. We can directly quote from the paper:\n\nUnfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirectional conditioning would allow each word to indirectly “see itself”, and the model could trivially predict the target word in a multi-layered context.\n\n\nIn order to train a deep bidirectional representation, we simply mask some percentage of the input tokens at random, and then predict those masked tokens. We refer to this procedure as a “masked LM” (MLM), although it is often referred to as a Cloze task in the literature. In this case, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM.\n\nIn the case of BERT model, 15% of each sentence were masked during training."
  },
  {
    "objectID": "posts/2020-09-19-distilling-bert.html#next-sentence-prediction-nsp",
    "href": "posts/2020-09-19-distilling-bert.html#next-sentence-prediction-nsp",
    "title": "Distilling BERT Pre Training",
    "section": "Next Sentence Prediction (NSP)",
    "text": "Next Sentence Prediction (NSP)\nIn order to learn relationships between pair of sentence (e.g. Question and Ansering tasks) the authors needed a different approach than plain Language Modeling. Then:\n\nIn order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus. Specifically, when choosing the sentences A and B for each pretraining example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext).\n\nOnce defined, both objected functions are used in BERT Pre training learning :)\n\n\n\n\n\n\n\nNote\n\n\n\nThe training loss is the sum of the mean masked LM (MLM) likelihood and the mean next sentence prediction (NSP) likelihood\n\n\n\n\n\n\n\n\nImportant\n\n\n\nYou may have noticed but this training procedure does not require labeling. As we are using the raw text inputs to generate the labels during training, e considerer this BERT Pre Training as a self-surpervised model!"
  },
  {
    "objectID": "posts/2020-09-19-distilling-bert.html#footnotes",
    "href": "posts/2020-09-19-distilling-bert.html#footnotes",
    "title": "Distilling BERT Pre Training",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFun Fact: Whereas nowadays Miklov LinkedIn profile points out that he has worked for Microsoft, Google and Facebook; another of W2V authors, Ilya Sutskever worked with some of the prestigious researchers in the recent AI area, such as Geoffrey Hinton and Andrew Ng. Moreover, he is one of the founders of Open AI!↩︎\nI am not going to cover word embeddings through this blog post. If you are not familiarized with them, I highly recommend this; this and this as potential resources :)↩︎"
  }
]