---
keywords: fastai
description: "Step by step about a specific fine tuning task:)"
title: "BERT applied to Multiple Choice"
toc: true
branch: master
author: Andre Barbosa
badges: true
hide_binder_badge: false
hide_colab_badge: false
comments: true
categories: [masters, nlp, knowledge-distill]
hide: false
search_exclude: false
nb_path: _notebooks/2021-02-21-Downstream-BERT-QA.ipynb
layout: notebook
---

<!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-02-21-Downstream-BERT-QA.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Drilling-down-Multiple-Choice-downstream-task">Drilling down Multiple Choice downstream task<a class="anchor-link" href="#Drilling-down-Multiple-Choice-downstream-task"> </a></h1><p>{% include note.html content='I have learned how to use bibtex citations with fastpages! Therefore, all my next post are going to follow these kind of formatting whenever possible. If you are interested, check <a href="https://drscotthawley.github.io/devblog4/2020/07/01/Citations-Via-Bibtex.html">this</a> out.' %}
When I started studying Language Models, I remember when I've found the following image from Open AI transformer paper {% cite Radford2018ImprovingLU %} :</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/personal_blog/images/copied_from_nb/images/downstream-gpt.png" alt="" title="Example of fine-tuning tasks from GPT paper"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>However, the only difference is that the <strong>input data</strong> should be <em>slightly</em> different:</p>
<blockquote><p>For these tasks, we are given a context
document $z$, a question $q$, and a set of possible answers ${a_k}$. We concatenate the document context
and question with each possible answer, adding a delimiter token in between to get [$z$; $q$; $ \$ $; $a_k$]. Each of these sequences are <strong>processed independently with our model and then normalized via a softmax layer to produce an output distribution over possible answers</strong>.</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Therefore, these inputs could be optimized via <a href="https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html">Categorical Cross Entropy Loss</a>, where $C$ is the number of options available. For a specific question.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="From-GPT-to-BERT">From GPT to BERT<a class="anchor-link" href="#From-GPT-to-BERT"> </a></h1><p>As we will see with <a href="https://huggingface.co/transformers/">Hugging Face's transformer library</a>, when we considerer application from a fine tuning task, the approach of BERT can be derived directly from the tecnique presented by {% cite Radford2018ImprovingLU %}.
It is possible to check it from <a href="https://huggingface.co/transformers/model_doc/bert.html#transformers.BertForMultipleChoice">documentation</a></p>
<blockquote><p>Bert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a softmax) e.g. for RocStories/SWAG tasks.</p>
</blockquote>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p><div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertTokenizer</span><span class="p">,</span> <span class="n">BertForMultipleChoice</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertForMultipleChoice</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>
</p>
    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stderr output_text">
<pre>Downloading: 100%|██████████| 232k/232k [00:01&lt;00:00, 171kB/s]
Downloading: 100%|██████████| 433/433 [00:00&lt;00:00, 122kB/s]
Downloading: 100%|██████████| 440M/440M [02:06&lt;00:00, 3.48MB/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultipleChoice: [&#39;cls.predictions.bias&#39;, &#39;cls.predictions.transform.dense.weight&#39;, &#39;cls.predictions.transform.dense.bias&#39;, &#39;cls.predictions.decoder.weight&#39;, &#39;cls.seq_relationship.weight&#39;, &#39;cls.seq_relationship.bias&#39;, &#39;cls.predictions.transform.LayerNorm.weight&#39;, &#39;cls.predictions.transform.LayerNorm.bias&#39;]
- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: [&#39;classifier.weight&#39;, &#39;classifier.bias&#39;]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">question</span> <span class="o">=</span> <span class="s2">&quot;George wants to warm his hands quickly by rubbing them. Which skin surface will produce the most heat?&quot;</span>
<span class="n">option_a</span> <span class="o">=</span> <span class="s2">&quot;dry palms&quot;</span>
<span class="n">option_b</span> <span class="o">=</span> <span class="s2">&quot;wet palms&quot;</span>
<span class="n">option_c</span> <span class="o">=</span> <span class="s2">&quot;palms covered with oil&quot;</span>
<span class="n">option_d</span> <span class="o">=</span> <span class="s2">&quot;palms covered with lotion&quot;</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>In this case, option A is the correct one. Furthermore, the batch size here would be 1</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> 
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Notice that the question is the same for each option</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">encoding</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
            <span class="p">[</span><span class="n">question</span><span class="p">,</span> <span class="n">question</span><span class="p">,</span> <span class="n">question</span><span class="p">,</span> <span class="n">question</span><span class="p">],</span>
            <span class="p">[</span><span class="n">option_a</span><span class="p">,</span> <span class="n">option_b</span><span class="p">,</span> <span class="n">option_c</span><span class="p">,</span> <span class="n">option_d</span><span class="p">],</span>
            <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="kc">True</span>
           <span class="p">)</span>

<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">encoding</span><span class="o">.</span><span class="n">items</span><span class="p">()},</span> <span class="n">labels</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include important.html content='Notice that if we have a dataset such as SquaD where each question comes with a context, we could append this context to either the question text or the option text and we would then have the tuple cited by <a href="https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf">Open AI transformer paper</a>' %}
The output is a linear layer which would still be trained through a Cross Entropy loss. Then, as stated by the documentation, we still need to apply softmax to the logits</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">loss</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">loss</span>
<span class="n">logits</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">logits</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Linear Logits output:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[-0.3457, -0.3295, -0.3271, -0.3342]], grad_fn=&lt;ViewBackward&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Logits after the softmax function. Since this model did not learn anything, the result below is expected:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([[0.2471, 0.2511, 0.2518, 0.2500]], grad_fn=&lt;SoftmaxBackward&gt;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Conclusion">Conclusion<a class="anchor-link" href="#Conclusion"> </a></h1><p>Congratulations! Adding up with the <a href="https://abarbosa94.github.io/personal_blog/masters/nlp/knowledge-distill/2020/09/19/Distilling-BERT.html">first part</a>, you have learned the end-to-end BERT Flow :)</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="References">References<a class="anchor-link" href="#References"> </a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% bibliography --cited %}</p>

</div>
</div>
</div>
</div>
 

