---
keywords: fastai
description: "Inspired by Jay Alamar Ilustrated BERT blog post, I have decided to explain it using my own words :)"
title: "BERT Introduction"
toc: true
branch: master
author: Andre Barbosa
badges: true
hide_binder_badge: true
hide_colab_badge: true
comments: true
categories: [masters, nlp]
hide: false
search_exclude: false
nb_path: _notebooks/2020-09-19-BERT-Introduction.ipynb
layout: notebook
---

<!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-09-19-BERT-Introduction.ipynb
-->

<div class="container" id="notebook-container">
        
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="A-recap-from-Word-Embeddings">A recap from Word Embeddings<a class="anchor-link" href="#A-recap-from-Word-Embeddings"> </a></h1><p>I remember some day of 2016 while I was starting my carrer as a Data Scientist when I've stumped into <a href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/">Chirs McCormick  blog about Word2Vec</a>. Honestly, I think that <a href="https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf">Tomas Mikolov paper</a> was one of the most elegant and simple idea that I have ever found so far {% fn 1 %} :)</p>
<p>{{ 'Fun Fact: Whereas nowadays <a href="https://www.linkedin.com/in/tomas-mikolov-59831188/?originalSubdomain=cz">Miklov LinkedIn profile</a> points out that he has worked for Microsoft, Google and Facebook; another of W2V authors, <a href="http://www.cs.toronto.edu/~ilya/">Ilya Sutskever</a> worked with some of the prestigious researchers in the recent AI area, such as <a href="https://www.cs.toronto.edu/~hinton/">Geoffrey Hinton</a> and <a href="https://www.andrewng.org/">Andrew Ng</a>. Moreover, he is one of the founders of <a href="https://openai.com/">Open AI</a>! ' | fndetail: 1 }}</p>
<h2 id="What-are-Word-Embeddings">What are Word Embeddings<a class="anchor-link" href="#What-are-Word-Embeddings"> </a></h2><p>According to <a href="https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html">Pytorch documentation</a> an <strong>Emnedding</strong> can be defined as the following:</p>
<blockquote><p>A simple lookup table that stores embeddings of a fixed <em>dictionary</em> and <em>size</em>.</p>
</blockquote>
<p>Then, we can interpret embeddings as a simple way to convert <em>integers</em> into <em>vectors</em> of a given size. Then, for <strong>word embeddings</strong>, we can interpret simply as words that are encoded as integers and then _these integers serves as inputs for a vector space.</p>
<p>A have written some code with <a href="https://github.com/3b1b/manim">manim</a> to illustrate this process:</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><img src="/personal_blog/images/copied_from_nb/images/EmbeddingText.gif" alt="" title="In this example, the embedding dimension is NxM, where N is the vocab size (8) and M is 4. The row representing word &#39;the&#39; was duplicated for illustration purposes"></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can then interpret each dimension as a single neuron of a hidden layer and then <strong>these embedding numbers can be modified</strong> from a learning algorithm through a neural network. This is the main motivation behind Word Embeddings algorithms such as <a href="https://patents.google.com/patent/US9037464B1/en">Word2Vec</a>; <a href="https://nlp.stanford.edu/projects/glove/">GloVe</a> and <a href="https://fasttext.cc/">fastText</a> {% fn 2 %}</p>
<p>Nowadays, there are some libraries that provides already trained vectors based on a fixed and previously trained vocabulary. For instance, considerer the following <a href="https://spacy.io/models">Spacy</a> code:</p>
<p>{{ 'I am not going to cover word embeddings thourgh this blog post. If you are not familiarized with them, I highly recommend <a href="http://jalammar.github.io/illustrated-word2vec/">this</a>; <a href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/">this</a> and <a href="https://www.youtube.com/watch?v=ASn7ExxLZws">this</a> as potential resources :)' | fndetail: 2 }}</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p><div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">spacy</span>
<span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s2">&quot;en_core_web_md&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Coniderer the sentence &#39;The quick brown fox jumps over the lazy dog!!&#39;&quot;</span><span class="p">)</span>
<span class="n">text</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s2">&quot;The quick brown fox jumps over the lazy dog!!&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">text</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;&#39;</span><span class="si">{</span><span class="n">word</span><span class="o">.</span><span class="n">text</span><span class="si">}</span><span class="s2">&#39; vector representation has size of </span><span class="si">{</span><span class="n">word</span><span class="o">.</span><span class="n">vector</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">. Its first five elements are: </span><span class="si">{</span><span class="n">word</span><span class="o">.</span><span class="n">vector</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>
</p>
    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>Coniderer the sentence &#39;The quick brown fox jumps over the lazy dog!!&#39;
&#39;The&#39; vector representation has size of 300. Its first five elements are: [ 0.27 -0.06 -0.19  0.02 -0.02]
&#39;quick&#39; vector representation has size of 300. Its first five elements are: [-0.45  0.19 -0.25  0.47  0.16]
&#39;brown&#39; vector representation has size of 300. Its first five elements are: [-0.37 -0.08  0.11  0.19  0.03]
&#39;fox&#39; vector representation has size of 300. Its first five elements are: [-0.35 -0.08  0.18 -0.09 -0.45]
&#39;jumps&#39; vector representation has size of 300. Its first five elements are: [-0.33  0.22 -0.35 -0.26  0.41]
&#39;over&#39; vector representation has size of 300. Its first five elements are: [-0.3   0.01  0.04  0.1   0.12]
&#39;the&#39; vector representation has size of 300. Its first five elements are: [ 0.27 -0.06 -0.19  0.02 -0.02]
&#39;lazy&#39; vector representation has size of 300. Its first five elements are: [-0.35 -0.3  -0.18 -0.32 -0.39]
&#39;dog&#39; vector representation has size of 300. Its first five elements are: [-0.4   0.37  0.02 -0.34  0.05]
&#39;!&#39; vector representation has size of 300. Its first five elements are: [-0.27  0.34  0.22 -0.3  -0.06]
&#39;!&#39; vector representation has size of 300. Its first five elements are: [-0.27  0.34  0.22 -0.3  -0.06]
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Contains word representations that were trained on <a href="https://github.com/explosion/spacy-models/releases//tag/en_core_web_md-2.3.1">Common Crawl data using GloVe algorithm</a>. Different thant the example that I used at the beggining, the word '!' was encoded as well. Other interesting fact is that since GloVe probably passed thourgh a preprocessing step, both '<em>The</em>' and '<em>the</em>' got the same representation.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p><div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;First 5 values of word &#39;The&#39; vector: </span><span class="si">{</span><span class="n">nlp</span><span class="p">(</span><span class="s1">&#39;The&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">vector</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;First 5 values of word &#39;the&#39; vector: </span><span class="si">{</span><span class="n">nlp</span><span class="p">(</span><span class="s1">&#39;the&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">vector</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>
</p>
    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>First 5 values of word &#39;The&#39; vector: [ 0.27 -0.06 -0.19  0.02 -0.02]
First 5 values of word &#39;the&#39; vector: [ 0.27 -0.06 -0.19  0.02 -0.02]
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can combine different words to form the embedding of a phrase. According to <a href="https://spacy.io/usage/vectors-similarity#_title">spacy documentation</a>:</p>
<blockquote><p>Models that come with built-in word vectors make them available as the Token.vector attribute. Doc.vector and Span.vector will default to an average of their token vectors.</p>
</blockquote>
<p>Then, the phrase the we are using as example has the following single representation:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>First 5 values of &#39;The quick brown fox jumps over the lazy dog!!&#39;: [-0.23  0.08 -0.03 -0.07 -0.02]
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h2 id="Limitations-of-Word-Embeddings">Limitations of Word Embeddings<a class="anchor-link" href="#Limitations-of-Word-Embeddings"> </a></h2><p>Despite the fact that Word Embeddings brings a lot of benefits in the realm of computational linguistics, it has some limitations. There is a linguistic phenomena called <em>polyseme</em> where according to <a href="https://en.wikipedia.org/wiki/Polysemy#:~:text=English%20has%20many%20polysemous%20words,a%20subset%20of%20the%20other.">wikipedia</a>:</p>
<blockquote><p>A polyseme is a word or phrase with different, but related senses.(...) English has many polysemous words. For example, the verb "to get" can mean "procure" (I'll get the drinks), "become" (she got scared), "understand" (I get it) etc.</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>So considering the example above, despite the fact that the verb has <strong>different meaning</strong> depending on the contexts, <strong>it's word representation would always be the same</strong></p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>First 5 values of verb &#39;to get&#39; vector: [ 0.03  0.12 -0.32  0.13  0.12]
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Then, if we pick two phrases: <code>She got scared</code> and <code>She understand it</code>, we will get the following vectors</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">text1</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s2">&quot;She will get scared&quot;</span><span class="p">)</span>
<span class="n">text2</span> <span class="o">=</span> <span class="n">nlp</span><span class="p">(</span><span class="s2">&quot;She will get the drinks&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;First 5 values of verb &#39;</span><span class="si">{</span><span class="n">text1</span><span class="si">}</span><span class="s2">&#39; vector: </span><span class="si">{</span><span class="n">text1</span><span class="o">.</span><span class="n">vector</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;First 5 values of verb &#39;</span><span class="si">{</span><span class="n">text2</span><span class="si">}</span><span class="s2">&#39; vector: </span><span class="si">{</span><span class="n">text2</span><span class="o">.</span><span class="n">vector</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span><span class="o">.</span><span class="n">round</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>First 5 values of verb &#39;She will get scared&#39; vector: [-0.08  0.16 -0.22 -0.03  0.02]
First 5 values of verb &#39;She will get the drinks&#39; vector: [ 0.01  0.13 -0.04 -0.08  0.03]
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Then, if we take the cosine simlarity by taking the average of the word vectors:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p><div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">text1</span><span class="o">.</span><span class="n">similarity</span><span class="p">(</span><span class="n">text2</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>
</p>
    </details>
<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>0.88455245783805</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This indicates that both vectors would be a lot similar. However, the reason for that is the usage of <em>similar</em> words, even considering that they were applied in different contexts! So there is the objective that BERT tries to solve.{% fn 3 %}</p>
<p>{{ 'There are some BERT percursors such as <a href="https://allennlp.org/elmo">ELMo</a>; <a href="https://arxiv.org/abs/1801.06146">ULMFit</a> and <a href="https://openai.com/blog/language-unsupervised/">Open AI Transformer</a> that I am not going to cover here. Please reach out to <a href="http://jalammar.github.io/illustrated-bert/">Illustrated BERT blog</a> to know more' | fndetail: 3 }}</p>

</div>
</div>
</div>
</div>
 

