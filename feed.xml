<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://abarbosa94.github.io/personal_blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://abarbosa94.github.io/personal_blog/" rel="alternate" type="text/html" /><updated>2021-02-23T15:28:43-06:00</updated><id>https://abarbosa94.github.io/personal_blog/feed.xml</id><title type="html">Andre’s Technical Blog</title><subtitle>This website is powered by fastpages</subtitle><entry><title type="html">BERT applied to Multiple Choice</title><link href="https://abarbosa94.github.io/personal_blog/masters/nlp/knowledge-distill/2021/02/21/Downstream-BERT-QA.html" rel="alternate" type="text/html" title="BERT applied to Multiple Choice" /><published>2021-02-21T00:00:00-06:00</published><updated>2021-02-21T00:00:00-06:00</updated><id>https://abarbosa94.github.io/personal_blog/masters/nlp/knowledge-distill/2021/02/21/Downstream-BERT-QA</id><content type="html" xml:base="https://abarbosa94.github.io/personal_blog/masters/nlp/knowledge-distill/2021/02/21/Downstream-BERT-QA.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2021-02-21-Downstream-BERT-QA.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Drilling-down-Multiple-Choice-downstream-task&quot;&gt;Drilling down Multiple Choice downstream task&lt;a class=&quot;anchor-link&quot; href=&quot;#Drilling-down-Multiple-Choice-downstream-task&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;I have learned how to use bibtex citations with fastpages! Therefore, all my next post are going to follow these kind of formatting whenever possible. If you are interested, check &lt;a href=&quot;https://drscotthawley.github.io/devblog4/2020/07/01/Citations-Via-Bibtex.html&quot;&gt;this&lt;/a&gt; out.
&lt;/div&gt;
When I started studying Language Models, I remember when I've found the following image from Open AI transformer paper &lt;a class=&quot;citation&quot; href=&quot;#Radford2018ImprovingLU&quot;&gt;(Radford &amp;amp; Narasimhan, 2018)&lt;/a&gt; :&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/personal_blog/images/copied_from_nb/images/downstream-gpt.png&quot; alt=&quot;&quot; title=&quot;Example of fine-tuning tasks from GPT paper&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;However, the only difference is that the &lt;strong&gt;input data&lt;/strong&gt; should be &lt;em&gt;slightly&lt;/em&gt; different:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;For these tasks, we are given a context
document $z$, a question $q$, and a set of possible answers ${a_k}$. We concatenate the document context
and question with each possible answer, adding a delimiter token in between to get [$z$; $q$; $ \$ $; $a_k$]. Each of these sequences are &lt;strong&gt;processed independently with our model and then normalized via a softmax layer to produce an output distribution over possible answers&lt;/strong&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Therefore, these inputs could be optimized via &lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html&quot;&gt;Categorical Cross Entropy Loss&lt;/a&gt;, where $C$ is the number of options available. For a specific question.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;From-GPT-to-BERT&quot;&gt;From GPT to BERT&lt;a class=&quot;anchor-link&quot; href=&quot;#From-GPT-to-BERT&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;As we will see with &lt;a href=&quot;https://huggingface.co/transformers/&quot;&gt;Hugging Face's transformer library&lt;/a&gt;, when we considerer application from a fine tuning task, the approach of BERT can be derived directly from the tecnique presented by &lt;a class=&quot;citation&quot; href=&quot;#Radford2018ImprovingLU&quot;&gt;(Radford &amp;amp; Narasimhan, 2018)&lt;/a&gt;.
It is possible to check it from &lt;a href=&quot;https://huggingface.co/transformers/model_doc/bert.html#transformers.BertForMultipleChoice&quot;&gt;documentation&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Bert Model with a multiple choice classification head on top (a linear layer on top of the pooled output and a softmax) e.g. for RocStories/SWAG tasks.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;details class=&quot;description&quot;&gt;
      &lt;summary class=&quot;btn btn-sm&quot; data-open=&quot;Hide Code&quot; data-close=&quot;Show Code&quot;&gt;&lt;/summary&gt;
        &lt;p&gt;&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;transformers&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertTokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertForMultipleChoice&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertTokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;bert-base-uncased&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertForMultipleChoice&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;bert-base-uncased&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
    &lt;/details&gt;
&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stderr output_text&quot;&gt;
&lt;pre&gt;Downloading: 100%|██████████| 232k/232k [00:01&amp;lt;00:00, 171kB/s]
Downloading: 100%|██████████| 433/433 [00:00&amp;lt;00:00, 122kB/s]
Downloading: 100%|██████████| 440M/440M [02:06&amp;lt;00:00, 3.48MB/s]
Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMultipleChoice: [&amp;#39;cls.predictions.bias&amp;#39;, &amp;#39;cls.predictions.transform.dense.weight&amp;#39;, &amp;#39;cls.predictions.transform.dense.bias&amp;#39;, &amp;#39;cls.predictions.decoder.weight&amp;#39;, &amp;#39;cls.seq_relationship.weight&amp;#39;, &amp;#39;cls.seq_relationship.bias&amp;#39;, &amp;#39;cls.predictions.transform.LayerNorm.weight&amp;#39;, &amp;#39;cls.predictions.transform.LayerNorm.bias&amp;#39;]
- This IS expected if you are initializing BertForMultipleChoice from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMultipleChoice from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of BertForMultipleChoice were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: [&amp;#39;classifier.weight&amp;#39;, &amp;#39;classifier.bias&amp;#39;]
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;question&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;George wants to warm his hands quickly by rubbing them. Which skin surface will produce the most heat?&amp;quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;option_a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;dry palms&amp;quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;option_b&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;wet palms&amp;quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;option_c&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;palms covered with oil&amp;quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;option_d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;palms covered with lotion&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;In this case, option A is the correct one. Furthermore, the batch size here would be 1&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unsqueeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Notice that the question is the same for each option&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encoding&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;question&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;question&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;question&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;question&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;option_a&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;option_b&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;option_c&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;option_d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;return_tensors&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&amp;#39;pt&amp;#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;padding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;
           &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;v&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unsqueeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;k&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;v&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;encoding&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;items&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()},&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;div class=&quot;flash flash-warn&quot;&gt;
    &lt;svg class=&quot;octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M10.561 1.5a.016.016 0 00-.01.004L3.286 8.571A.25.25 0 003.462 9H6.75a.75.75 0 01.694 1.034l-1.713 4.188 6.982-6.793A.25.25 0 0012.538 7H9.25a.75.75 0 01-.683-1.06l2.008-4.418.003-.006a.02.02 0 00-.004-.009.02.02 0 00-.006-.006L10.56 1.5zM9.504.43a1.516 1.516 0 012.437 1.713L10.415 5.5h2.123c1.57 0 2.346 1.909 1.22 3.004l-7.34 7.142a1.25 1.25 0 01-.871.354h-.302a1.25 1.25 0 01-1.157-1.723L5.633 10.5H3.462c-1.57 0-2.346-1.909-1.22-3.004L9.503.429z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Important: &lt;/strong&gt;Notice that if we have a dataset such as SquaD where each question comes with a context, we could append this context to either the question text or the option text and we would then have the tuple cited by &lt;a href=&quot;https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf&quot;&gt;Open AI transformer paper&lt;/a&gt;
&lt;/div&gt;
The output is a linear layer which would still be trained through a Cross Entropy loss. Then, as stated by the documentation, we still need to apply softmax to the logits&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loss&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;logits&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Linear Logits output:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;tensor([[-0.3457, -0.3295, -0.3271, -0.3342]], grad_fn=&amp;lt;ViewBackward&amp;gt;)&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Logits after the softmax function. Since this model did not learn anything, the result below is expected:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;tensor([[0.2471, 0.2511, 0.2518, 0.2500]], grad_fn=&amp;lt;SoftmaxBackward&amp;gt;)&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Conclusion&quot;&gt;Conclusion&lt;a class=&quot;anchor-link&quot; href=&quot;#Conclusion&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Congratulations! Adding up with the &lt;a href=&quot;https://abarbosa94.github.io/personal_blog/masters/nlp/knowledge-distill/2020/09/19/Distilling-BERT.html&quot;&gt;first part&lt;/a&gt;, you have learned the end-to-end BERT Flow :)&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;References&quot;&gt;References&lt;a class=&quot;anchor-link&quot; href=&quot;#References&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;Radford2018ImprovingLU&quot;&gt;Radford, A., &amp;amp; Narasimhan, K. (2018). &lt;i&gt;Improving Language Understanding by Generative Pre-Training&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Andre Barbosa</name></author><category term="masters" /><category term="nlp" /><category term="knowledge-distill" /><summary type="html"></summary></entry><entry><title type="html">Distilling BERT Pre Training</title><link href="https://abarbosa94.github.io/personal_blog/masters/nlp/knowledge-distill/2020/09/19/Distilling-BERT.html" rel="alternate" type="text/html" title="Distilling BERT Pre Training" /><published>2020-09-19T00:00:00-05:00</published><updated>2020-09-19T00:00:00-05:00</updated><id>https://abarbosa94.github.io/personal_blog/masters/nlp/knowledge-distill/2020/09/19/Distilling-BERT</id><content type="html" xml:base="https://abarbosa94.github.io/personal_blog/masters/nlp/knowledge-distill/2020/09/19/Distilling-BERT.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-09-19-Distilling-BERT.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;For a &lt;strong&gt;portuguese&lt;/strong&gt; version of this post, please check &lt;a href=&quot;https://abarbosa94.github.io/personal_blog/masters/nlp/knowledge-distill/2020/09/19/Distilling-BERT-pt.html&quot;&gt;this&lt;/a&gt;
&lt;/div&gt;&lt;/p&gt;
&lt;h1 id=&quot;A-quick-review&quot;&gt;A quick review&lt;a class=&quot;anchor-link&quot; href=&quot;#A-quick-review&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;I remember someday of 2016 while I was starting my career as a Data Scientist when I've stumped into &lt;a href=&quot;http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/&quot;&gt;Chirs McCormick blog about Word2Vec&lt;/a&gt;. Honestly, I think that &lt;a href=&quot;https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf&quot;&gt;Tomas Mikolov paper&lt;/a&gt; was one of the most elegant and simple idea that I have ever found so far &lt;sup id=&quot;fnref-1&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-1&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; :)&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-1&quot;&gt;1. Fun Fact: Whereas nowadays &lt;a href=&quot;https://www.linkedin.com/in/tomas-mikolov-59831188/?originalSubdomain=cz&quot;&gt;Miklov LinkedIn profile&lt;/a&gt; points out that he has worked for Microsoft, Google and Facebook; another of W2V authors, &lt;a href=&quot;http://www.cs.toronto.edu/~ilya/&quot;&gt;Ilya Sutskever&lt;/a&gt; worked with some of the prestigious researchers in the recent AI area, such as &lt;a href=&quot;https://www.cs.toronto.edu/~hinton/&quot;&gt;Geoffrey Hinton&lt;/a&gt; and &lt;a href=&quot;https://www.andrewng.org/&quot;&gt;Andrew Ng&lt;/a&gt;. Moreover, he is one of the founders of &lt;a href=&quot;https://openai.com/&quot;&gt;Open AI&lt;/a&gt;! &lt;a href=&quot;#fnref-1&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h2 id=&quot;What-are-Word-Embeddings&quot;&gt;What are Word Embeddings&lt;a class=&quot;anchor-link&quot; href=&quot;#What-are-Word-Embeddings&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;According to &lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html&quot;&gt;Pytorch documentation&lt;/a&gt; an &lt;strong&gt;Embedding&lt;/strong&gt; can be defined as the following:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;A simple lookup table (...) of a fixed &lt;em&gt;dictionary&lt;/em&gt; and &lt;em&gt;size&lt;/em&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Then, we can interpret embeddings as a simple way to convert &lt;em&gt;integers&lt;/em&gt; into &lt;em&gt;vectors&lt;/em&gt; of a given size. Then, for &lt;strong&gt;word embeddings&lt;/strong&gt;, we can interpret simply as words that are encoded as integers, and then &lt;em&gt;these&lt;/em&gt; integers serve as inputs for a vector space.'&lt;/p&gt;
&lt;p&gt;A have written some code with &lt;a href=&quot;https://github.com/3b1b/manim&quot;&gt;manim&lt;/a&gt; to illustrate this process:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/personal_blog/images/copied_from_nb/images/media/videos/scene/720p30/EmbeddingExample.gif&quot; alt=&quot;&quot; title=&quot;In this example, the embedding dimension is NxM, where N is the vocab size (8) and M is 4.&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We can then interpret each dimension as a single neuron of a hidden layer, and then &lt;strong&gt;these embedding numbers can be modified&lt;/strong&gt; from a learning algorithm through a neural network. This is the main motivation behind Word Embeddings algorithms such as &lt;a href=&quot;https://patents.google.com/patent/US9037464B1/en&quot;&gt;Word2Vec&lt;/a&gt; and &lt;a href=&quot;https://fasttext.cc/&quot;&gt;fastText&lt;/a&gt; &lt;sup id=&quot;fnref-2&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-2&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Nowadays, there are some libraries that provide already trained vectors based on a fixed and previously trained vocabulary. For instance, considerer the following &lt;a href=&quot;https://spacy.io/models&quot;&gt;Spacy&lt;/a&gt; code:&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-2&quot;&gt;2. I am not going to cover word embeddings through this blog post. If you are not familiarized with them, I highly recommend &lt;a href=&quot;http://jalammar.github.io/illustrated-word2vec/&quot;&gt;this&lt;/a&gt;; &lt;a href=&quot;http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/&quot;&gt;this&lt;/a&gt; and &lt;a href=&quot;https://www.youtube.com/watch?v=ASn7ExxLZws&quot;&gt;this&lt;/a&gt; as potential resources :)&lt;a href=&quot;#fnref-2&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;details class=&quot;description&quot;&gt;
      &lt;summary class=&quot;btn btn-sm&quot; data-open=&quot;Hide Code&quot; data-close=&quot;Show Code&quot;&gt;&lt;/summary&gt;
        &lt;p&gt;&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;spacy&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;nlp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spacy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;en_core_web_md&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Coniderer the sentence &amp;#39;The quick brown fox jumps over the lazy dog!!&amp;#39;&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nlp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;The quick brown fox jumps over the lazy dog!!&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&amp;#39;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;#39; vector representation has size of &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;. Its first five elements are: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
    &lt;/details&gt;
&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Coniderer the sentence &amp;#39;The quick brown fox jumps over the lazy dog!!&amp;#39;
&amp;#39;The&amp;#39; vector representation has size of 300. Its first five elements are: [ 0.27 -0.06 -0.19  0.02 -0.02]
&amp;#39;quick&amp;#39; vector representation has size of 300. Its first five elements are: [-0.45  0.19 -0.25  0.47  0.16]
&amp;#39;brown&amp;#39; vector representation has size of 300. Its first five elements are: [-0.37 -0.08  0.11  0.19  0.03]
&amp;#39;fox&amp;#39; vector representation has size of 300. Its first five elements are: [-0.35 -0.08  0.18 -0.09 -0.45]
&amp;#39;jumps&amp;#39; vector representation has size of 300. Its first five elements are: [-0.33  0.22 -0.35 -0.26  0.41]
&amp;#39;over&amp;#39; vector representation has size of 300. Its first five elements are: [-0.3   0.01  0.04  0.1   0.12]
&amp;#39;the&amp;#39; vector representation has size of 300. Its first five elements are: [ 0.27 -0.06 -0.19  0.02 -0.02]
&amp;#39;lazy&amp;#39; vector representation has size of 300. Its first five elements are: [-0.35 -0.3  -0.18 -0.32 -0.39]
&amp;#39;dog&amp;#39; vector representation has size of 300. Its first five elements are: [-0.4   0.37  0.02 -0.34  0.05]
&amp;#39;!&amp;#39; vector representation has size of 300. Its first five elements are: [-0.27  0.34  0.22 -0.3  -0.06]
&amp;#39;!&amp;#39; vector representation has size of 300. Its first five elements are: [-0.27  0.34  0.22 -0.3  -0.06]
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Contains word representations that were trained on &lt;a href=&quot;https://github.com/explosion/spacy-models/releases//tag/en_core_web_md-2.3.1&quot;&gt;Common Crawl data using GloVe algorithm&lt;/a&gt;. Unlike the example that I used initially, the word '!' was encoded as well.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;We can combine different words to form the embedding of a phrase. According to &lt;a href=&quot;https://spacy.io/usage/vectors-similarity#_title&quot;&gt;spacy documentation&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Models that come with built-in word vectors make them available as the Token.vector attribute. Doc.vector and Span.vector will default to an average of their token vectors.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Then, the phrase the we are using as example has the following single representation:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;First 5 values of &amp;#39;The quick brown fox jumps over the lazy dog!!&amp;#39;: [-0.23  0.08 -0.03 -0.07 -0.02]
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Limitations-of-Word-Embeddings&quot;&gt;Limitations of Word Embeddings&lt;a class=&quot;anchor-link&quot; href=&quot;#Limitations-of-Word-Embeddings&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Even though Word Embeddings brings many benefits in the realm of computational linguistics, they have some limitations. There is a linguistic phenomenon called &lt;em&gt;polysemy&lt;/em&gt;. According to &lt;a href=&quot;https://en.wikipedia.org/wiki/Polysemy#:~:text=English%20has%20many%20polysemous%20words,a%20subset%20of%20the%20other.&quot;&gt;wikipedia&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;A polyseme is a word or phrase with different, but related senses.(...) English has many polysemous words. For example, the verb &quot;to get&quot; can mean &quot;procure&quot; (I'll get the drinks), &quot;become&quot; (she got scared), &quot;understand&quot; (I get it) etc.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;So considering the example above, despite the fact that the verb has &lt;strong&gt;different meaning&lt;/strong&gt; depending on the contexts, &lt;strong&gt;it's word representation would always be the same&lt;/strong&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;First 5 values of verb &amp;#39;to get&amp;#39; vector: [ 0.03  0.12 -0.32  0.13  0.12]
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Then, if we pick two phrases: &lt;code&gt;She got scared&lt;/code&gt; and &lt;code&gt;She understand it&lt;/code&gt;, we will get the following vectors&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nlp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;He will get scared&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;text2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nlp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;She will get the drinks&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;First 5 values of sentence &amp;#39;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text1&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;#39; vector: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;First 5 values of sentence &amp;#39;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text2&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;#39; vector: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;First 5 values of verb &amp;#39;He will get scared&amp;#39; vector: [-0.12  0.19 -0.21 -0.14  0.09]
First 5 values of verb &amp;#39;She will get the drinks&amp;#39; vector: [ 0.01  0.13 -0.04 -0.08  0.03]
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Then, if we take the cosine similarity by taking the average of the word vectors:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;details class=&quot;description&quot;&gt;
      &lt;summary class=&quot;btn btn-sm&quot; data-open=&quot;Hide Code&quot; data-close=&quot;Show Code&quot;&gt;&lt;/summary&gt;
        &lt;p&gt;&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.metrics.pairwise&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cosine_similarity&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Similarity between:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt; &amp;#39;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text1&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;#39; and &amp;#39;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text2&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;#39;: &amp;quot;&lt;/span&gt;
    &lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cosine_similarity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
    &lt;/details&gt;
&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Simlarity between:
 &amp;#39;He will get scared&amp;#39; and &amp;#39;She will get the drinks&amp;#39;: 0.8653444051742554
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;This indicates that both vectors would be a lot similar. However, the reason for that is the usage of &lt;em&gt;similar&lt;/em&gt; words, even considering that they were applied in different contexts! So there is the objective that BERT tries to solve.&lt;sup id=&quot;fnref-3&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-3&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-3&quot;&gt;3. There are some BERT percursors such as &lt;a href=&quot;https://allennlp.org/elmo&quot;&gt;ELMo&lt;/a&gt;; &lt;a href=&quot;https://arxiv.org/abs/1801.06146&quot;&gt;ULMFit&lt;/a&gt; and &lt;a href=&quot;https://openai.com/blog/language-unsupervised/&quot;&gt;Open AI Transformer&lt;/a&gt; that I am not going to cover here. Please reach out to &lt;a href=&quot;http://jalammar.github.io/illustrated-bert/&quot;&gt;Illustrated BERT blog&lt;/a&gt; to know more&lt;a href=&quot;#fnref-3&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;BERT-Model&quot;&gt;BERT Model&lt;a class=&quot;anchor-link&quot; href=&quot;#BERT-Model&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Attention-is-all-you-need&quot;&gt;Attention is all you need&lt;a class=&quot;anchor-link&quot; href=&quot;#Attention-is-all-you-need&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;The &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;Attention is all you need&lt;/a&gt; paper have introduced the Transformer architeture for us :) In sense, it can be summarized as the picture below:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/personal_blog/images/copied_from_nb/images/transformer.png&quot; alt=&quot;&quot; title=&quot;The transformer- model architeture, taken from: https://arxiv.org/abs/1706.03762&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Strictly speaking, the motivation behind the paper is that &lt;em&gt;RNN&lt;/em&gt;-like architetures are memory-expensive. The purpose behind Transformer models is that it you can achieve similar results using more computer efficient resources by applying &lt;strong&gt;just attention mechanisms&lt;/strong&gt; (and exluding the CNN or RNN-like architetures) !&lt;sup id=&quot;fnref-4&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-4&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; Despite the fact that the Transformer model was proposed to deal with translation problems, it turns out that we can also use variations of it to achieve awesome results in different tasks. This is the &lt;strong&gt;motivation behind BERT&lt;/strong&gt;!&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-4&quot;&gt;4. &lt;a href=&quot;http://nlp.seas.harvard.edu/2018/04/03/attention.html&quot;&gt;The NLP group from Harvard&lt;/a&gt; has written a great blog post distilling the paper as well as implementing them in pytorch. If you have some interest in knowing details about the transformer architecture, I recommend looking at it! &lt;a href=&quot;#fnref-4&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h3 id=&quot;Attention?&quot;&gt;Attention?&lt;a class=&quot;anchor-link&quot; href=&quot;#Attention?&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;According to the &lt;a href=&quot;https://atcold.github.io/pytorch-Deep-Learning/en/week12/12-3/&quot;&gt;Transformer and Attention lecture from NYU foundations of Deep Learning Course&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Transformers are made up of attention modules, which are mappings between sets, rather than sequences, which means we do not impose an ordering to our inputs/outputs.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;When we analyze the transformer architeture, we can see that both &lt;em&gt;Multi-Head Attention&lt;/em&gt; and &lt;em&gt;Multi-Head Masked Attention&lt;/em&gt; box have 3 Arrow Heads. Each one represents one of the following:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Q&lt;/em&gt; that stands for &lt;strong&gt;query&lt;/strong&gt; vector with dimension $d_k$ &lt;/li&gt;
&lt;li&gt;&lt;em&gt;K&lt;/em&gt; that stands for &lt;strong&gt;key&lt;/strong&gt; vector that also has dimension $d_k$&lt;/li&gt;
&lt;li&gt;&lt;em&gt;V&lt;/em&gt; that stands for &lt;strong&gt;value&lt;/strong&gt; vector that also has dimension $d_v$&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;KV&lt;/strong&gt; pair can be understood as the encoded representation of the input whereas the &lt;strong&gt;Q&lt;/strong&gt; is the output of a previous layer.&lt;/p&gt;
&lt;h3 id=&quot;Key-Value-Store&quot;&gt;Key-Value Store&lt;a class=&quot;anchor-link&quot; href=&quot;#Key-Value-Store&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Again, from the &lt;a href=&quot;https://atcold.github.io/pytorch-Deep-Learning/en/week12/12-3/&quot;&gt;Deep Learning Foundations Course from NYU&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;A key-value store is a paradigm designed for storing (saving), retrieving (querying), and managing associative arrays (dictionaries/hash tables)&lt;/p&gt;
&lt;p&gt;For example, say we wanted to find a recipe to make lasagne. We have a recipe book and search for “lasagne” - this is the query. This query is checked against all possible keys in your dataset - in this case, this could be the titles of all the recipes in the book. We check how aligned the query is with each title to find the maximum matching score between the query and all the respective keys. If our output is the argmax function - we retrieve the single recipe with the highest score. Otherwise, if we use a soft argmax function, we would get a probability distribution and can retrieve in order from the most similar content to less and less relevant recipes matching the query.&lt;/p&gt;
&lt;p&gt;Basically, the query is the question. Given one query, we check this query against every key and retrieve all matching content.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;div class=&quot;flash flash-error&quot;&gt;
    &lt;svg class=&quot;octicon octicon-alert octicon octicon-alert&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8.22 1.754a.25.25 0 00-.44 0L1.698 13.132a.25.25 0 00.22.368h12.164a.25.25 0 00.22-.368L8.22 1.754zm-1.763-.707c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0114.082 15H1.918a1.75 1.75 0 01-1.543-2.575L6.457 1.047zM9 11a1 1 0 11-2 0 1 1 0 012 0zm-.25-5.25a.75.75 0 00-1.5 0v2.5a.75.75 0 001.5 0v-2.5z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Warning: &lt;/strong&gt;I have decided not to cover attention concepts in this post, giving just a higher-level introduction. As you might have noticed, NYU Deep Learning Foundations Course provides a really nice introduction about the topic that I recommend going through if you want to learn more :)
&lt;/div&gt;&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;Attention can be basically understood as measure of correlation of words between a set of sentences. For those interested to learn a little bit more, I &lt;em&gt;highly&lt;/em&gt; recommend &lt;a href=&quot;https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html&quot;&gt;this&lt;/a&gt; blog post
&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Positional-Encoding&quot;&gt;Positional Encoding&lt;a class=&quot;anchor-link&quot; href=&quot;#Positional-Encoding&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;This was taken from &lt;a href=&quot;http://nlp.seas.harvard.edu/2018/04/03/attention.html#prelims&quot;&gt;The annotated transformer blog&lt;/a&gt; where you can find a cool pytorch implementation. It turns out that actually this is a quote from Attention is all you need &lt;a href=&quot;https://arxiv.org/pdf/1706.03762.pdf&quot;&gt;paper&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence. To this end, we add “positional encodings” to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension 
$d_{model}$ as the embeddings, so that the two can be summed&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;/personal_blog/images/copied_from_nb/images/positional_encoding.png&quot; alt=&quot;&quot; title=&quot;One example of a positional encoding that generates sine wave based on length. Notice that each dimension generates a sine wave with different frequency. Source:http://nlp.seas.harvard.edu/2018/04/03/attention.html&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;The-BERT-model&quot;&gt;The BERT model&lt;a class=&quot;anchor-link&quot; href=&quot;#The-BERT-model&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;BERT model itself is an &lt;em&gt;encoder model&lt;/em&gt; only from the transformer model. Considering the models trained from the &lt;a href=&quot;https://arxiv.org/pdf/1810.04805.pdf&quot;&gt;paper&lt;/a&gt;, the &lt;strong&gt;base&lt;/strong&gt; model consists of 12 &lt;em&gt;encoder-stacked&lt;/em&gt; layers and the &lt;strong&gt;large&lt;/strong&gt; model consists of 24 &lt;em&gt;encoder-stacked&lt;/em&gt; layers.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;According to the &lt;a href=&quot;https://arxiv.org/pdf/1706.03762.pdf&quot;&gt;Attention is all you need paper&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;The encoder is composed of a stack of $N = 6$ identical layers. Each layer has &lt;strong&gt;two
sub-layers&lt;/strong&gt;. The first is a &lt;strong&gt;multi-head self-attention mechanism&lt;/strong&gt;, and the second is a simple, &lt;strong&gt;position wise fully connected feed-forward network&lt;/strong&gt;. We employ a &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;&gt;residual connection&lt;/a&gt; around &lt;strong&gt;each&lt;/strong&gt; of the two sub-layers, followed by &lt;a href=&quot;https://arxiv.org/abs/1607.06450&quot;&gt;layer normalization&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/personal_blog/images/copied_from_nb/images/sublayers.jpg&quot; alt=&quot;&quot; title=&quot;The encoder layer. Source: https://atcold.github.io/pytorch-Deep-Learning/en/week12/12-1/&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;The-Multi-Head-Attention&quot;&gt;The Multi-Head Attention&lt;a class=&quot;anchor-link&quot; href=&quot;#The-Multi-Head-Attention&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Basically, the multi head attention is a &lt;em&gt;type&lt;/em&gt; of an attention mechanism. It is a &lt;em&gt;concatenation&lt;/em&gt; of another type of attention, the &lt;em&gt;scaled dot&lt;/em&gt;. Both mechanisms works together as represented in the following image:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/personal_blog/images/copied_from_nb/images/attention_specific.png&quot; alt=&quot;&quot; title=&quot;(left) Scaled Dot-Product Attention followed by the Multi-Head Attention which consists of several attention layers running in parallel. Source: https://atcold.github.io/pytorch-Deep-Learning/en/week12/12-1/&quot; /&gt;
&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;Scaled Dot-Product Attention is calculated by $softmax(\frac{QK^T}{\sqrt{n}})V$, where &lt;em&gt;K&lt;/em&gt;, &lt;em&gt;V&lt;/em&gt; and &lt;em&gt;Q&lt;/em&gt; are the same as the ones described in a previous section whereas &lt;em&gt;n&lt;/em&gt; represents the number of elements in the set. 
&lt;/div&gt;
Here, &lt;em&gt;h&lt;/em&gt;, or the number o attention heads (or layers) is equal to $12$ in the case of $\text{BERT}_\text{base}$ and $16$ in the case of  $\text{BERT}_\text{large}$&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Residual-Conections&quot;&gt;Residual Conections&lt;a class=&quot;anchor-link&quot; href=&quot;#Residual-Conections&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Each sublayer of the encoder stack contains a residual connection (the left curved arrow) added to the sublayer output before layer normalization. The &lt;a href=&quot;https://arxiv.org/pdf/1512.03385.pdf&quot;&gt;idea of Residual Conections&lt;/a&gt; came from Computer Vision domain, and actually, it is a relatively simple technique that can be summarized by the following image:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/personal_blog/images/copied_from_nb/images/residual_connection.png&quot; alt=&quot;&quot; title=&quot;Residual Connection example. Source (https://arxiv.org/pdf/1512.03385.pdf)&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Considering the image above and the case of Encoder stack, each $\mathcal{F}(x)$ means either the &lt;em&gt;Multi-Head Attention&lt;/em&gt; or &lt;em&gt;Feed Forward&lt;/em&gt;. Therefore, quoting the paper:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;That is, the &lt;strong&gt;output of each sub-layer is LayerNorm(x + Sublayer(x))&lt;/strong&gt;, where Sublayer(x) is the function implemented by the sub-layer itself. To &lt;em&gt;facilitate these residual connections&lt;/em&gt;, all sub-layers in the model, as well as the embedding
layers, produce outputs of dimension $d_{model} = 512$ &lt;sup id=&quot;fnref-5&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-5&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-5&quot;&gt;5. In the case of &lt;a href=&quot;https://arxiv.org/pdf/1810.04805.pdf&quot;&gt;BERT model&lt;/a&gt;, please have in mind that $N$ is either $12$ (BERT&lt;sub&gt;base&lt;/sub&gt;) or $24$ ((BERT&lt;sub&gt;large&lt;/sub&gt;) and &lt;em&gt;d&lt;sub&gt;model&lt;/sub&gt;&lt;/em&gt; is 768 for BERT base and 1024 for BERT large&lt;a href=&quot;#fnref-5&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Then, what, in fact, is being encoded?&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Embedding-Representation&quot;&gt;Embedding Representation&lt;a class=&quot;anchor-link&quot; href=&quot;#Embedding-Representation&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;The authors would like to make BERT to perform well in different downstream tasks such as &lt;em&gt;binary and multi lablel classification&lt;/em&gt;; &lt;em&gt;language modeling&lt;/em&gt;; &lt;em&gt;question and answering&lt;/em&gt;; &lt;em&gt;named entity recognition&lt;/em&gt;; &lt;em&gt;etc&lt;/em&gt;. Therefore, they said the following:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;our input representation is able to unambiguously represent both a single sentence and a pair of sentences
(e.g., h Question, Answer) in one token sequence. Throughout this work, a “sentence” can be an arbitrary span of contiguous text, rather than an actual linguistic sentence. A “sequence” refers to the input token sequence to BERT, which may be a single sentence or two sentences packed together&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In order to perform and create the sentence embeddings, &lt;a href=&quot;https://arxiv.org/abs/1609.08144&quot;&gt;WordPiece tokenize is applied&lt;/a&gt;. Then, besides adding [CLS] token, pairs of sentence (e.g. sentence &lt;em&gt;A&lt;/em&gt; and &lt;em&gt;B&lt;/em&gt;) are concatenated into a single sentence, being separated with a special token [SEP] (e.g. &lt;em&gt;A&lt;/em&gt; [SEP] &lt;em&gt;B&lt;/em&gt;).
Then:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;For a given token, its input representation is constructed by summing the corresponding token, segment, and position embeddings.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/personal_blog/images/copied_from_nb/images/token_embeddings.png&quot; alt=&quot;&quot; title=&quot;BERT input representation. Source: https://arxiv.org/pdf/1810.04805.pdf&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;BERT-Pre-Training&quot;&gt;BERT Pre Training&lt;a class=&quot;anchor-link&quot; href=&quot;#BERT-Pre-Training&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;The first part of BERT is a pre Training procedure that involved &lt;strong&gt;two&lt;/strong&gt; objective functions&lt;/p&gt;
&lt;h2 id=&quot;Masked-Language-Model-(MLM)&quot;&gt;Masked Language Model (MLM)&lt;a class=&quot;anchor-link&quot; href=&quot;#Masked-Language-Model-(MLM)&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;As we are feeding the whole sentence into the model, it is possible to say that the model is bidirectional and hence as we are trying to predict the next word in a sentence, it would has access to it! Then, the idea behind this task is pretty simple. We can directly quote from the &lt;a href=&quot;https://arxiv.org/pdf/1810.04805.pdf&quot;&gt;paper&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Unfortunately, standard conditional language models can only be trained left-to-right or right-to-left, since bidirectional conditioning would allow each word to indirectly “see itself”, and the model could trivially
predict the target word in a multi-layered context.&lt;/p&gt;
&lt;p&gt;In order to train a deep bidirectional representation, we simply mask some percentage of the input
tokens at random, and then predict those masked tokens. We refer to this procedure as a “masked LM” (MLM), although it is often referred to as a &lt;em&gt;Cloze task&lt;/em&gt; in the &lt;a href=&quot;https://journals.sagepub.com/doi/abs/10.1177/107769905303000401&quot;&gt;literature&lt;/a&gt;. In thiscase, the final hidden vectors corresponding to the mask tokens are fed into an output softmax over the vocabulary, as in a standard LM.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In the case of BERT model, 15% of each sentence were masked during training.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/personal_blog/images/copied_from_nb/images/mlm.png&quot; alt=&quot;&quot; title=&quot;MLM task. Taken from here: http://jalammar.github.io/illustrated-bert/&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Next-Sentence-Prediction-(NSP)&quot;&gt;Next Sentence Prediction (NSP)&lt;a class=&quot;anchor-link&quot; href=&quot;#Next-Sentence-Prediction-(NSP)&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;In order to learn relationships between pair of sentence (e.g. Question and Ansering tasks) the authors needed a different approach than plain Language Modeling. Then:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;In order to train a model that understands sentence relationships, we pre-train for a binarized next sentence prediction task that can be trivially generated from any monolingual corpus. Specifically, when choosing the sentences A and B for each pretraining example, 50% of the time B is the actual next sentence that follows A (labeled as &lt;code&gt;IsNext&lt;/code&gt;), and 50% of the time it is a random sentence from the corpus (labeled as &lt;code&gt;NotNext&lt;/code&gt;).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Once defined, both objected functions are used in BERT Pre training learning :)
&lt;img src=&quot;/personal_blog/images/copied_from_nb/images/nsp.png&quot; alt=&quot;&quot; title=&quot;Next Sentence Preiction. Taken from here: http://jalammar.github.io/illustrated-bert/&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;The training loss is the sum of the mean masked LM (MLM) likelihood and the mean next sentence prediction (NSP) likelihood
&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;div class=&quot;flash flash-warn&quot;&gt;
    &lt;svg class=&quot;octicon octicon-zap octicon octicon-zap octicon octicon-zap&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M10.561 1.5a.016.016 0 00-.01.004L3.286 8.571A.25.25 0 003.462 9H6.75a.75.75 0 01.694 1.034l-1.713 4.188 6.982-6.793A.25.25 0 0012.538 7H9.25a.75.75 0 01-.683-1.06l2.008-4.418.003-.006a.02.02 0 00-.004-.009.02.02 0 00-.006-.006L10.56 1.5zM9.504.43a1.516 1.516 0 012.437 1.713L10.415 5.5h2.123c1.57 0 2.346 1.909 1.22 3.004l-7.34 7.142a1.25 1.25 0 01-.871.354h-.302a1.25 1.25 0 01-1.157-1.723L5.633 10.5H3.462c-1.57 0-2.346-1.909-1.22-3.004L9.503.429z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Important: &lt;/strong&gt;You may have noticed but this training procedure &lt;strong&gt;does not require labeling&lt;/strong&gt;. As we are using the raw text inputs to generate the &lt;em&gt;labels&lt;/em&gt; during training, e considerer this BERT Pre Training as a &lt;em&gt;self-surpervised&lt;/em&gt; model!
&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Putting-all-together&quot;&gt;Putting all together&lt;a class=&quot;anchor-link&quot; href=&quot;#Putting-all-together&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;As we are dealing with &lt;strong&gt;sentence&lt;/strong&gt; embeddings than &lt;strong&gt;word&lt;/strong&gt; embeddings we need a clever way to, well, encode these sentences. Let's see how BERT do it:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We first take a text as input&lt;/li&gt;
&lt;li&gt;We apply WordPiece Tokenizer&lt;/li&gt;
&lt;li&gt;We fed the input into the Encoder stack&lt;/li&gt;
&lt;li&gt;We train the network (Pre-Training step)&lt;/li&gt;
&lt;li&gt;For those familiar with &lt;em&gt;CNN&lt;/em&gt; we can say that [CLS] embedding works as a &quot;pooled&quot; representation (&lt;a href=&quot;https://arxiv.org/pdf/2002.08909.pdf&quot;&gt;ref&lt;/a&gt;) of the sentence and then can be used as a &lt;strong&gt;contextual embedding feature&lt;/strong&gt;. Hence, it can be fed into a Neural Net to solve classification tasks!&lt;/li&gt;
&lt;li&gt;Depending on the downstreaming task (&lt;em&gt;Fine tuning task&lt;/em&gt;) other token embeddings can be used as well
&lt;div class=&quot;flash flash-warn&quot;&gt;
    &lt;svg class=&quot;octicon octicon-zap octicon octicon-zap octicon octicon-zap octicon octicon-zap&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M10.561 1.5a.016.016 0 00-.01.004L3.286 8.571A.25.25 0 003.462 9H6.75a.75.75 0 01.694 1.034l-1.713 4.188 6.982-6.793A.25.25 0 0012.538 7H9.25a.75.75 0 01-.683-1.06l2.008-4.418.003-.006a.02.02 0 00-.004-.009.02.02 0 00-.006-.006L10.56 1.5zM9.504.43a1.516 1.516 0 012.437 1.713L10.415 5.5h2.123c1.57 0 2.346 1.909 1.22 3.004l-7.34 7.142a1.25 1.25 0 01-.871.354h-.302a1.25 1.25 0 01-1.157-1.723L5.633 10.5H3.462c-1.57 0-2.346-1.909-1.22-3.004L9.503.429z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Important: &lt;/strong&gt;without the fine-tuning task, CLS vector is not a meaninful representation since it was trained with NSP (&lt;a href=&quot;https://arxiv.org/pdf/1810.04805.pdf&quot;&gt;ref&lt;/a&gt;)
&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;I have tried to summarize a foward pass of BERT thorugh the following gif:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/personal_blog/images/copied_from_nb/images/media/videos/scene/720p30/TransformerEncoderExample.gif&quot; alt=&quot;&quot; title=&quot;Entire Forward passing in BERT&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Working-in-Practice&quot;&gt;Working in Practice&lt;a class=&quot;anchor-link&quot; href=&quot;#Working-in-Practice&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;To show sentence embedding from BERT working, I usually rely on &lt;a href=&quot;https://huggingface.co/transformers/&quot;&gt;Hugging Face's transformer library&lt;/a&gt;. Here, since the &lt;strong&gt;Bert Model for Language Model&lt;/strong&gt; was trained already, I will be using the bare BERT Model without any specific head (e.g., &lt;code&gt;LanguageModeling head&lt;/code&gt; or &lt;code&gt;Sentence Classification head&lt;/code&gt;) on top of it!&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;details class=&quot;description&quot;&gt;
      &lt;summary class=&quot;btn btn-sm&quot; data-open=&quot;Hide Code&quot; data-close=&quot;Show Code&quot;&gt;&lt;/summary&gt;
        &lt;p&gt;&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;transformers&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BertTokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertForPreTraining&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertTokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;bert-base-uncased&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertModel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;bert-base-uncased&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
    &lt;/details&gt;
&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence_0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;He will get scared&amp;quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sequence_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;She will get the drinks&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;details class=&quot;description&quot;&gt;
      &lt;summary class=&quot;btn btn-sm&quot; data-open=&quot;Hide Code&quot; data-close=&quot;Show Code&quot;&gt;&lt;/summary&gt;
        &lt;p&gt;&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence_0_w2id&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence_0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# we need to map words to id&amp;#39;s :)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sequence_1_w2id&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Sequence 0 word2Id mapping: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence_0_w2id&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Sequence 1 word2Id mapping: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence_1_w2id&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
    &lt;/details&gt;
&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Sequence 0 word2Id mapping: [101, 2002, 2097, 2131, 6015, 102]
Sequence 1 word2Id mapping: [101, 2016, 2097, 2131, 1996, 8974, 102]
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;details class=&quot;description&quot;&gt;
      &lt;summary class=&quot;btn btn-sm&quot; data-open=&quot;Hide Code&quot; data-close=&quot;Show Code&quot;&gt;&lt;/summary&gt;
        &lt;p&gt;&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence_0_embeddings&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence_0_w2id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unsqueeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# Batch size 1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sequence_0_embeddings&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence_0_embeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_dict&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&amp;quot;last_hidden_state&amp;quot;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detach&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sequence_1_embeddings&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence_1_w2id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unsqueeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# Batch size 1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sequence_1_embeddings&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence_1_embeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_dict&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;kc&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&amp;quot;last_hidden_state&amp;quot;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detach&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
    &lt;/details&gt;
&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence_0_embeddings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequence_1_embeddings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;((1, 6, 768), (1, 7, 768))&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Since the first dimension means the batch size, we can get rid of it!&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;details class=&quot;description&quot;&gt;
      &lt;summary class=&quot;btn btn-sm&quot; data-open=&quot;Hide Code&quot; data-close=&quot;Show Code&quot;&gt;&lt;/summary&gt;
        &lt;p&gt;&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence_0_embeddings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence_0_embeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sequence_1_embeddings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence_1_embeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sequence_0_embeddings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequence_1_embeddings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
    &lt;/details&gt;
&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;((6, 768), (7, 768))&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;It turns out that this model generates one embedding for each word plus &lt;code&gt;CLS&lt;/code&gt; and &lt;code&gt;SEP&lt;/code&gt; tokens. This explains why sentence_0 and sentence_1 both start and end with the same token number! Let's perform some cool math to analyze some patterns :)&lt;/p&gt;
&lt;p&gt;First, let's analyze the similarity between CLS and token words&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;details class=&quot;description&quot;&gt;
      &lt;summary class=&quot;btn btn-sm&quot; data-open=&quot;Hide Code&quot; data-close=&quot;Show Code&quot;&gt;&lt;/summary&gt;
        &lt;p&gt;&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CLS_TOKEN_0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequence_0_embeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;CLS_TOKEN_WORDS_0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence_0_embeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Cosine Similatiry between CLS token and the average of&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence_0&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;#39;&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot; tokens: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cosine_similarity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CLS_TOKEN_0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CLS_TOKEN_WORDS_0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
    &lt;/details&gt;
&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Cosine Similatiry between CLS token and the average of
&amp;#39;He will get scared&amp;#39; tokens: 0.29071152210235596
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;details class=&quot;description&quot;&gt;
      &lt;summary class=&quot;btn btn-sm&quot; data-open=&quot;Hide Code&quot; data-close=&quot;Show Code&quot;&gt;&lt;/summary&gt;
        &lt;p&gt;&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CLS_TOKEN_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequence_1_embeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;CLS_TOKEN_WORDS_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence_1_embeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Cosine Similatiry between CLS token and the average of &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence_1&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;#39;&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot; tokens: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cosine_similarity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CLS_TOKEN_1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CLS_TOKEN_WORDS_1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
    &lt;/details&gt;
&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Cosine Similatiry between CLS token and the average of 
&amp;#39;She will get the drinks&amp;#39; tokens: 0.32392317056655884
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;It is interesting since as stated by the paper, the CLS token &lt;em&gt;seems to be meaninfulless&lt;/em&gt;. Then, let's analyze the similarity between the average tokens embeddings of each sentence&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Cosine Similatiry between average of embedding tokens of
&amp;#39;He will get scared&amp;#39;and &amp;#39;She will get the drinks&amp;#39; tokens :0.6591895222663879
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;strong&gt;As expected&lt;/strong&gt;, despite the fact that &lt;em&gt;similar&lt;/em&gt; words were used, their contexts were totally different and therefore, their embeddings similarities were less than the plain word vectors :)&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Conclusion&quot;&gt;Conclusion&lt;a class=&quot;anchor-link&quot; href=&quot;#Conclusion&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Congratulations! You have learned the main concepts behind the BERT model :) Please stay tuned, tor future blog posts :) I intend adding distillation about some BERT fine tuning as well as dissecting it from scratch!&lt;/p&gt;
&lt;p&gt;However, if you want to have a higher level approach about how this works, I &lt;a href=&quot;https://huggingface.co/blog/how-to-train&quot;&gt;highly recommend this blog post&lt;/a&gt;!&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Resources-that-have-inspired-me&quot;&gt;Resources that have inspired me&lt;a class=&quot;anchor-link&quot; href=&quot;#Resources-that-have-inspired-me&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Besides all other papers that I have referenced through this post, I would like to emphaisze the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://jalammar.github.io/illustrated-bert/&quot;&gt;http://jalammar.github.io/illustrated-bert/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://jalammar.github.io/illustrated-transformer/&quot;&gt;https://jalammar.github.io/illustrated-transformer/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://nlp.seas.harvard.edu/2018/04/03/attention.html&quot;&gt;http://nlp.seas.harvard.edu/2018/04/03/attention.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Acknowledgments&quot;&gt;Acknowledgments&lt;a class=&quot;anchor-link&quot; href=&quot;#Acknowledgments&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;I would really like to appreciate the effort made by some colleagues that provided a fantastic technical review for this blog post :)&lt;/p&gt;
&lt;p&gt;In alphabetical order:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/in/alan-barzilay-58754855/&quot;&gt;Alan Barzilay&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/in/alvaro-marques-9a10aa131/&quot;&gt;Alvaro Marques&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/in/ighoelscher/&quot;&gt;Igor Hoelscher&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Andre Barbosa</name></author><category term="masters" /><category term="nlp" /><category term="knowledge-distill" /><summary type="html"></summary></entry><entry><title type="html">Destilando Pré Treinamento do BERT</title><link href="https://abarbosa94.github.io/personal_blog/masters/nlp/knowledge-distill/2020/09/19/Distilling-BERT-pt.html" rel="alternate" type="text/html" title="Destilando Pré Treinamento do BERT" /><published>2020-09-19T00:00:00-05:00</published><updated>2020-09-19T00:00:00-05:00</updated><id>https://abarbosa94.github.io/personal_blog/masters/nlp/knowledge-distill/2020/09/19/Distilling-BERT-pt</id><content type="html" xml:base="https://abarbosa94.github.io/personal_blog/masters/nlp/knowledge-distill/2020/09/19/Distilling-BERT-pt.html">&lt;!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/2020-09-19-Distilling-BERT-pt.ipynb
--&gt;

&lt;div class=&quot;container&quot; id=&quot;notebook-container&quot;&gt;
        
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;Para uma versão em &lt;strong&gt;inglês&lt;/strong&gt; confira &lt;a href=&quot;https://abarbosa94.github.io/personal_blog/masters/nlp/knowledge-distill/2020/09/19/Distilling-BERT.html&quot;&gt;aqui&lt;/a&gt;
&lt;/div&gt;&lt;/p&gt;
&lt;h1 id=&quot;Uma-r&amp;#225;pida-revis&amp;#227;o&quot;&gt;Uma r&amp;#225;pida revis&amp;#227;o&lt;a class=&quot;anchor-link&quot; href=&quot;#Uma-r&amp;#225;pida-revis&amp;#227;o&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Eu lembro algum dia de 2016, quando eu estava no início da kinha carreira, eu encontrei por acaso o &lt;a href=&quot;http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/&quot;&gt;blog do Chirs McCormick sobre Word2Vec&lt;/a&gt;. Honestamente, acredito que o &lt;a href=&quot;https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf&quot;&gt;artigo escrito pelo Tomas Mikolov&lt;/a&gt; foi uma das indéias mais interessantes que eu já encontrei nessa minha jornada como cientista de dados &lt;sup id=&quot;fnref-1&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-1&quot;&gt;1&lt;/a&gt;&lt;/sup&gt; :)&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-1&quot;&gt;1. Fun Fact: O &lt;a href=&quot;https://www.linkedin.com/in/tomas-mikolov-59831188/?originalSubdomain=cz&quot;&gt;perfil do LinkedIn do Miklov&lt;/a&gt; mostra que ele trabalhou na Microsoft, Google e Facebook; outro autor do W2V, &lt;a href=&quot;http://www.cs.toronto.edu/~ilya/&quot;&gt;Ilya Sutskever&lt;/a&gt; teve oportunidades de trabalhar com os maiores pesquisadores da área moderna de IA, tais como &lt;a href=&quot;https://www.cs.toronto.edu/~hinton/&quot;&gt;Geoffrey Hinton&lt;/a&gt; e &lt;a href=&quot;https://www.andrewng.org/&quot;&gt;Andrew Ng&lt;/a&gt;. Além disso, ele é um dos fundadores da &lt;a href=&quot;https://openai.com/&quot;&gt;Open AI&lt;/a&gt;! &lt;a href=&quot;#fnref-1&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h2 id=&quot;O-que-s&amp;#227;o-Word-Embeddings&quot;&gt;O que s&amp;#227;o Word Embeddings&lt;a class=&quot;anchor-link&quot; href=&quot;#O-que-s&amp;#227;o-Word-Embeddings&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Segundo a documentação do &lt;a href=&quot;https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html&quot;&gt;Pytorch&lt;/a&gt;,  um &lt;strong&gt;Embedding&lt;/strong&gt; pode ser definido da seguinte forma:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Uma tabela de lookup formada por um &lt;em&gt;dicionário&lt;/em&gt; de tamanho fixo.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Podemos interpretar os embeddings como uma forma de converter &lt;em&gt;índices&lt;/em&gt; em &lt;em&gt;vetores&lt;/em&gt; de um tamanho específico. Logo, &lt;strong&gt;word embeddings&lt;/strong&gt;, podem ser entendidos como palavras que são convertidas para inteiros e &lt;strong&gt;esses&lt;/strong&gt; números servem de índices para diferentes linhas de uma matriz que representa o espaço vetorial.'&lt;/p&gt;
&lt;p&gt;Eu escrevi um código usando &lt;a href=&quot;https://github.com/3b1b/manim&quot;&gt;manim&lt;/a&gt; que ilustra isso:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/personal_blog/images/copied_from_nb/images/media/videos/scene/720p30/EmbeddingExample.gif&quot; alt=&quot;&quot; title=&quot;Nesse exemplo, a dimensão do embedding é NxM, em que N seria o tamanho do vocabulário (8) e M é 4.&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Podemos interpretar cada dimensão como um único neurônio de uma camada oculta, e, então, &lt;strong&gt;o tamanho desses embeddings podem ter seus números alterados&lt;/strong&gt; a partir de uma rede neural. Essa é, basicamente, a ideia por trás de algoritmos como &lt;a href=&quot;https://patents.google.com/patent/US9037464B1/en&quot;&gt;Word2Vec&lt;/a&gt; e &lt;a href=&quot;https://fasttext.cc/&quot;&gt;fastText&lt;/a&gt; &lt;sup id=&quot;fnref-2&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-2&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;Já existem algumas bibliotecas que já fornecem alguns vetores pré-treinados. Por exemplo, considere o &lt;a href=&quot;https://spacy.io/models&quot;&gt;código Spacy&lt;/a&gt; abaixo:&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-2&quot;&gt;2. Eu não irei cobrir Word2Vec nesse blog post. Se você não tem familiaridade com isso, &lt;a href=&quot;http://jalammar.github.io/illustrated-word2vec/&quot;&gt;consulte aqui&lt;/a&gt;; &lt;a href=&quot;http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/&quot;&gt;aqui&lt;/a&gt; e &lt;a href=&quot;https://www.youtube.com/watch?v=ASn7ExxLZws&quot;&gt;aqui&lt;/a&gt;. Infelizmente, todos os links estão em inglês. Se você achar quiser que eu escreva um post sobre Word2Vec em português, me envie uma mensagem no meu &lt;a href=&quot;https://www.linkedin.com/in/barbosaandre/&quot;&gt;linkedin&lt;/a&gt; :)&lt;a href=&quot;#fnref-2&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;details class=&quot;description&quot;&gt;
      &lt;summary class=&quot;btn btn-sm&quot; data-open=&quot;Hide Code&quot; data-close=&quot;Show Code&quot;&gt;&lt;/summary&gt;
        &lt;p&gt;&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;spacy&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;nlp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;spacy&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;load&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;pt_core_news_sm&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Considere a sentença &amp;#39;O rato roeu a roupa do rei de Roma!&amp;#39;&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nlp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;O rato roeu a roupa do rei de Roma!&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;word&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
        &lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&amp;#39;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;#39; representação vetorial com tamanho &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;. Seus primeiros 5 elementos são: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;word&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
    &lt;/details&gt;
&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Considere a sentença &amp;#39;O rato roeu a roupa do rei de Roma!&amp;#39;
&amp;#39;O&amp;#39; representação vetorial com tamanho 96. Seus primeiros 5 elementos são: [ 1.2   0.18 -0.97 -5.64 -4.65]
&amp;#39;rato&amp;#39; representação vetorial com tamanho 96. Seus primeiros 5 elementos são: [ 3.17  5.36  0.14 -1.27  3.09]
&amp;#39;roeu&amp;#39; representação vetorial com tamanho 96. Seus primeiros 5 elementos são: [ 1.17 -2.8   2.39 -0.33  0.4 ]
&amp;#39;a&amp;#39; representação vetorial com tamanho 96. Seus primeiros 5 elementos são: [-0.99  4.67  1.21 -3.48 -2.62]
&amp;#39;roupa&amp;#39; representação vetorial com tamanho 96. Seus primeiros 5 elementos são: [ 3.6   3.54 -0.66 -1.9   1.99]
&amp;#39;do&amp;#39; representação vetorial com tamanho 96. Seus primeiros 5 elementos são: [-3.28 -2.15 -1.62  4.33  0.55]
&amp;#39;rei&amp;#39; representação vetorial com tamanho 96. Seus primeiros 5 elementos são: [ 2.43  2.99 -2.72  2.31  5.31]
&amp;#39;de&amp;#39; representação vetorial com tamanho 96. Seus primeiros 5 elementos são: [-4.49 -1.73  2.27  7.9   3.35]
&amp;#39;Roma&amp;#39; representação vetorial com tamanho 96. Seus primeiros 5 elementos são: [ 4.87  0.42  1.91 -1.68  6.37]
&amp;#39;!&amp;#39; representação vetorial com tamanho 96. Seus primeiros 5 elementos são: [ 0.61 -3.03 -1.37 -0.38 -2.72]
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Essas palavras são representações que foram treinadas com base nos dados do &lt;a href=&quot;https://github.com/explosion/spacy-models/releases/tag/en_core_web_md-3.0.0&quot;&gt;Common Crawl usando o algoritmo GloVe&lt;/a&gt;. Diferente do exemplo usado no começo deste blog, a palavra '!' também teve uma representação vetorial.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Para formar frases, podemos combinar embeddings de palavras de formas diferentes. Segundo a &lt;a href=&quot;https://spacy.io/usage/vectors-similarity#_title&quot;&gt;documentação do spacy&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Modelos que possuem vetores de palavras estão disponíveis pelo atributo Token.vector. Doc.vector e Span.vector, por padrão são representados pela &lt;strong&gt;média&lt;/strong&gt; da representação de seus vetores.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Logo, a frase que estamos usando como exemplo tem a seguinte representação vetorial:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Os primeiros 5 valores de &amp;#39;The quick brown fox jumps over the lazy dog!!&amp;#39;: [-0.23  0.08 -0.03 -0.07 -0.02]
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Limita&amp;#231;&amp;#245;es-dos-Word-Embeddings&quot;&gt;Limita&amp;#231;&amp;#245;es dos Word Embeddings&lt;a class=&quot;anchor-link&quot; href=&quot;#Limita&amp;#231;&amp;#245;es-dos-Word-Embeddings&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Apesar de Word Embeddings trouxeram muitos benefícios na área de linguística computacional, eles possuem algumas limitações. Existe um fenômeno na linguística chamado &lt;em&gt;polissemia&lt;/em&gt;. De acordo com o &lt;a href=&quot;https://pt.wikipedia.org/wiki/Sem%C3%A2ntica&quot;&gt;wikipedia&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;É a propriedade que uma mesma palavra tem de apresentar vários significados. Exemplos:Ele ocupa um alto posto na empresa. / Abasteci meu carro no posto da esquina. / Os convites eram de graça. / Os fiéis agradecem a graça recebida.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Considerando o exemplo acima, mesmo que as palavras tenham &lt;strong&gt;significados diferentes&lt;/strong&gt; por conta do contexto, &lt;strong&gt;sua representação vetorial é a mesma&lt;/strong&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Primeiros cinco valores da palavra &amp;#39;posto&amp;#39;: [ 2.23  0.89  1.63  1.8  -0.12]
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Se pegarmos duas frases: &lt;code&gt;Ele ocupa um alto posto na empresa&lt;/code&gt; e &lt;code&gt;Abasteci meu carro no posto da esquina&lt;/code&gt;, então nós teremos os seguintes vetores:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nlp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Ele ocupa um alto posto na empresa&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;text2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;nlp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Abasteci meu carro no posto do alto do morro&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Os primeiros 5 valores do vetor da sentença &amp;#39;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text1&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt; &amp;#39;: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Os primeiros 5 valores do vetor da sentença &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text2&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt; &amp;#39;: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;round&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Os primeiros 5 valores do vetor da sentença &amp;#39;Ele ocupa um alto posto na empresa &amp;#39;: [ 0.55  1.04  0.21 -0.36  0.51]
Os primeiros 5 valores do vetor da sentença Abasteci meu carro no posto do alto do morro &amp;#39;: [ 0.73  0.82 -0.78  2.11  0.61]
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Ao calcular a &lt;strong&gt;similaridade de cossenos&lt;/strong&gt; entre a média destes vetores:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;details class=&quot;description&quot;&gt;
      &lt;summary class=&quot;btn btn-sm&quot; data-open=&quot;Hide Code&quot; data-close=&quot;Show Code&quot;&gt;&lt;/summary&gt;
        &lt;p&gt;&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.metrics.pairwise&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;cosine_similarity&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Similaridade:&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt; &amp;#39;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text1&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;#39; and &amp;#39;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text2&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;#39;: &amp;quot;&lt;/span&gt;
    &lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cosine_similarity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text2&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;vector&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
    &lt;/details&gt;
&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Similaridade:
 &amp;#39;Ele ocupa um alto posto na empresa&amp;#39; and &amp;#39;Abasteci meu carro no posto do alto do morro&amp;#39;: 0.5666443705558777
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Isso indica que ambos os vetores tem alguma similares. Contudo, a razão disso foi o uso de palavras parecidas, uma vez que o significado das sentenças é &lt;strong&gt;completamente&lt;/strong&gt; diferente.&lt;/p&gt;
&lt;p&gt;Isso é algo que o BERT tenta resolver.&lt;sup id=&quot;fnref-3&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-3&quot;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-3&quot;&gt;3. Existem alguns percursores do BERT como o &lt;a href=&quot;https://allennlp.org/elmo&quot;&gt;ELMo&lt;/a&gt;; &lt;a href=&quot;https://arxiv.org/abs/1801.06146&quot;&gt;ULMFit&lt;/a&gt; e &lt;a href=&quot;https://openai.com/blog/language-unsupervised/&quot;&gt;Open AI Transformer&lt;/a&gt; que eu não irei cobrir aqui. Por favor, caso você queira, confira esse post &lt;a href=&quot;http://jalammar.github.io/illustrated-bert/&quot;&gt;aqui&lt;/a&gt; para saber mais&lt;a href=&quot;#fnref-3&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;BERT&quot;&gt;BERT&lt;a class=&quot;anchor-link&quot; href=&quot;#BERT&quot;&gt; &lt;/a&gt;&lt;/h1&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Attention-&amp;#233;-tudo-o-que-voc&amp;#234;-precisa&quot;&gt;Attention &amp;#233; tudo o que voc&amp;#234; precisa&lt;a class=&quot;anchor-link&quot; href=&quot;#Attention-&amp;#233;-tudo-o-que-voc&amp;#234;-precisa&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;O artigo &lt;a href=&quot;https://arxiv.org/abs/1706.03762&quot;&gt;Attention is all you need&lt;/a&gt; introduziu a chamada arquitetura Transformer, que pode ser resumida pela imagem abaixo:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/personal_blog/images/copied_from_nb/images/transformer.png&quot; alt=&quot;&quot; title=&quot;A arquitetura Transformer. Fonte: https://arxiv.org/abs/1706.03762&quot; /&gt;&lt;/p&gt;
&lt;p&gt;A principal motivação por trás desse paper é que arquiteturas baseadas em &lt;em&gt;RNN&lt;/em&gt; tem um custo computacional de memória caro. A proposta por trás dos Transformers, então, é que resultados similareas à uma &lt;em&gt;RNN&lt;/em&gt; poderiam ser obtidos de uma forma muito mais eficiente aplicas, &lt;strong&gt;apenas&lt;/strong&gt;, mecanismos de atenção (e evitando arquiteturas até então conhecidas, como &lt;em&gt;CNN&lt;/em&gt; ou &lt;em&gt;RNN&lt;/em&gt;) !&lt;sup id=&quot;fnref-4&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-4&quot;&gt;4&lt;/a&gt;&lt;/sup&gt; Apesar do fato de que a proposta original em torno dos Transformers é que eles resolveriam problemas de tradução, percebeu-se que apenas algumas variações em seu funcionamento seriam capazes de atingir resultados &lt;strong&gt;incríveis&lt;/strong&gt; em outras áreas. Essa é, basicamente, a principal motivação por trás do modelo &lt;strong&gt;BERT&lt;/strong&gt;!&lt;/p&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-4&quot;&gt;4. &lt;a href=&quot;http://nlp.seas.harvard.edu/2018/04/03/attention.html&quot;&gt;O grupo de NLP de Harvard&lt;/a&gt; escreveu um blog post muito bom que explica o passo a passo desse paper, além de apresentar uma implementação do mesmo em pytorch. Se você tiver interesse de entender essa arquiteutra com mais detalhes, eu recomendo dar uma lida! &lt;a href=&quot;#fnref-4&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;
&lt;h3 id=&quot;Aten&amp;#231;&amp;#227;o?&quot;&gt;Aten&amp;#231;&amp;#227;o?&lt;a class=&quot;anchor-link&quot; href=&quot;#Aten&amp;#231;&amp;#227;o?&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Segundo a aula de &lt;a href=&quot;https://atcold.github.io/pytorch-Deep-Learning/en/week12/12-3/&quot;&gt;Transformer e Atenção do curso de Fundamentos de Deep Learning da NYU&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Transformers são compostos por módulos de atenção, os quais podem ser entendidos como um mapeamento entre conjuntos (não sequências). Em outras palavras, nós não precisamos nos preocupar entre a relação de &lt;strong&gt;ordenação&lt;/strong&gt; entre os valores de entrada e saída.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Ao analisarmos os mecanismos de atenção da arquitetura transformer, tanto o &lt;em&gt;Multi-Head Attention&lt;/em&gt; quanto &lt;em&gt;Multi-Head Masked Attention&lt;/em&gt; possuem 3 &lt;em&gt;Arrow Heads&lt;/em&gt;. Cada uma dessas cabeças tem a seguinte representação:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Q&lt;/em&gt; Significa o vetor da &lt;strong&gt;query&lt;/strong&gt; , com uma dimensão $d_k$ &lt;/li&gt;
&lt;li&gt;&lt;em&gt;K&lt;/em&gt; Significa o vetor da &lt;strong&gt;chave&lt;/strong&gt; (key), o qual também tem $d_k$&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;em&gt;V&lt;/em&gt; Significa o vetor de &lt;strong&gt;value&lt;/strong&gt;, com uma dimensão $d_v$&lt;/p&gt;
&lt;p&gt;O par &lt;strong&gt;KV&lt;/strong&gt;, no caso, são os inputs da rede, enquanto o &lt;em&gt;Q&lt;/em&gt; é a saída de uma camada específica.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;Armazenamento-Key-Value&quot;&gt;Armazenamento Key-Value&lt;a class=&quot;anchor-link&quot; href=&quot;#Armazenamento-Key-Value&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Ainda de acordo com uma das aulas do curso&lt;a href=&quot;https://atcold.github.io/pytorch-Deep-Learning/en/week12/12-3/&quot;&gt; Fundamentos de Deep Learning da NYU&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;O armazenamento chave-valor (&lt;em&gt;key-value&lt;/em&gt;) é um paradigma desenvolvido para armazenar (saving), recuperar (querying), e gerenciar arrays associativos (dictionaries/hash tables)&lt;/p&gt;
&lt;p&gt;Por exemplo, considere que queremos fazer uma receita de &lt;em&gt;lasanha&lt;/em&gt;. Nós temos a receita em um livro e , para encontrala, procuramos pela palavra &lt;em&gt;lasanha&lt;/em&gt;, que seria a nossa &lt;strong&gt;query&lt;/strong&gt;. Essa query é comparada contra todas as outras &lt;strong&gt;chaves&lt;/strong&gt; possíveis. Estas, por sua vez, poderiam representar os títulos de todas as receitas no livro. Então, podemos checar aplicar um matching score entre todas as &lt;strong&gt;chaves&lt;/strong&gt; em relação à &lt;strong&gt;query&lt;/strong&gt;. Caso a saída desse score seja o argmax, podemos retornar apenas a receita (&lt;strong&gt;value&lt;/strong&gt;) com o valor máximo. Se for a softmax, podemos retornar uma distribuição de probabilidades e, então, descobrir as receitas mais similares com a query ou as menos similares.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;div class=&quot;flash flash-error&quot;&gt;
    &lt;svg class=&quot;octicon octicon-alert&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8.22 1.754a.25.25 0 00-.44 0L1.698 13.132a.25.25 0 00.22.368h12.164a.25.25 0 00.22-.368L8.22 1.754zm-1.763-.707c.659-1.234 2.427-1.234 3.086 0l6.082 11.378A1.75 1.75 0 0114.082 15H1.918a1.75 1.75 0 01-1.543-2.575L6.457 1.047zM9 11a1 1 0 11-2 0 1 1 0 012 0zm-.25-5.25a.75.75 0 00-1.5 0v2.5a.75.75 0 001.5 0v-2.5z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Warning: &lt;/strong&gt;Eu decidi não cobrir os conceitos de atenção em grandes detalhes. Para quem quiser saber mais, eu fortemente recomendo o curso da NYU de Fundamentos de Deep Learning que eu já citei acima.
&lt;/div&gt;&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;De uma maneira genérica, um mecanismo de atenção pode ser entendido, basicamente, como uma medida de correção entre dois conjuntos de palavras. Para quem quer, realmente, estudar o assunto em profundidade, esse &lt;a href=&quot;https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html&quot;&gt;blog&lt;/a&gt; é excelente.
&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Encoding-Posicional&quot;&gt;Encoding Posicional&lt;a class=&quot;anchor-link&quot; href=&quot;#Encoding-Posicional&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;&lt;a href=&quot;http://nlp.seas.harvard.edu/2018/04/03/attention.html#prelims&quot;&gt;Eu retirei essa sessão por parte do blog annotated transformer&lt;/a&gt;, onde é possível encontrar uma implementação em pytorch. Na verdade, o quote abaixo é retirado diretamente do paper &lt;a href=&quot;https://arxiv.org/pdf/1706.03762.pdf&quot;&gt;Attention is all you need&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Uma vez que nosso modelo não contém recorrência ou convolução, para fazer com que o modelo aprenda alguma noção de sequência, nós precisamos injetar alguma informação sobre a posição absoluta ou relativa sobre as palavras (tokens) de uma certa sequência. Esses &quot;encoding posicionais&quot; são somados aos embeddings de entrada tanto na pilha de encoder, quanto na pilha de decoder. Por conta disso, o &quot;encoding posicional&quot; tem a mesma dimensão $d_{model}$ que os embeddings (para que, então, eles possam ser somados).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;img src=&quot;/personal_blog/images/copied_from_nb/images/positional_encoding.png&quot; alt=&quot;&quot; title=&quot;Um exemplo de um embedding posicional que gera ondas senóides com base no tamanho. Note que cada dimensão gera uma senóide com uma frequência diferente. Fonte:http://nlp.seas.harvard.edu/2018/04/03/attention.html&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;O-modelo-BERT&quot;&gt;O modelo BERT&lt;a class=&quot;anchor-link&quot; href=&quot;#O-modelo-BERT&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;O modelo BERT é, na prática, um modelo &lt;em&gt;encoder&lt;/em&gt;, derivado da arquitetura transformer. Considerando os modelos treinados do &lt;a href=&quot;https://arxiv.org/pdf/1810.04805.pdf&quot;&gt;paper&lt;/a&gt;, o modelo &lt;strong&gt;base&lt;/strong&gt; consiste de 12 camadas &lt;em&gt;encoder&lt;/em&gt; empilhados, enquanto o modelo &lt;strong&gt;large&lt;/strong&gt; é composto de 24 camadas &lt;em&gt;encoder&lt;/em&gt; empilhadas.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;De acordo com o paper &lt;a href=&quot;https://arxiv.org/pdf/1706.03762.pdf&quot;&gt;Attention is all you need paper&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;O encoder é composto por uma pilha de $N = 6$ camadas. Cada camada é formada por &lt;strong&gt;dois 
sub-camada&lt;/strong&gt;. A primeira sub-camada é um mecanismo &lt;strong&gt;multi-head self-attention&lt;/strong&gt;, enquanto a segunda é uma rede &lt;strong&gt;fully connected feed-forward position wise &lt;/strong&gt;. Nós aplicamos uma &lt;a href=&quot;https://arxiv.org/abs/1512.03385&quot;&gt;residual connection&lt;/a&gt; ao redor de cada uma das duas sub-camadas, seguido de uma &lt;a href=&quot;https://arxiv.org/abs/1607.06450&quot;&gt;camada de normalização&lt;/a&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/personal_blog/images/copied_from_nb/images/sublayers.jpg&quot; alt=&quot;&quot; title=&quot;The encoder layer. Source: https://atcold.github.io/pytorch-Deep-Learning/en/week12/12-1/&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;The-Multi-Head-Attention&quot;&gt;The Multi-Head Attention&lt;a class=&quot;anchor-link&quot; href=&quot;#The-Multi-Head-Attention&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Basicamente, o mecanismo de atençção multi head é um &lt;em&gt;tipo&lt;/em&gt; de mecanismo de atenção. Ele formado pela &lt;em&gt;concatenação&lt;/em&gt; de outro mecanosmo, o &lt;em&gt;produto interno&lt;/em&gt; (scaled dot). A representação de ambos mecanismos se dá pela imagem abaixo:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/personal_blog/images/copied_from_nb/images/attention_specific.png&quot; alt=&quot;&quot; title=&quot;(escerda) Atenção Scaled Dot-Product seguida da atenção Multi-Head, que consistem em uma sére de camadas de atenção rodando em paralelo. Fonte: https://atcold.github.io/pytorch-Deep-Learning/en/week12/12-1/&quot; /&gt;
&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;A forma de calcular a atenção Scaled Dot-Product é dada por $softmax(\frac{QK^T}{\sqrt{n}})V$, em que &lt;em&gt;K&lt;/em&gt;, &lt;em&gt;V&lt;/em&gt; and &lt;em&gt;Q&lt;/em&gt; são os mesmos que os descritos na sessão antetior, enquanto &lt;em&gt;n&lt;/em&gt; representa o número de elementos dentro do conjunto. 
&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;h&lt;/em&gt;, ou o número de camadas de atenção é igual a $12$ no caso do $\text{BERT}_\text{base}$, e $16$ no caso do $\text{BERT}_\text{large}$&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h3 id=&quot;Conex&amp;#245;es-Residuais&quot;&gt;Conex&amp;#245;es Residuais&lt;a class=&quot;anchor-link&quot; href=&quot;#Conex&amp;#245;es-Residuais&quot;&gt; &lt;/a&gt;&lt;/h3&gt;&lt;p&gt;Cada subcamada da pilha de encoder contém uma conexão resiudal (a flecha curvada à esquerda) adicionada à saída da subcamada anterior à camada de normalização. A &lt;a href=&quot;https://arxiv.org/pdf/1512.03385.pdf&quot;&gt;idea de Conexões Residuais&lt;/a&gt; vem do campo de visão computacional e, na verdade, é uma técnica que pode ser resumida pela seguinte imagem:&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/personal_blog/images/copied_from_nb/images/residual_connection.png&quot; alt=&quot;&quot; title=&quot;Residual Connection example. Source (https://arxiv.org/pdf/1512.03385.pdf)&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Considerando a arquitetura de pilha Encoder, cada $\mathcal{F}(x)$ significa ou a atenção &lt;em&gt;Multi-Head&lt;/em&gt; ou a camada &lt;em&gt;Feed Forward&lt;/em&gt;. Logo, citando o paper:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Ou seja, a &lt;strong&gt;saída de cada sub camada é LayerNorm(x + Sublayer(x))&lt;/strong&gt;, onde cada Sublayer(x) é a função implementada pela subcamada em si. Para &lt;em&gt;facilitar essas conexões individuais&lt;/em&gt;, todas as sub-camadas do modelo, assim como as camadas de embedding, produzem saídas de dimenção $d_{model} = 512$ &lt;sup id=&quot;fnref-5&quot; class=&quot;footnote-ref&quot;&gt;&lt;a href=&quot;#fn-5&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;div class=&quot;footnotes&quot;&gt;&lt;p id=&quot;fn-5&quot;&gt;5. No caso do &lt;a href=&quot;https://arxiv.org/pdf/1810.04805.pdf&quot;&gt;BERT&lt;/a&gt;, tenha em mente que $N$ pode ser $12$ (BERT&lt;sub&gt;base&lt;/sub&gt;) ou $24$ ((BERT&lt;sub&gt;large&lt;/sub&gt;) e &lt;em&gt;d&lt;sub&gt;model&lt;/sub&gt;&lt;/em&gt; é 768 para o BERT base e 1024 para o BERT large&lt;a href=&quot;#fnref-5&quot; class=&quot;footnote footnotes&quot;&gt;↩&lt;/a&gt;&lt;/p&gt;&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Mas o que, de fato, está sendo encodado?&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Representa&amp;#231;&amp;#227;o-dos-Embeddings&quot;&gt;Representa&amp;#231;&amp;#227;o dos Embeddings&lt;a class=&quot;anchor-link&quot; href=&quot;#Representa&amp;#231;&amp;#227;o-dos-Embeddings&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Quando o paper foi escrito, os autores tinham em mente que o BERT deveria performar bem em diferentes tarefas, tais como binary e multi lablel classification_; &lt;em&gt;language modeling&lt;/em&gt;; &lt;em&gt;question and answering&lt;/em&gt;; &lt;em&gt;named entity recognition&lt;/em&gt;; &lt;em&gt;etc&lt;/em&gt;. Fazendo a paráfrase do original:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Nossa representação de entrada tem que, de maneira desambiguável, representar tanto uma sentença única, quanto um par de sentenças em uma única sequência de tokens. Diferente de uma sentença na linguística tradicional, no BERT, uma &quot;sentença&quot; pode ser um span arbitrário de texto contínuo. Uma &quot;sequência&quot; representa a sequência de tokens de entrada no BERT, o que pode ser uma sentença única ou duas sentenças agrupadas.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Para performar e criar esses embeddings de sentenças, utilizou-se o &lt;a href=&quot;https://arxiv.org/abs/1609.08144&quot;&gt;WordPiece&lt;/a&gt;. Então, além de adicionar o [CLS] token, pares de sentença (e.g. sentence &lt;em&gt;A&lt;/em&gt; and &lt;em&gt;B&lt;/em&gt;) são concatenados em uma sentença única, sendo separados por um token especial [SEP] (e.g. &lt;em&gt;A&lt;/em&gt; [SEP] &lt;em&gt;B&lt;/em&gt;). 
Então:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Para um dado token, sua representação é construída ao somar o token respectivo, o segmento (A ou B) e os positional embeddings.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/personal_blog/images/copied_from_nb/images/token_embeddings.png&quot; alt=&quot;&quot; title=&quot;BERT input representation. Source: https://arxiv.org/pdf/1810.04805.pdf&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Pr&amp;#233;-Treinamento-do-BERT&quot;&gt;Pr&amp;#233; Treinamento do BERT&lt;a class=&quot;anchor-link&quot; href=&quot;#Pr&amp;#233;-Treinamento-do-BERT&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;A primeira parte do BERT é um processo de pé treinamento que tem &lt;strong&gt;duas&lt;/strong&gt; funções objetivo&lt;/p&gt;
&lt;h2 id=&quot;Modelo-de-Linguagem-Mascarado-(MLM)&quot;&gt;Modelo de Linguagem Mascarado (MLM)&lt;a class=&quot;anchor-link&quot; href=&quot;#Modelo-de-Linguagem-Mascarado-(MLM)&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Conforme estamos alimentando o modelo com sentenças e considerando que estamos treinando um modelo de linguagem (isto é, queremos prever a palavra seguinte dado as palavras anteriores), como o BERT é bidirecional, isso acaba sendo problemático. A solução, proposta por essa &lt;em&gt;loss function&lt;/em&gt; é relativamente simples. Para fraseando o &lt;a href=&quot;https://arxiv.org/pdf/1810.04805.pdf&quot;&gt;paper&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Infelizmente, modelos de linguagem convencionais são apenas treinados considerando como input sentenças da esquerda para direita ou direita para esquerda, já que condicionalidade bidirecional permitiria que cada palavra &quot;tivesse acesso a ela mesma&quot; e, logo, o modelo conseguiria fazer a previsão de uma maneira direta.&lt;/p&gt;
&lt;p&gt;Para treinar uma representação bidirecional, nós mascaramos, de forma aleatória, uma certa porcentagem da entrada e prevismos estes tokens que forem ocultados. Nos referimos a esse processo como modelo de linguagem mascarado, apesar de que ele também recebe o nome de &lt;em&gt;cloze task&lt;/em&gt; na &lt;a href=&quot;https://journals.sagepub.com/doi/abs/10.1177/107769905303000401&quot;&gt;literatura&lt;/a&gt;. 
No caso do BERT, 15% de cada sentença é mascarada durante a etapa de treinamento.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;img src=&quot;/personal_blog/images/copied_from_nb/images/mlm.png&quot; alt=&quot;&quot; title=&quot;MLM task. Taken from here: http://jalammar.github.io/illustrated-bert/&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h2 id=&quot;Predi&amp;#231;&amp;#227;o-da-Pr&amp;#243;xima-Senten&amp;#231;a-(Next-Sentence-Prediction--NSP)&quot;&gt;Predi&amp;#231;&amp;#227;o da Pr&amp;#243;xima Senten&amp;#231;a (Next Sentence Prediction- NSP)&lt;a class=&quot;anchor-link&quot; href=&quot;#Predi&amp;#231;&amp;#227;o-da-Pr&amp;#243;xima-Senten&amp;#231;a-(Next-Sentence-Prediction--NSP)&quot;&gt; &lt;/a&gt;&lt;/h2&gt;&lt;p&gt;Para aprender relações entre pares de sentença (i.e. tarefas de perguntas e respostas), os autores precisaram pensar em algo além da modelagem de língua tradicional. Então:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Para treinar um modelo que entenda relacionamento de sentenças, nós pre treinamos um modelo binarizado previsor de próxima sentença que poderia ser gerado de qualquer corpus monolingual. Especificamente, quando escolhemos sentenças A e B para cada exemplo de pré treino, 50% das vezes B é, de fato, a sentença seguinte de A (marcada como &lt;code&gt;IsNext&lt;/code&gt;) e 50% das vezes é uma sequência aleatória (marcada como &lt;code&gt;NotNext&lt;/code&gt;).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Ambas funções objetivo (MLM e NSP) são usadas para o pré treinamento do BERT  :)
&lt;img src=&quot;/personal_blog/images/copied_from_nb/images/nsp.png&quot; alt=&quot;&quot; title=&quot;Next Sentence Preiction. Fonte: http://jalammar.github.io/illustrated-bert/&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;div class=&quot;flash&quot;&gt;
    &lt;svg class=&quot;octicon octicon-info octicon octicon-info octicon octicon-info octicon octicon-info&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M8 1.5a6.5 6.5 0 100 13 6.5 6.5 0 000-13zM0 8a8 8 0 1116 0A8 8 0 010 8zm6.5-.25A.75.75 0 017.25 7h1a.75.75 0 01.75.75v2.75h.25a.75.75 0 010 1.5h-2a.75.75 0 010-1.5h.25v-2h-.25a.75.75 0 01-.75-.75zM8 6a1 1 0 100-2 1 1 0 000 2z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Note: &lt;/strong&gt;A &lt;em&gt;loss&lt;/em&gt; de treinamento é a soma das médias da MLM e NSP
&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;div class=&quot;flash flash-warn&quot;&gt;
    &lt;svg class=&quot;octicon octicon-zap&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M10.561 1.5a.016.016 0 00-.01.004L3.286 8.571A.25.25 0 003.462 9H6.75a.75.75 0 01.694 1.034l-1.713 4.188 6.982-6.793A.25.25 0 0012.538 7H9.25a.75.75 0 01-.683-1.06l2.008-4.418.003-.006a.02.02 0 00-.004-.009.02.02 0 00-.006-.006L10.56 1.5zM9.504.43a1.516 1.516 0 012.437 1.713L10.415 5.5h2.123c1.57 0 2.346 1.909 1.22 3.004l-7.34 7.142a1.25 1.25 0 01-.871.354h-.302a1.25 1.25 0 01-1.157-1.723L5.633 10.5H3.462c-1.57 0-2.346-1.909-1.22-3.004L9.503.429z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Important: &lt;/strong&gt;Você deve ter notado que, durante o treinamento, não é necessário o uso de &lt;em&gt;labels&lt;/em&gt;, já que derivamos &lt;em&gt;labels&lt;/em&gt; a partir do input. Logo, o modelo de Pré Treinamento do BERT é considerado &lt;em&gt;self-surpervised&lt;/em&gt;!
&lt;/div&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Resum&amp;#227;o-de-tudo&quot;&gt;Resum&amp;#227;o de tudo&lt;a class=&quot;anchor-link&quot; href=&quot;#Resum&amp;#227;o-de-tudo&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Como estamos lidando com embeddings de &lt;strong&gt;sentenças&lt;/strong&gt; (não &lt;strong&gt;palavras&lt;/strong&gt;), precisamos de uma forma de fazer o encoding desse input da maneira certa. Vamos ver como o BERT faz:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Primeiro recebemos os tokens de texto como entrada&lt;/li&gt;
&lt;li&gt;Aplicamos o WordPiece Tokenizer&lt;/li&gt;
&lt;li&gt;Essa entrada entra na pilha de Encoder&lt;/li&gt;
&lt;li&gt;Treinamos a rede (Pre-Training step)&lt;/li&gt;
&lt;li&gt;Para os familiarizados com redes convolucionais, podemos dizer que o embedding do token [CLS] funciona como uma representação &quot;pooled&quot; (&lt;a href=&quot;https://arxiv.org/pdf/2002.08909.pdf&quot;&gt;ref&lt;/a&gt;) da sentença e, logo, pode ser usada como um embedding &lt;strong&gt;contextual&lt;/strong&gt;. No caso, ela serve de entrada para uma rede neural para resolver problemas de classificação!&lt;/li&gt;
&lt;li&gt;Dependendo da tarefa de &lt;em&gt;Fine tuning&lt;/em&gt;, é possível usar os embeddings de um token diferente do CLS
&lt;div class=&quot;flash flash-warn&quot;&gt;
    &lt;svg class=&quot;octicon octicon-zap octicon octicon-zap&quot; viewBox=&quot;0 0 16 16&quot; version=&quot;1.1&quot; width=&quot;16&quot; height=&quot;16&quot; aria-hidden=&quot;true&quot;&gt;&lt;path fill-rule=&quot;evenodd&quot; d=&quot;M10.561 1.5a.016.016 0 00-.01.004L3.286 8.571A.25.25 0 003.462 9H6.75a.75.75 0 01.694 1.034l-1.713 4.188 6.982-6.793A.25.25 0 0012.538 7H9.25a.75.75 0 01-.683-1.06l2.008-4.418.003-.006a.02.02 0 00-.004-.009.02.02 0 00-.006-.006L10.56 1.5zM9.504.43a1.516 1.516 0 012.437 1.713L10.415 5.5h2.123c1.57 0 2.346 1.909 1.22 3.004l-7.34 7.142a1.25 1.25 0 01-.871.354h-.302a1.25 1.25 0 01-1.157-1.723L5.633 10.5H3.462c-1.57 0-2.346-1.909-1.22-3.004L9.503.429z&quot;&gt;&lt;/path&gt;&lt;/svg&gt;
    &lt;strong&gt;Important: &lt;/strong&gt;Se desconsiderarmos a tarefa de fine tuning, o vetor CLS não tem uma representação muito grande, uma vez que ele foi treinado por meio da &lt;em&gt;loss&lt;/em&gt; NSP (&lt;a href=&quot;https://arxiv.org/pdf/1810.04805.pdf&quot;&gt;ref&lt;/a&gt;)
&lt;/div&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Eu tentei resumir o processo todo com o gif abaixo&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/personal_blog/images/copied_from_nb/images/media/videos/scene/720p30/TransformerEncoderExample.gif&quot; alt=&quot;&quot; title=&quot;Entire Forward passing in BERT&quot; /&gt;&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Na-pr&amp;#225;tica&quot;&gt;Na pr&amp;#225;tica&lt;a class=&quot;anchor-link&quot; href=&quot;#Na-pr&amp;#225;tica&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Para mostrar o embedding de sentenças do BERT funcionando, eu irei usar a biblioteca &lt;a href=&quot;https://huggingface.co/transformers/&quot;&gt;Hugging Face's transformer&lt;/a&gt;. Aqui, uma vez que o &lt;strong&gt;Bert Model para Modelos de Linguagem&lt;/strong&gt; já foi treinado, Eu usarei o BERT sem nenhuma cabeça (i.g., &lt;code&gt;LanguageModeling head&lt;/code&gt; or &lt;code&gt;SentenceClassification head&lt;/code&gt;) na ponta!&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;details class=&quot;description&quot;&gt;
      &lt;summary class=&quot;btn btn-sm&quot; data-open=&quot;Hide Code&quot; data-close=&quot;Show Code&quot;&gt;&lt;/summary&gt;
        &lt;p&gt;&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;torch&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;transformers&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BertTokenizer&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertForPreTraining&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertTokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;bert-base-multilingual-uncased&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BertModel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;from_pretrained&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;bert-base-multilingual-uncased&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
    &lt;/details&gt;
&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stderr output_text&quot;&gt;
&lt;pre&gt;Downloading: 100%|██████████| 872k/872k [00:01&amp;lt;00:00, 699kB/s]
Downloading: 100%|██████████| 625/625 [00:00&amp;lt;00:00, 153kB/s]
Downloading: 100%|██████████| 672M/672M [03:07&amp;lt;00:00, 3.58MB/s]
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence_0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;Ele ocupa um alto posto na empresa&amp;quot;&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sequence_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&amp;quot;Abasteci meu carro no posto do alto do morro&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;details class=&quot;description&quot;&gt;
      &lt;summary class=&quot;btn btn-sm&quot; data-open=&quot;Hide Code&quot; data-close=&quot;Show Code&quot;&gt;&lt;/summary&gt;
        &lt;p&gt;&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence_0_w2id&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence_0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# we need to map words to id&amp;#39;s :)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sequence_1_w2id&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tokenizer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;encode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence_1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Mapeamento de sequencia 0 word2Id: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence_0_w2id&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Mapeamento de sequence 1 word2Id: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence_1_w2id&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
    &lt;/details&gt;
&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Mapeamento de sequencia 0 word2Id: [101, 12002, 28905, 10316, 13248, 14645, 10135, 14443, 102]
Mapeamento de sequence 1 word2Id: [101, 51448, 11176, 10532, 44780, 43562, 10181, 14645, 10154, 13248, 10154, 43522, 102]
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence_0_embeddings&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;tensor([[  101, 12002, 28905, 10316, 13248, 14645, 10135, 14443,   102]])&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;details class=&quot;description&quot;&gt;
      &lt;summary class=&quot;btn btn-sm&quot; data-open=&quot;Hide Code&quot; data-close=&quot;Show Code&quot;&gt;&lt;/summary&gt;
        &lt;p&gt;&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence_0_embeddings&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence_0_w2id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unsqueeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# Batch size 1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sequence_0_embeddings&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence_0_embeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detach&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sequence_1_embeddings&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;torch&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tensor&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence_1_w2id&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;unsqueeze&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;  &lt;span class=&quot;c1&quot;&gt;# Batch size 1&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sequence_1_embeddings&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence_1_embeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;detach&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
    &lt;/details&gt;
&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence_0_embeddings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequence_1_embeddings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;((1, 9, 768), (1, 13, 768))&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Podemos remover a primeira sentença, já que ela representa o tamanho do batch&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;details class=&quot;description&quot;&gt;
      &lt;summary class=&quot;btn btn-sm&quot; data-open=&quot;Hide Code&quot; data-close=&quot;Show Code&quot;&gt;&lt;/summary&gt;
        &lt;p&gt;&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence_0_embeddings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence_0_embeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sequence_1_embeddings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence_1_embeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sequence_0_embeddings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequence_1_embeddings&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
    &lt;/details&gt;
&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;



&lt;div class=&quot;output_text output_subarea output_execute_result&quot;&gt;
&lt;pre&gt;((9, 768), (13, 768))&lt;/pre&gt;
&lt;/div&gt;

&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Podemos ver que esse modelo gera um embedding para cada palavra das frases mais dois: um para o token &lt;code&gt;CLS&lt;/code&gt; e outro para o token &lt;code&gt;SEP&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Agora, vamos calcular a similaridade entre o token CLS e a média dos tokens que compõe a frase:&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;details class=&quot;description&quot;&gt;
      &lt;summary class=&quot;btn btn-sm&quot; data-open=&quot;Hide Code&quot; data-close=&quot;Show Code&quot;&gt;&lt;/summary&gt;
        &lt;p&gt;&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CLS_TOKEN_0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequence_0_embeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;CLS_TOKEN_WORDS_0&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence_0_embeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Similaridade de Cosseno entre o token CLS e a média dos tokens de&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence_0&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;#39;&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot; tokens: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cosine_similarity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CLS_TOKEN_0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CLS_TOKEN_WORDS_0&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
    &lt;/details&gt;
&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Similaridade de Cosseno entre o token CLS e a média dos tokens de
&amp;#39;Ele ocupa um alto posto na empresa&amp;#39; tokens: -0.04759013652801514
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;
&lt;details class=&quot;description&quot;&gt;
      &lt;summary class=&quot;btn btn-sm&quot; data-open=&quot;Hide Code&quot; data-close=&quot;Show Code&quot;&gt;&lt;/summary&gt;
        &lt;p&gt;&lt;div class=&quot;input&quot;&gt;

&lt;div class=&quot;inner_cell&quot;&gt;
    &lt;div class=&quot;input_area&quot;&gt;
&lt;div class=&quot; highlight hl-ipython3&quot;&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CLS_TOKEN_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sequence_1_embeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;CLS_TOKEN_WORDS_1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence_1_embeddings&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;Similaridade de Cosseno entre o token CLS e a média dos tokens de&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;#39;&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sequence_1&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;#39;&amp;quot;&lt;/span&gt;
    &lt;span class=&quot;sa&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot; tokens: &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;cosine_similarity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;CLS_TOKEN_1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;CLS_TOKEN_WORDS_1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reshape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&amp;quot;&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

    &lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/p&gt;
    &lt;/details&gt;
&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Similaridade de Cosseno entre o token CLS e a média dos tokens de
&amp;#39;Abasteci meu carro no posto do alto do morro&amp;#39; tokens: 0.05738348513841629
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;Como dito pelo paper, o token CLS não tem significado nenhum aqui, Vamos, então, analisar a similaridade entre as médias dos tokens de ambas as sentenças&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
    
    
&lt;div class=&quot;cell border-box-sizing code_cell rendered&quot;&gt;

&lt;div class=&quot;output_wrapper&quot;&gt;
&lt;div class=&quot;output&quot;&gt;

&lt;div class=&quot;output_area&quot;&gt;

&lt;div class=&quot;output_subarea output_stream output_stdout output_text&quot;&gt;
&lt;pre&gt;Similaridade de Cosseno entre o token CLS e a média dos tokens de
&amp;#39;Ele ocupa um alto posto na empresa&amp;#39;and &amp;#39;Abasteci meu carro no posto do alto do morro&amp;#39; tokens :0.5034023523330688
&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
&lt;/div&gt;

&lt;/div&gt;
    

&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;p&gt;&lt;strong&gt;Como esperado&lt;/strong&gt;, apesar de palavras parecidas terem sido usadas, ccomo o contexto entre palavras foram totalmente disferentes, a similaridade de embeddings aqui foi menor do que o de combinação de palavras, usado no começo do artigo.&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Conclusion&quot;&gt;Conclusion&lt;a class=&quot;anchor-link&quot; href=&quot;#Conclusion&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Parabéns! Você aprendeu os principais conceitos por trás do modelo BERT :) Se você tiver interesse em ver outros posts em português, por favor me envie uma mensagem!&lt;/p&gt;
&lt;p&gt;Além disso, se você quer ter mais detalhes de como usar o BERT de uma maneira prática, eu recomendo esse &lt;a href=&quot;https://huggingface.co/blog/how-to-train&quot;&gt;blog post&lt;/a&gt;!&lt;/p&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Recursos-que-me-inspiraram&quot;&gt;Recursos que me inspiraram&lt;a class=&quot;anchor-link&quot; href=&quot;#Recursos-que-me-inspiraram&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Além dos papers que eu usei para citar nesse post, eu também gostaria de enfatizar que os links abaixo serviram de uma inspiração absurda!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;http://jalammar.github.io/illustrated-bert/&quot;&gt;http://jalammar.github.io/illustrated-bert/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://jalammar.github.io/illustrated-transformer/&quot;&gt;https://jalammar.github.io/illustrated-transformer/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;http://nlp.seas.harvard.edu/2018/04/03/attention.html&quot;&gt;http://nlp.seas.harvard.edu/2018/04/03/attention.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;cell border-box-sizing text_cell rendered&quot;&gt;&lt;div class=&quot;inner_cell&quot;&gt;
&lt;div class=&quot;text_cell_render border-box-sizing rendered_html&quot;&gt;
&lt;h1 id=&quot;Reconhecimento&quot;&gt;Reconhecimento&lt;a class=&quot;anchor-link&quot; href=&quot;#Reconhecimento&quot;&gt; &lt;/a&gt;&lt;/h1&gt;&lt;p&gt;Eu gostaria de agradecer, de verdade, alguns colegas que fizeram a revisão técnica desse blog :)&lt;/p&gt;
&lt;p&gt;Em ordem alfabética:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/in/alan-barzilay-58754855/&quot;&gt;Alan Barzilay&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/in/alvaro-marques-9a10aa131/&quot;&gt;Alvaro Marques&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.linkedin.com/in/ighoelscher/&quot;&gt;Igor Hoelscher&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;</content><author><name>Andre Barbosa</name></author><category term="masters" /><category term="nlp" /><category term="knowledge-distill" /><summary type="html"></summary></entry></feed>